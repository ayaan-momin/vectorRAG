Title: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications
Abstract: Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.
Full Text: Large Language Models as Software Components:
A Taxonomy for LLM-Integrated Applications
Irene Weber
Kempten University of Applied Sciences, Germany
irene.weber@hs-kempten.de
Abstract
Large Language Models (LLMs) have become widely adopted recently. Research explores their use both
as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other
hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or
requiresignificantcodingeffort. WhileLLM-integratedapplicationengineeringisemergingasnewdiscipline,
its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-
integratedapplications, offeringaframeworkforanalyzinganddescribingthesesystems. Italsodemonstrates
various ways to utilize LLMs in applications, as well as options for implementing such integrations.
Following established methods, we analyze a sample of recent LLM-integrated applications to identify rel-
evant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that
applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term “LLM components”. To gain a clear understanding of an application’s
architecture, we examine each LLM component separately. We identify thirteen dimensions along which to
characterize an LLM component, including the LLM skills leveraged, the format of the output, and more.
LLM-integrated applications are described as combinations of their LLM components. We suggest a concise
representation using feature vectors for visualization.
The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in
the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers
and practitioners explore numerous creative ways to leverage LLMs in applications. Though challenges
persist, integrating LLMs may revolutionize the way software systems are built.
Keywords: large language model, LLM-integrated, taxonomy, copilot, architecture, AI agent, LLM
component
1. Introduction
Large Language Models (LLMs) have significantly
impacted various sectors of economy and society [47].
Due to their proficiency in text understanding, cre-
ative work, communication, knowledge work, and
code writing, they have been adopted in numerousfields, such as medicine, law, marketing, education,
human resources, etc.
Public discussions often focus on the ethical aspects
and societal consequences of these systems [36, 39].
Meanwhile, research investigates Artificial General
Intelligences and autonomous AI agents that can use
services, data sources, and other tools, and collabo-arXiv:2406.10300v1  [cs.SE]  13 Jun 2024rate to solve complex tasks [11, 62, 57, 21]. In addi-
tion, LLMs offer many opportunities to enhance soft-
ware systems. They enable natural language interac-
tion [59], automate complex tasks [19], and provide
supportive collaboration, as seen with recent LLM-
basedassistantproductsoftenbrandedas“copilots”1.
This paper addresses the potential of LLMs for soft-
ware development by integrating their capabilities as
components into software systems. This contrasts
with current software engineering research, which
views LLMs as tools for software development rather
than as software components [14, 22], and with the
considerable body of research examining LLMs as au-
tonomous agents within multiagent systems [21].
Software systems that invoke an LLM and process
its output are referred to as “LLM-integrated appli-
cations”, “LLM-integrated systems”, “LLM-based ap-
plications”, etc. [32, 13, 57]. LLMs are versatile, mul-
tipurpose tools capable of providing functionalities
that would otherwise be unfeasible or require sub-
stantial development efforts [15, 24]. By significantly
expediting system development, they have the poten-
tial to revolutionize not only the way users interact
with technology, but also the fundamental processes
of software development.
LLM-integrated applications engineering is emerging
as a research field. E.g., [10] proposes LLM Sys-
temsEngineering(LLM-SE)asanoveldiscipline,and
[44, 8, 7] discuss experiences and challenges that de-
velopers of such systems encounter in practice.
This study develops a taxonomy that provides a
structured framework for categorizing and analyzing
LLM-integrated applications across various domains.
To develop and evaluate the taxonomy, we collected
a sample of LLM-integrated applications, concentrat-
ing on technical and industrial domains. These ap-
plications showcase a broad range of opportunities
to leverage LLMs, often integrating LLMs in mul-
tiple ways for distinct purposes. In developing the
taxonomy, we found that examining each of these in-
tegrations, termed “LLM components”, separately is
1E.g., https://docs.github.com/en/copilot ,
https://copilot.cloud.microsoft/en-us/copilot-excel ,
https://www.salesforce.com/einsteincopilotcrucial for a clear understanding of an application’s
architecture.
The taxonomy adopts an original architectural per-
spective, focusing on how the application interacts
with the LLM while abstracting from the specifics
of application domains. For researchers, the taxon-
omy contributes to shape a common understanding
and terminology, thus aiding theory building in this
emerging domain [29, 50, 18]. For practitioners, the
taxonomy provides inspiration for potential uses of
LLMs in applications, presents design options, and
helps identify challenges and approaches to address
them.
Objectives. In this study, a taxonomy is understood
as a set of dimensions divided into characteristics.
Theobjectiveistoidentifydimensionsthatareuseful
for categorizing the integration of LLMs in applica-
tions from an architectural perspective. To be most
effective, the taxonomy should be easy to understand
and apply, yet distinctive enough to uncover the es-
sential aspects. Additionally, we aim to develop a
visual representation tailored to the taxonomy’s in-
tended purposes.
Overview. The following section 2 provides back-
ground on LLMs and introduces relevant concepts.
Section 3 presents an overview of related work. The
study design adheres to a Design Science Research
approach[46]. Weapplyestablishedmethodsfortax-
onomy design [42, 48] as described in Section 4. This
section also presents the sample of LLM-integrated
applications used for this study. The developed tax-
onomy is presented, demonstrated and formally eval-
uated in section 5. In section 6, we discuss its usabil-
ity and usefulness. Section 7 summarizes the contri-
butions, addresses limitations, and concludes.
2. Large Language Models
2.1. Background
State-of-the-art LLMs such as GPT-3.5, GPT-4,
Llama, PALM2, etc., are artificial neural networks
consisting of neurons, i.e., very simple processing
2units, that are organized in layers and connected by
weighted links. Training a neural network means
adapting these weights such that the neural network
shows a certain desired behavior. Specifically, an
LLM is trained to predict the likelihoods of pieces
of text termed, tokens, to occur as continuations of
a given text presented as input to the LLM. This in-
put is referred to as prompt. The prompt combined
with the produced output constitutes the contextof
an LLM. It may comprise more than 100k tokens in
state-of-the-artLLMs2. Still, itslengthislimitedand
determinesthemaximumsizeofpromptsandoutputs
that an LLM is capable of processing and generating
at a time.
Training of an LLM optimizes its parameters such
that its computed likelihoods align with real text ex-
amples. The training data is a vast body of text snip-
pets extracted, processed, and curated from sources
suchasWikipedia,Githubcoderepositories,common
websites, books, or news archives. An LLM trained
on massive examples is termed a foundation model
orpre-trained model . During training, an LLM not
only learns to produce correct language but also ab-
sorbs and stores information and factual knowledge.
However, it is well known that LLMs frequently pick
up biases, leading to ethical problems. They may
also produce factually incorrect outputs that sound
plausible and convincing, termed hallucinations .
Recent findings show that LLMs can be applied to
a wide range of tasks by appropriately formulating
prompts. Different prompt patterns succeed in dif-
ferent tasks. Basic approaches rely on instructing
the LLM to solve a task described or explained in
the prompt. In few-shot prompting (also known as
few-shot learning), the prompt is augmented with ex-
ampleinput-outputpairsillustratinghowtosolvethe
task, e.g., the requested output format. The number
of examples can vary. Prompting with one example is
calledone-shot prompting, while prompting without
anyexamplesiscalled zero-shot prompting. One-shot
andfew-shot prompting fall under the broader cat-
egory of in-context learning . Prompt patterns such
2https://platform.openai.com/docs/modelsaschain-of-thought andthinking-aloud aim to elicit
advanced reasoning capabilities from LLMs.
As effective prompts are crucial for unlocking the di-
verse capabilities of an LLM, the discipline of prompt
engineering is evolving, focusing on the systematic
design and management of prompts [66, 9, 53, 31].
2.2. Definitions
Invoking an LLM results in an input-processing-
output sequence: Upon receiving a prompt, the LLM
processes it and generates an output. We refer to an
individual sequence of input-processing-output per-
formed by the LLM as LLM invocation , and define
anLLM-integrated application as a system in which
the software generates the prompt for the LLM and
processes its output. The concept of an application
is broad, encompassing service-oriented architectures
and systems with components loosely coupled via
API calls.
Given an LLM’s versatility, an application can uti-
lize it for different tasks, each demanding a specific
approach to create the prompt and handle the re-
sult. This paper defines a particular software compo-
nentthataccomplishesthisasan LLM-based software
component or, simply, LLM component . An LLM-
integrated application can comprise several LLM
components. The study develops a taxonomy for
LLM components. LLM-integrated applications are
described as combinations of their LLM components.
3. Related Work
With the recent progress in generative AI and LLMs,
the interest in these techniques has increased, and
numerous surveys have been published, providing an
extensive overview of technical aspects of LLMs [72],
reviewingLLMsastoolsforsoftwareengineering[22],
and discussing the technical challenges of applying
LLMs across various fields [25]. Further studies ad-
dress the regulatory and ethical aspects of Genera-
tive AI and ChatGPT, with a particular focus on
AI-human collaboration [41], and Augmented Lan-
guage Models (ALMs), which are LLMs that enhance
3their capabilities by querying tools such as APIs,
databases, and web search engines [38].
Taxomonies related to LLMs include a taxonomy for
prompts designed to solve complex tasks [49] and a
taxonomy of methods for cost-effectively invoking a
remote LLM [60]. A comparative analysis of stud-
ies on applications of ChatGPT is provided by [27],
whereas LLMs are compared based on their applica-
tion domains and the tasks they solve in [20]. Most
closely related to the taxonomy developed here is a
taxonomy for LLM-powered multiagent architectures
[21] which focuses on autonomous agents with less
technical detail. Taxonomies of applications of AI in
enterprises [48] and applications of generative AI, in-
cluding but not limited to LLMs [52], are developed
using methods similar to those in our study.
Several taxonomies in the field of conversational
agents and task-oriented dialog (TOD) systems ad-
dresssystemarchitecture[1,40,12,3]. However, they
omitdetailedcoverageoftheintegrationofgenerative
language models.
4. Methods
We constructed the taxonomy following established
guidelines [42, 48, 29], drawing from a sample of
LLM-integrated applications. These applications are
detailed in section 4.1.
4.1. Development
Taxonomy. We derived an initial taxonomy from the
standard architecture of conversational assistants de-
scribed in [3], guided by the idea that conversational
assistants are essentially “chatbots with tools”, i.e.,
language-operated user interfaces that interact with
externalsystems. Thisapproachprovedunsuccessful.
The second version was based on the classical three-
tier software architecture, and then extended over
several development cycles. By repeatedly apply-
ing the evolving taxonomy to the example instances,
we identified dimensions and characteristics using an
“empirical-to-conceptual” approach. When new di-
mensionsemerged, additionalcharacteristicswerede-
rived in a “conceptual-to-empirical” manner. Afterfive major refinement cycles, the set of dimensions
and characteristics solidified. In the subsequent eval-
uation phase, we applied the taxonomy to a new set
of example instances that were not considered while
constructing the taxonomy. As the dimensions and
characteristics remained stable, the taxonomy was
considered complete. In the final phase, we refined
the wording and visual format of the taxonomy.
Visualization. Developing a taxonomy involves cre-
ating a representation that effectively supports its
intended purpose [29]. Taxonomies can be repre-
sented in various formats, with morphological boxes
[54, 55] or radar charts [21] being well-established
approaches. We evaluated morphological boxes, be-
cause they effectively position categorized instances
withinthedesignspace. However, wefoundthatthey
make it difficult to perceive a group of categorized in-
stances as a whole since they occupy a large display
area. This drawback is significant for our purposes,
as LLM-integrated applications often comprise mul-
tiple LLM components. Therefore, we developed a
more condensed visualization of the taxonomy based
on feature vectors.
Example instances. We searched for instances of
LLM-integrated applications for taxonomy develop-
ment that should meet the following criteria:
•The application aims for real-world use rather
than focusing on research only (such as testbeds
for experiments or proofs-of-concept). It demon-
strateseffortstowardspracticalusabilityandad-
dresses challenges encountered in real-world sce-
narios.
•The application’s architecture, particularly its
LLM components, is described in sufficient de-
tail for analysis.
•The sample of instances covers a diverse range
of architectures.
•Theexampleinstancesaresituatedwithinindus-
trial or technical domains, as we aim to focus on
LLM-integrated applications beyond well-known
fields like law, medicine, marketing, human re-
sources, and education.
4The search revealed a predominance of theoretical re-
search on LLM-integrated applications while papers
focusing on practically applied systems were scarce.
Searching non-scientific websites uncovered commer-
cially advertised AI-powered applications, but their
internalworkingsweretypicallyundisclosed, andreli-
able evaluations were lacking. Furthermore, the het-
erogeneous terminology and concepts in this emerg-
ing field make a comprehensive formal literature
search unfeasible. Instead, by repeatedly search-
ing Google Scholar and non-scientific websites using
terms “LLM-integrated applications”, “LLM-powered
applications”, “LLM-enhanced system”, “LLM” and
“tools”, alongsimilarvariants, weselectedsixsuitable
instances. Some of them integrate LLMs in multiple
ways, totaling eleven distinct LLM components.
For a thorough evaluation, we selected new instances
using relaxed criteria, including those intended for
research. Additionally, we included a real-world ex-
ample lacking explicit documentation to broaden the
diversity of our sample and assess the taxonomy’s
coverage. Within the five selected instances, we iden-
tified ten LLM components.
4.2. Sample of LLM-integrated applications
Table1givesanoverviewofthesample. Namesofap-
plications and LLM components are uniformly writ-
tenasoneCamelCasewordandtypesetinsmallcaps,
deviating from the format chosen by the respective
authors.
Honeycomb .Honeycomb is an observability plat-
form collecting data from software applications in
distributed environments for monitoring. Users
define queries to retrieve information about the
observed software systems through Honeycomb ’s
Query Builder UI. The recently added LLM-based
QueryAssistant allows users to articulate inquiries
in plain English, such as “slow endpoints by status
code” or “which service has the highest latency?”
TheQueryAssistant converts these into queries in
Honeycomb ’s format, which users can execute and
manually refine [7, 8].LowCode .LowCode is a web-based application
consisting of a prompt-definition section and a di-
alogue section. The prompt-definition section sup-
ports the design of prompts for complex tasks, such
as composing extensive essays, writing resumes for
job applications or acting as a hotel service chatbot
[5]. In the dialogue section, users converse with an
LLM to complete the complex task based on the de-
fined prompt.
LowCode comprises two LLM components termed
Planning andExecuting .Planning operates in
the prompt-definition section, where a user roughly
describes a complex task, and Planning designs a
workflowforsolvingit. Theprompt-definitionsection
offers a low-code development environment where the
LLM-generated workflow is visualized as a graphi-
cal flowchart, allowing a user to edit and adjust the
logic of the flow and the contents of its steps. For
instance, in essay-writing scenarios, this involves in-
serting additional sections, rearranging sections, and
refining the contents of sections. Once approved by
the user, LowCode translates the modified work-
flow back into natural language and incorporates it
into a prompt for Executing . In the dialogue sec-
tion, users converse in interactive, multi-turn dia-
logueswith Executing . Asdefinedintheprompt, it
acts as an assistant for tasks such as writing an essay
or resume, or as a hotel service chatbot. While the
idea of the LLM planning a workflow might suggest
using the LLM for application control, LowCode
Planning actuallyservesasapromptgeneratorthat
supports developing prompts for complex tasks.
MyCrunchGpt .MyCrunchGpt acts as an ex-
pert system within the engineering domain, specif-
ically for airfoil design and calculations in fluid me-
chanics. These tasks require complex workflows com-
prising several steps such as preparing data, param-
eterizing tools, and evaluating results, using vari-
ous software systems and tools. The aim of My-
CrunchGpt is to facilitate the definition of these
workflows and automate their execution [28].
MyCrunchGpt offers a web interface featuring a
dialogue window for inputting commands in plain
English, along with separate windows displaying the
5Table 1: Example instances selected for development (top 6) and evaluation (bottom 5)
Application References LLM components
Honeycomb [7, 8] QueryAssistant
LowCode [5],[35] Planning ,Executing
MyCrunchGpt [28] DesignAssistant ,SettingsEditor ,DomainExpert
MatrixProduction [69] Manager ,Operator
WorkplaceRobot [37] TaskPlanning
AutoDroid [64] TaskExecutor ,MemoryGenerator
ProgPrompt [51] ActionPlanning ,ScenarioFeedback
FactoryAssistants [26] QuestionAnswering
SgpTod [71] DstPrompter ,PolicyPrompter
TruckPlatoon [70] Reporting
ExcelCopilot [16, 44] ActionExecutor ,Advisor ,IntentDetector ,Explainer
output and results of software tools invoked by My-
CrunchGpt in the backend. MyCrunchGpt relies
on predefined workflows, not supporting deviations
or cycles. By appending a specific instruction to the
dialogue history in the prompt for each step of the
workflow, it uses the LLM as a smart parser to ex-
tract parameters for APIs and backend tools from
user input. APIs and tools are called in the prede-
fined order [28, p. 56].
MyCrunchGpt is still in development. The paper
[28] explains the domain as well as the integration of
the LLM, but does not fully detail the implementa-
tion of the latter. Still, MyCrunchGpt illustrates
innovative applications of an LLM in a technical do-
main. We categorize three LLM components solving
tasks within MyCrunchGpt : aDesignAssistant
guiding users through workflows and requesting pa-
rameters for function and API calls; a SettingsEd-
itorupdating a JSON file with settings for a back-
endsoftwaretool; anda DomainExpert whichhelps
evaluating results by comparing them to related re-
sults, e.g., existing airfoil designs, which it derives
from its trained knowledge.
MatrixProduction .MatrixProduction em-
ploys an LLM for controlling a matrix production
system [69]. While in a classical line production
setup, workstations are arranged linearly and the
manufacturing steps follow a fixed sequence, matrix
production is oriented towards greater flexibility.Autonomous transport vehicles carry materials
and intermediate products to workstations, termed
automation modules, each offering a spectrum of
manufacturing skills that it can contribute to the
production process. Compared to line production,
matrix production is highly adaptable and can
manufacture a variety of personalized products with
full automation. This requires intelligent production
management to (a) create workplans that orchestrate
and schedule the automation modules’ skills, and (b)
program the involved automation modules such that
they execute the required processing steps.
MatrixProduction incorporates two LLM compo-
nents: Manager creates workplans as sequences of
skills (a), while Operator generates programs for
the involved automation modules (b).
MatrixProduction prompts Manager andOp-
erator to provide textual explanations in addition
to the required sequences of skills or automation
module programs. The LLM output is processed
by a parser before being used to control the physi-
cal systems. Manager relies on built-in production-
specific knowledge of the LLM such as “a hole is pro-
duced by drilling”.
Noteworthy in this approach is its tight integra-
tion into the system landscape of Industry 4.0.
Thefew-shot Manager andOperator prompts
are generated automatically using Asset Adminis-
tration Shells , which are standardized, technology-
6independent data repositories storing digital twins of
manufacturing assets for use in Industry 4.0 [2].
WorkplaceRobot .An experimental robot system
is enhanced with LLM-based task planning in [37].
The robot operates in a workplace environment fea-
turing a desk and several objects. It has previously
been trained to execute basic operations expressed
in natural language such as “open the drawer” or
“take the pink object and place it in the drawer”.
LLM-based task planning enables the robot to per-
form more complex orders like “tidy up the work area
and turn off all the lights”. To this end, an LLM is
prompted to generate a sequence of basic operations
that accomplish the complex order.
Although the robot expects operations phrased in
natural language, the LLM is prompted with a
Python coding task. For instance, the basic opera-
tion“turnonthegreenlight” correspondstoaPython
command push_button(’green’) . The prompt for
the LLM includes several examples each consisting
of a description of an environment state, a complex
order formatted as a comment, and a sequence of
Python robot commands that accomplish the com-
plex order. When invoking the LLM to generate the
Python program for a new order, the prompt is aug-
mented with a description of the environment’s cur-
rent state and the new order as a comment.
The Python code produced by the LLM is trans-
lated back to a sequence of basic operations in nat-
ural language. When the robot executes these oper-
ations, there is no feedback about successful comple-
tion. Rather, the system assumes that all basic op-
erations require a fixed number of timesteps to com-
plete.
AutoDroid .The goal of mobile task automation is
hands-free user interaction for smartphones through
voice commands. AutoDroid is a voice control sys-
tem for smartphones that can automatically execute
complex orders such as “remind me to do laundry on
May 11th” or “delete the last photo I took” [64, 65].
Such complex orders are fulfilled by performing se-
quences of basic operations in an Android app, suchas “scroll down, then press button x” in the calen-
dar app. AutoDroid employs an LLM component
TaskExecutor to plan these sequences of opera-
tions. The challenge is that the next operation to ex-
ecutedependsonthecurrentstateoftheAndroidapp
which continuously changes as the app is operated.
AutoDroid solves this by invoking the TaskEx-
ecutor repeatedly after each app operation with the
prompt comprising the updated state of the Graph-
ical User Interface (GUI) along with the user’s com-
plex order.
Before executing irrevocable operations, such as per-
manently deleting data or calling a contact, Auto-
Droidprompts the user to confirm or adjust the op-
eration. TaskExecutor is instructed to include a
“confirmation needed” hint in its output for such op-
erations.
The prompt for TaskExecutor comprises an ex-
tract from a knowledge base which is built automati-
cally in an offline learning phase as follows: In a first
step, a “UI Automator” (which is not an LLM com-
ponent) automatically and randomly operates the
GUI elements of an Android app to generate a UI
Transition Graph (UTG). The UTG has GUI states
as nodes and the possible transitions between GUI
states as edges. As next steps, AutoDroid invokes
two LLM components referred to as MemoryGen-
erators to analyze the UTG.
The first MemoryGenerator is prompted repeat-
edly for each GUI state in the UTG. Its task is to
explain the functionality of the GUI elements. Be-
sides instructions and examples of the table format
desired as output, its prompt includes an HTML rep-
resentation of the GUI state, the GUI actions preced-
ing this state, and the GUI element operated next.
Its output consists of tuples explaining the function-
ality of a GUI element by naming the derived func-
tionality (e.g., “delete all the events in the calendar
app”)andtheGUIstatesandGUIelementactionsin-
volved. Similarly, the second MemoryGenerator
is prompted to output a table listing GUI states and
explanations of their functions. These tables consti-
tuteAutoDroid ’s knowledge base.
ProgPrompt .ProgPrompt [51] is an approach
to LLM-based robot task planning similar to
7WorkplaceRobot . Its robot is controlled by
Python code and works in a real and a simulated
household environment.
ProgPrompt comprisestwoLLMcomponents. Ac-
tionPlanning generates Python scripts for tasks
such as “microwave salmon” using basic opera-
tions like grab(’salmon’) ,open(’microwave’) ,
andputin(’salmon’, ’microwave’) , notably with-
out considering the current state of the environment.
To establish a feedback loop with the environment,
ActionPlanning adds assertstatements. These
statements verify the preconditions of basic opera-
tionsandtriggerremedialactionswhenpreconditions
are not met. For instance, a script for “microwave
salmon” comprises the following code fragment:
if assert(’microwave’ is ’opened’)
else: open(’microwave’)
putin(’salmon’, ’microwave’)
When operating in the simulated environment,
ProgPrompt can verify an assert statement
through its second LLM component, Scenario-
Feedback . Prompted with the current state of the
environment and the assertstatement, Scenario-
Feedback evaluates it and outputs TrueorFalse.
FactoryAssistants .FactoryAssistants advise
workers on troubleshooting production line issues in
two manufacturing domains: detergent production
and textile production [26]. The assistants leverage
domain knowledge from FAQs and documented prob-
lem cases to answer user queries. The required do-
main knowledge is provided as a part of the prompt.
SgpTod .SgpTod employs an LLM to implement a
chatbot, specifically, a task-oriented dialogue (TOD)
system [71]. TOD systems are also known as conver-
sational assistants. In contrast to open-domain dia-
logue (ODD) systems, which engage users in goalless
conversations, they are designed for assisting users in
specific tasks.
In general, TOD systems require the following
components [3]: Natural Language Understanding
(NLU), analyzing the user’s input to classify intents
and extract entities; Dialogue Management (DM) fordeciding on a system action that is appropriate in
a given dialogue state (e.g., ask for more informa-
tion or invoke a hotel booking service); and Natu-
ral Language Generation (NLG) for producing a re-
sponse that the TOD system can present to the user.
Intent classification, also known as intent detection,
matches free-text user input to one of several tasks a
TOD system can perform (e.g., book a hotel). Entity
extraction isolates situational values, called entities,
from the user input (e.g., the town and the date of
the hotel booking). The TOD system may require
several dialogue turns to elicit all necessary entities
from the user. In TOD research, the system’s in-
ternal representation of the user’s intentions and the
entity values is commonly referred to as its “belief
state”. For example, in the restaurant search domain,
the belief state may include attribute-value pairs like
cuisine:Indian andpricerange:medium.
SgpTod is a multi-domain TOD system, concur-
rently handling multiple task domains found in stan-
dard TOD evaluation datasets, such as recommend-
ing restaurants or finding taxis. Similar to other ex-
perimental TOD systems [23], SgpTod accesses a
database that stores information from the task do-
mains, such as available hotels and restaurants.
SgpTod comprises two LLM components, called
DstPrompter andPolicyPrompter , that are
bothinvokedineverydialogueturnbetween SgpTod
and the user. The DstPrompter handles the NLU
aspect, analyzing the user’s input and populating the
system’s belief state. It outputs is an SQL query
suited to extract the database entries that match the
current belief state. Upon retrieving the database en-
tries, SgpTod invokes its PolicyPrompter which
covers both DM and NLG. Prompted with the dia-
logue history and the database entries retrieved, it
produces a two-part output: a natural language re-
sponse for NLG and a system action for DM.
TruckPlatoon .The concept of truck platooning
means that trucks travel closely together for bet-
ter fuel efficiency and traffic flow. TruckPla-
tooncomprises an algorithmic control loop which
autonomously maintains a consistent distance be-
tween trucks. It invokes an LLM to generate natural-
language reports on the platoon’s performance and
8stability from measurements tracked by the control
algorithm, providing easily understandable informa-
tion for engineers involved in monitoring and opti-
mizing the truck platooning system.
ExcelCopilot .ExcelCopilot is an example of
a recent trend where software companies integrate
LLM-based assistants, often termed “copilots”, into
their products [44]. These copilots not only provide
textual guidance but also perform actions within the
software environment, constituting a distinctive type
of LLM-integrated application. We chose Excel-
Copilot as an example for evaluating our taxonomy.
Since its implementation is undisclosed, we infer its
architecturefromindirectsources, includingascreen-
cast and a report on insights and experiences from
copilot developers [16, 44]. This inferred architecture
may deviate from the actual implementation.
ExcelCopilot is accessible in a task bar along-
side the Excel worksheet. It features buttons with
context-dependent suggestions of actions and a text
box for users to type in commands in natural lan-
guage. ExcelCopilot only works with data tables,
so its initial suggestion is to convert the active work-
sheet’s data into a data table. Copilot functions ac-
tivate when a data table or part of it is selected. It
then presents buttons for four top-level tasks: “add
formula columns”, “highlight”, “sort and filter”, and
“analyze”. The “analyze” button triggers the copilot
to display more buttons, e.g., one that generates a
pivot chart from the selected data. ExcelCopilot
can also add a formula column to the data table and
explain the formula in plain language.
When a user inputs a free-text command, Excel-
Copilot may communicate its inability to fulfill
it. This constantly occurs with commands requiring
multiple steps, indicating that ExcelCopilot lacks
a planning LLM component as seen in, for example,
MatrixProduction . This observation, along with
itsmentionin[44], suggeststhat ExcelCopilot em-
ploys an intent detection-skill routing architecture.
This architecture includes an LLM component that
maps free-text user commands to potential intents
and then delegates to other LLM components tasked
with generating actions to fulfill those intents. Ac-cordingly, ExcelCopilot comprises several types of
LLM components:
•Several distinct Action Executor s generate
code for specific application actions, such as cre-
ating a pivot table, designing a worksheet for-
mula, inserting a diagram, and so on.
•AnAdvisor suggests meaningful next actions.
Its outputs serve to derive button captions and
prompts for ActionExecutor s.
•When a user inputs a free-text command, the
IntentDetector is invoked to determine and
trigger a suitable ActionExecutor . The In-
tentDetector communicates its actions to
users and informs them when it cannot devise
a suitable action.
•TheExplainer generates natural language ex-
planations of formulae designed by ExcelCopi-
lot. It is unclear whether under the hood, the
ActionExecutor is generating both the for-
mula and the explanation, or if two separate
LLM components are being invoked. We assume
the latter, i.e., that a separate Explainer LLM
component exists.
While users interact repeatedly with ExcelCopi-
lot, each interaction adheres to a single-turn pat-
tern, with the user providing a command and Ex-
celCopilot executing it [44].
5. A Taxonomy for LLM Components and
LLM-Integrated Applications
When developing the taxonomy, it emerged that an-
alyzing an LLM-integrated application should begin
withidentifyinganddescribingitsdistinctLLMcom-
ponents. Analyzing each LLM component separately
helpscapturedetailsandprovidesaclearunderstand-
ing of how the application utilizes LLM capabili-
ties. The LLM-integrated application can then be
described as a combination of the LLM components
it employs.
9Table 2: Dimensions and characteristics of the taxonomy. Codes of characteristics are printed in uppercase. “Meta” means
“metadimension”. “MuEx” means “mutual exclusiveness”.
Meta Dimension Characteristics MuEx
Invocation Interaction App,Command,Dialog enforced
Frequency Single,Iterative yes
Function Logic c Alculate,Control yes
UI none ,Input,Output,Both yes
Data none ,Read,Write,Both yes
Prompt Instruction none ,User,LLM,Program enforced
State none ,User,LLM,Program enforced
Task none ,User,LLM,Program yes
Check none ,User,LLM,Program enforced
Skills re Write,Create,conVerse,Inform,Reason,Planno
Output Format FreeText,Item,Code,Structure no
Revision none ,User,LLM,Program enforced
Consumer User,LLM,Program,Engine enforced
5.1. Overview and demonstration
The taxonomy identifies 13 dimensions for LLM com-
ponents, grouped into five metadimensions as shown
in table 2. It comprises both dimensions with gen-
uinely mutually exclusive characteristics and those
with non-exclusive characteristics. For dimensions
related to the technical integration of LLMs within
applications, mutual exclusiveness is enforced. Given
the open nature of software architecture, the inte-
gration of LLMs allows for significant diversity. In
practice, LLM components may show multiple char-
acteristics within these dimensions. Nonetheless, the
taxonomy requires categorizing each component with
a predominant characteristic, enforcing a necessary
level of abstraction to effectively organize and struc-
ture the domain.
We applied the taxonomy to categorize each of the
example instances described in section 4.2. The re-
sults are depicted in figure 1. The dimensions and
their characteristics are detailed and illustrated with
examples in section 5.2.
The taxonomy visualizes an LLM component by a
feature vector comprising binary as well as multi-
valued features. Non-mutually exclusive dimensions
are represented by a set of binary features. The re-
maining dimensions are encoded as n-valued features
where ndenotes the number of characteristics. Forcompactness, we use one-letter codes of the charac-
teristics as feature values in the visualizations. In
table 2, these codes are printed in upper case in the
respective characteristic’s name.
A feature vector representing an LLM component
is visualized in one line. For dimensions with non-
mutually exclusive characteristics, all possible codes
are listed, with the applicable ones marked. The re-
maining dimensions are represented by the code of
the applicable characteristic, with the characteris-
ticnoneshown as an empty cell. We shade feature
values with different tones to support visual percep-
tion. LLM components within the same application
are grouped together, visualizing an LLM-integrating
application in a tabular format.
5.2. Dimensions and characteristics
5.2.1. Invocation dimensions
TwoInvocation dimensions address the way the LLM
is invoked within the application.
Interaction describes how the user interacts with the
LLM with three characteristics:
App: Users never converse with the LLM directly
in natural language, rather the application invokes
the LLM automatically. E.g., users do not interact
10Invocation Function Prompt Skills Out. Format Output
z}| { z }| { z }| { z }| { z }| { z }|{Interaction
Frequency
Logic
UI
Data
Instruction
State
Task
Check
reWrite
Create
conVerse
Inform
Reason
Plan
FreeText
Item
Code
Structure
Revision
Consumer
Honeycomb QueryAssistant CSA RPPUP P C PE
LowCode Planning CSA P U I P SUL
LowCode Executing DIAB PLU CVI F U
MyGrunchGpt DesignAssistant DIAB PPU V S E
MyGrunchGpt SettingsEditor CSA PPP W C E
MyGrunchGpt DomainExpert CSA PPP I F U
MatrixProduction Manager CSCIPPU I PF S L
MatrixProduction Operator ASC PPL PF S E
WorkplaceRobot CSCIPPU P C E
AutoDroid Executor CICIPLUP P IS E
AutoDroid MemoryGenerator2AIA PPP R S L
ProgPrompt ActionPlanning CSCIP U P C E
ProgPrompt ScenarioFeedback AIC PPL R I E
FactoryAssistant DSA PPU W V F U
SgpTod DstPrompter DSAIRPPU V R C E
SgpTod PolicyPrompter ASCO PPP R FI P
TruckPlatoon ASAO PPP W F U
ExcelCopilot ActionExecutor∗ASA PPL PF C E
ExcelCopilot Advisor ASA PPP R F S P
ExcelCopilot IntentDetector CSC PPU R IS P
ExcelCopilot Explainer ASA PPP R F U
Figure 1: Categorized example instances. See table 2 for a legend. ∗,2: multiple LLM components.
directly with ExcelCopilot ActionExecutor or
withMatrixProduction Operator .
Command : Users input single natural language
commands. E.g., users interact with AutoDroid
TaskExecutor through single natural language
commands.
Dialog: Usersengageinmulti-turndialogueswiththe
LLM component to achieve a use goal. E.g., users
repeatedly prompt LowCode Executing orMy-
CrunchGpt DesignAssistant in multi-turn dia-logues to obtain an essay or an airfoil design, respec-
tively.
Frequency addresses how often the application in-
vokes a specific LLM component to fulfill a goal:
Single: A single invocation of an LLM component
is sufficient to produce the result. E.g., in My-
CrunchGpt , the application internally invokes dis-
tinct LLM components once for each user input by
injecting varying prompt instructions.
Iterative: The LLM component is invoked repeatedly
to produce the result. E.g., AutoDroid TaskEx-
11ecutor is invoked multiple times to fulfill a com-
mand with an updated environment description in
theStateprompt; LowCode Executing is repeat-
edly prompted by the user to achieve the use goal
while the application updates the dialogue history.
5.2.2. Function dimensions
TheFunction dimensions are derived from the classi-
cal three-tier software architecture model which seg-
regates an application into three distinct layers: pre-
sentation, logic and data [17]. The presentation layer
implements the UI. On the input side, it allows users
to enter data and commands that control the appli-
cation. On the output side, it presents information
andprovidesfeedbackontheexecutionofcommands.
The logic layer holds the code that directly realizes
the core objectives and processes of an application
such as processing data, performing calculations, and
making decisions. The data layer of an application
manages the reading and writing of data from and
to persistent data storage. Due to its versatility, an
LLM component can simultaneously implement func-
tionality for all three layers. The taxonomy addresses
this with three Function dimensions.
UIindicateswhetheranLLMcomponentcontributes
significantly to the user interface of an application,
avoiding the need to implement graphical UI controls
or display elements:
none: No UI functionality is realized by the LLM.
E.g., in ExcelCopilot , the LLM does not replace
any UI elements.
Input: Input UI is (partially) implemented by
the LLM. E.g., in MatrixProduction Manager ,
users input their order in natural language, obviating
a product configuration GUI.
Output: Output UI is (partially) implemented by the
LLM. E.g., in TruckPlatoon , the output gener-
ated by the LLM component can replace a data cock-
pit with gauges and other visuals displaying numeri-
cal data.
Both: Input and output UI are (partially) imple-
mented by the LLM. E.g., in MyCrunchGpt , the
DesignAssistant provides a convenient conversa-
tional interface for parameterization of APIs andtools and feedback on missing values, which other-
wise might require a complex GUI.
Logicindicates whether the LLM component deter-
mines the control flow of the application. It discerns
two characteristics:
cAlculate : The output does not significantly impact
the control flow of the application, i.e., the output
is processed like data. E.g., MyCrunchGpt Set-
tingsEditor modifies a JSON file, replacing a pro-
grammed function; MyCrunchGpt DesignAssis-
tantasks the user for parameters, but the sequence
of calling APIs and tools follows a predefined work-
flow; the workflow computed by LowCode Plan-
ningis displayed without influencing the applica-
tion’s control flow.
Control: The output of the LLM is used for con-
trolling the application. E.g., the plans generated
byMatrixProduction Manager serve to sched-
ule and activate production modules; the actions pro-
posed by AutoDroid TaskExecutor are actually
executed and determine how the control flow of the
app proceeds.
Since an LLM invocation always computes a result,
cAlculate is interpreted as “calculate only”, making
cAlculate andControlmutually exclusive.
DataaddresseswhethertheLLMcontributestoread-
ing or writing persistent data:
none: The LLM does not contribute to reading or
writing persistent data. This characteristic applies
to most sample instances.
Read: TheLLMisappliedforreadingfrompersistent
data store. E.g., SgpTod DstPrompter generates
SQL queries which the application executes; Honey-
comb QueryAssistant devises analytical database
queries.
WriteandBoth: No LLM component among the
samples generates database queries for creating or
updating persistent data.
5.2.3. Prompt-related dimensions
Integrating an LLM into an application poses spe-
cific requirements for prompts, such as the need for
prompts to reliably elicit output in the requested
12form [68]. While a broad range of prompt patterns
have been identified and investigated [66], there is
still a lack of research on successful prompt pat-
terns specifically for LLM-integrated applications, on
whichthistaxonomycouldbuild. Developingprompt
taxonomiesisachallengingresearchendeavorinitself
[49] and is beyond the scope of this research. There-
fore, the taxonomy does not define a dimension with
specificpromptpatternsascharacteristics, butrather
focuses on how the application generates the prompt
for an LLM component from a technical perspective.
Prompts generally consist of several parts with dis-
tinct purposes, generated by different mechanisms.
Although many authors explore the concepts, a com-
mon terminology has yet to be established. This is
illustrated in table 3, showing terms from an ad-hoc
selection of recent papers addressing prompt gener-
ation in applications. In the table, italics indicate
that the authors refrain from introducing an abstract
term and instead use a domain-specific description.
The term “examples” indicates a one-shot orfew-shot
prompt pattern. The terms that are adopted for the
taxonomy are underlined.
The taxonomy distinguishes three prompt parts re-
ferred to as Prompt Instruction ,Prompt State , and
Prompt Task . These parts can occur in any order,
potentially interleaved, and some parts may be ab-
sent.
•Instruction is the part of a prompt that outlines
how to solve the task. Defined during LLM com-
ponent development, it remains static through-
out an application’s lifespan.
•Stateis the situation-dependent part of the
prompt that is created dynamically every time
the LLM is invoked. The taxonomy opts for the
termStateinstead of “context” in order to avoid
confusion with the “LLM context” as explained
in section 2. The Statemay include the current
dialogue history, an extract of a knowledge base
needed specifically for the current LLM invoca-
tion, or a state or scene description, etc.
•Taskis the part of the prompt conveying the
task to solve in a specific invocation.Prompt Instruction ,StateandTaskdescribe the ori-
gins of the prompt parts by uniform characteristics:
none: The prompt part is not present. E.g., Prog-
Prompt ActionPlanning has noStateprompt,
nor does LowCode Planning (except the dialogue
history when planning a subprocess). Instruction
andTaskprompt parts are present in all sample in-
stances.
User: The user phrases the prompt part. E.g., the
TaskforExcelCopilot IntentDetector or for
LowCode Planning is phrased by the user. There
are no sample instances where the user provides the
Instruction orStateprompt parts.
LLM:ThepromptpartisgeneratedbyanLLM.E.g.,
LowCode Planning generates the StateforLow-
Code Executing andExcelCopilot IntentDe-
tector generates the TaskforExcelCopilot Ac-
tionExecutor s.
Program: Application code generates the prompt
part. E.g., AutoDroid programmatically generates
theStateand theTaskparts for its MemoryGen-
erators in the knowledge base building phase.
ThePrompt Instruction dimension is always gener-
ated byProgram. While a user and possibly an LLM
have defined this prompt part during application de-
velopment, this falls outside the scope of this taxon-
omy. Therefore, the Prompt Instruction dimension is
not discriminating and categorizes all cases as Pro-
gram. Itisretainedinthetaxonomyforcompleteness
and better understandability.
Prompt Check describes whether the application em-
ploys a review mechanism to control and modify the
prompt before invoking the LLM. The same charac-
teristics as for the prompt parts are applicable:
none: The prompt is used without check.
User: The user checks and revises the prompt.
LLM: Another LLM component checks or revises the
prompt.
Program: The application comprises code to check
or revise the prompt. E.g., AutoDroid removes
personal data, such as names, to ensure privacy
before invoking the TaskExecutor ;Honeycomb
QueryAssistant incorporates a coded mechanism
against prompt injection attacks.
13Table 3: Terms used for prompt parts. Expressions specific to a domain are printed in italics, “examples” indicates a one-shot
orfew-shot prompt pattern. Terms adopted for the taxonomy are underlined.
Source Instruction State Task
[72] task description + examples test instance
[34] instruction prompt data prompt
[32] predefined prompt user prompt
[45] prompt template + examples DB schema user input question
[45] examples SQL query result
[37] prompt context, i.e., examples environment state , scene
descriptioninput task commands
[5] education prompt dialogue history user input task prompt
[5] education prompt dialogue history + provided
workflow(circumscribed)
[69] role and goal + instruction + examples context current task
[26] predefined system instruction +
domain-specific informationquery results from
knowledge graphthe user’s request
Most example instances omit prompt checks. There
are no examples where a Checkis performed by a
Useror anLLM.
5.2.4. Skills dimensions
TheSkillsdimension captures the types of LLM ca-
pabilities that an application utilizes. It is designed
as a dimension with six non-mutually exclusive char-
acteristics.
Skillsis decomposed into six specific capabilities:
reWrite: The LLM edits or transforms data or
text, such as rephrasing, summarizing, reformat-
ting, correcting, or replacing values. E.g., My-
CrunchGpt SettingsEditor replaces values in
JSON files; TruckPlatoon converts measurements
into textual explanations.
Create: The LLM generates novel output. E.g.,
LowCode Executing generates substantial bodies
of text for tasks like essay writing.
conVerse : The application relies on the LLM’s capa-
bility to engage in purposeful dialogues with humans.
E.g.,MyCrunchGpt DesignAssistant asks users
for missing parameters; SgpTod PolicyPrompter
decides how to react to user inputs and formulates
chatbot responses.Inform: The application depends on knowledge that
the LLM has acquired during its training, unlike
applications that provide all necessary information
within the prompt. E.g., MyCrunchGpt Domain-
Expert providesexpertknowledgeonairfoildesigns;
MatrixProduction relies on built-in knowledge of
production processes, such as “a hole is produced
by drilling”; LowCode Executing uses its learned
knowledge for tasks like essay writing.
Reason: The LLM draws conclusions or makes log-
ical inferences. E.g., FormulaExplainer inEx-
celCopilot explains the effects of Excel functions
in formulas; AutoDroid MemoryGenerator s ex-
plain the effects of GUI elements in Android apps.
Plan: The LLM designs a detailed method or course
of action to achieve a specific goal. E.g., Au-
toDroid TaskExecutor andWorkplaceRobot
TaskPlanning devise action plans to achieve goals.
ThePlanandReasoncharacteristics are interrelated,
as planning also requires reasoning. The intended
handling of these characteristics is to categorize an
LLM component as Planonly and understand Plan
as implicitly subsuming Reason.
The effectiveness of LLMs as components of software
applications relies on their commonsense knowledge
and their ability to correctly interpret and handle a
broad variety of text inputs, including instructions,
14examples, and code. It is reasonable to assume that a
fundamental capability, which might be termed Un-
terstand, is leveraged by every LLM component. As
it is not distinctive, the taxonomy does not list it
explicitly in the Skillsdimension.
Applying this taxonomy dimension requires users to
determine which skills are most relevant and worth
highlighting in an LLM component. Given the versa-
tilityofLLMs, reducingthefocustofewpredominant
skills is necessary to make categorizations distinctive
and expressive.
5.2.5. Output-related dimensions
Output Format characterizestheformatoftheLLM’s
output. As an output may consist of several parts in
diverse formats, this dimension is designed as non-
mutually exclusive, same as the Skillsdimension. It
distinguishes four characteristics that are distinctive
and well discernible:
FreeText : unstructured natural language text out-
put. E.g., TruckPlatoon andMyCrunchGpt
DomainExpert generate text output in natural lan-
guage; MatrixProduction Manager andMa-
trixProduction Operator produceFreeText ex-
planations complementing output in custom formats
to be parsed by the application.
Item: a single text item from a predefined set of
items, such as a class in a classification task. E.g.,
ProgPrompt ScenarioFeedback outputs either
TrueorFalse.
Code: source code or other highly formalized output
that the LLM has learned during its training, such
as a programming language, XML, or JSON. E.g.,
AutoDroid TaskExecutor producescodetosteer
an Android app; MyCrunchGpt SettingsEditor
outputs JSON.
Structure : structured, formalized output adhering to
a custom format. E.g., LowCode Planning out-
puts text in a format that can be displayed as a flow
chart; MatrixProduction Manager andOper-
atorproduce output in custom formats combined
withFreeText explanations.
Output Revision indicates whether the application
checks or revises the LLM-generated output beforeutilization. These characteristics and their interpre-
tations mirror those in the Prompt Check dimension:
none: There is no revision of the LLM output.
User: The user revises the LLM output. E.g.,
the user improves the plan generated by LowCode
Planning .
LLM: A further LLM component checks or revises
the output of the LLM component under considera-
tion.
Program: Programmed code checks or revises the
LLM output. E.g., Honeycomb QueryAssistant
corrects the query produced by the LLM before exe-
cuting it [7].
There are no instances in the sample set where an-
other LLM revises or checks the output of the LLM.
Most sample applications do not check or revise the
LLM’s output, though several of them parse and
transform it. The purpose of the Output Revision
dimension is to indicate whether the application in-
cludes control or correction mechanisms, rather than
just parsing it.
Output Consumer addresses the way of utilizing the
LLM output:
Usersignifies that the LLM output is presented to
a human user. E.g., the text output of TruckPla-
toonis intended for humans, as well as the output
ofMyCrunchGPT DomainExpert .
LLMindicates that the output serves as a prompt
part in a further LLM invocation. E.g., the knowl-
edgebaseentriesgeneratedbyan AutoDroid Mem-
oryGenerator become part of the prompt for
AutoDroid TaskExecutor ; the plan output by
LowCode Planning serves as a part of the prompt
forLowCode Executing .
Program describes instances where the LLM output
isconsumedandprocessedfurtherbyasoftwarecom-
ponent of the application. E.g., the output of Ma-
trixProduction Manager is handled by software
systems (including a Manufacturing Execution Sys-
tem) which use it to compute prompts for other LLM
components.
Enginecovers scenarios where the LLM output is in-
tended for execution on a runtime engine. E.g., the
SQL query generated by SgpTod DstPrompter is
15processed by a SQL interpreter; a part of the output
ofMatrixProduction Operator is executed by
automation modules.
Although applications may parse and transform the
LLM output before use, the Output Consumer di-
mension is meant to identify the ultimate consumer,
such as an execution engine, rather than an interme-
diary parser or transformation code. When applica-
tions divide the LLM output into parts for different
consumers, users applying the taxonomy need to de-
termine which consumer is most relevant, since this
dimension is designed to be mutually exclusive.
5.3. Evaluation
Figure 2 displays the number of occurrences of char-
acteristics within the example instances. It must
be noted, however, that these do not reflect actual
frequencies, as similar LLM components within the
same application are aggregated together, indicated
by symbols ∗and 2in figure 1. Furthermore, Ex-
celCopilot likely includes occurrences of Prompt
CheckandOutput Revision which are not counted
due to insufficient system documentation.
We evaluate the taxonomy against commonly ac-
cepted quality criteria: comprehensiveness, robust-
ness, conciseness, mutual exclusiveness, explanatory
power, and extensibility [58, 42]. The taxonomy
encompasses all example instances including those
that were not considered during its development.
Thisdemonstrates comprehensiveness . Asfigure1
shows, all example instances have unique categoriza-
tions, supporting the taxonomy’s robustness . This
not only indicates that the dimensions and charac-
teristics are distinctive for the domain, but also high-
lightsthewidevarietypossibleinthisfield. Concise-
nessdemands that the taxonomy uses the minimum
number of dimensions and characteristics. The tax-
onomy gains conciseness by identifying relatively few
and abstract characteristics within each dimension.
However, it does not adhere to the related subcri-
terion that each characteristic must be present in at
leastoneinvestigatedinstance[54]. Unoccupiedchar-
acteristics are retained for dimensions whose char-
acteristics were derived conceptually, specifically, forthePromptdimensions, the Output Revision dimen-
sion, and the Data Function dimension, enhancing
the taxonomy’s ability to illustrate design options
and inspire novel uses for LLM integrations in ap-
plications. Some dimensions are constructed in par-
allel, sharing common sets of characteristics. While
this affects conciseness, it makes the taxonomy easier
to understand and apply. As is often seen in tax-
onomy development [54], we deliberately waived the
requirement for mutual exclusiveness for some di-
mensions, specifically the Output Format andSkills
dimensions. In the context of this taxonomy, these
can equivalently be understood as a set of of six
and four binary dimensions respectively, each divided
into characteristics “yes” and “no”. However, framing
them as a single dimension with non-mutually exclu-
sive characteristics seems more intuitive.
Metadimensions structure the taxonomy, and most
of the characteristics are illustrated through exam-
ples. These measures are recognized for enhancing
theexplanatory power of a taxonomy [58]. The
taxonomy’s flat structure allows for the easy addition
of dimensions and characteristics, indicating that its
extensibility is good. Potential extensions and fur-
ther aspects of the taxonomy, including its usefulness
and ease of use, are discussed in section 6.
We visualize the taxonomy (or, strictly speaking, cat-
egorized instances) in a compact form using feature
vectors with characteristics abbreviated to single-
letter codes. This approach has a drawback, as
it requires referencing a legend. Additionally, non-
applicable characteristics in mutually exclusive di-
mensions are not visible, which means the design
space is not completely shown. However, the com-
pactness of the representation allows LLM compo-
nents within a common application to be grouped
closely, so that an LLM-integrated application can
be perceived as a unit without appearing convoluted.
This is a significant advantage for our purposes.
6. Discussion
The discussion first focuses on the taxonomy’s appli-
cability and ease of use before considering its overall
usefulness.
16Invocation Function Prompt Output Output
z}| { z }| { z }| { Skills Format z }| {
Inter. Freq. Logic UI Data Instr. State Task Check z }| { z }|{Revision Consumer
A C D I SC AI O BR W B U L P U L P U L P U L PW C V I R P F I C S U L P U L P E
8 9 4 5 168 135 2 2 2 0 0 0 0 21 0 2 1711 3 7 0 0 2 3 1 4 4 7 8 10 4 6 8 1 0 1 5 3 3 10
Figure 2: Occurrences of characteristics in the sample set of LLM-integrated applications.
6.1. Applicability and ease of use
The taxonomy was effectively applied to LLM-
integrated applications based on research papers,
source code blog posts, recorded software demonstra-
tions, and developer experiences. The analysis of
LowCode revealed it to be a prompt definition tool
combined with an LLM-based chatbot, which devi-
ates from the strict definition of an LLM-integrated
application. Still, the taxonomy provided an effective
categorization and led to a clear understanding of the
system’s architecture.
Obviously, the ease of categorization depends on the
clarity and comprehensiveness of the available infor-
mation, which varies across analyzed systems. An-
alyzing applications of LLMs in novel and uncom-
mon domains can be challenging. While these papers
present inspiring and innovative ideas for LLM inte-
gration, such as MyCrunchGpt andTruckPla-
toon, they may prioritize explaining the application
areaandstruggletodetailthetechnicalaspectsofthe
LLM integration. A taxonomy for LLM-integrated
applications can guide and facilitate the writing pro-
cess and lead to more standardized and comparable
descriptions.
Applying the taxonomy is often more straightforward
for research-focused systems. Omitting the com-
plexities required for real-world applications, such as
prompt checks and output revisions, their architec-
tures are simpler and easier to describe. A taxonomy
can point out such omissions.
A fundamental challenge in applying the taxonomy
arises from the inherent versatility of LLMs, which
allows to define LLM components serving multiple
purposes. This is exemplified by SgpTod Poli-cyPrompter , where the prompt is designed to pro-
duce a structure with two distinct outcomes (a class
label and a chatbot response), and similarly by Ma-
trixProduction , as detailed section 4.2. Draw-
ing an analogy to “function overloading” in classical
programming, such LLM components can be termed
“overloaded LLM components”.
A taxonomy can handle overloaded LLM components
in several ways: (1) define more dimensions as non-
mutually exclusive, (2) label overloaded LLM compo-
nents as“overloaded” withoutamoredetailedcatego-
rization, or (3) categorize them by their predominant
purpose or output. While the first approach allows
for the most precise categorization, it complicates the
taxonomy. Moreover, it will likely result in nearly all
characteristics being marked for some LLM compo-
nents, which is ultimately not helpful. The second
approachsimplifiescategorizationbutsacrificesmuch
detail. Our taxonomy adopts the third approach, en-
forcing simplification and abstraction in descriptions
of overloaded LLM components while retaining es-
sential detail. The taxonomy can easily be extended
to include approach (2) as an additional binary di-
mension.
6.2. Usefulness
The search for instances of LLM-integrated appli-
cations uncovered activities across various domains.
Substantial research involving LLM integrations, of-
ten driven by theoretical interests, is notable in robot
task planning [37, 51, 61, 33, 63] and in the TOD
field [23, 71, 4, 6, 56]. Research exploring LLM po-
tentials from a more practical perspective can be
found in novel domains, such as industrial produc-
tion [69, 26] and other technical areas [28, 70]. Fur-
17thermore, developers of commercial LLM-based ap-
plications are beginning to communicate their efforts
and challenges [44, 7]. The taxonomy has been ap-
plied to example instances from these and additional
areas. This demonstrates its potential as a common,
unified framework for describing LLM-integrated ap-
plications, facilitating the comparison and sharing
of development knowledge between researchers and
practitioners across various domains.
When applying the taxonomy to the example in-
stances, it proved to be effective and useful as an
analytical lens. Descriptions of LLM-integrated ap-
plications commonly explain background information
and details of the application domain in addition to
its LLM integration. When used as an analytical
lens, the taxonomy quickly directs the analysis to-
wards the aspects of LLM integration, abstracting
from the specificities of the domain.
ThetaxonomydescribeshowLLMcapabilitiescanbe
leveraged in software systems, offers inspiration for
LLM-based functions, and outlines options for their
implementation as follows. The Skillsdimension out-
lines the range of capabilities an LLM can contribute
to an application through a concise set of characteris-
tics, while the Function dimension suggests potential
uses, furthersupportedbythe Interaction dimension.
TheOutput Type dimension indicates options for en-
coding the output of an LLM in formats beyond plain
text, making it processable by software. The Output
Consumer dimension illustrates the diverse ways to
utilizeoractuponLLMoutput. Thus, thetaxonomy,
as intended, spans a design space for LLM integra-
tions.
The sampled LLM-integrated applications showcase
the creativity of researchers and developers in ap-
plying and exploiting the potentials of LLMs, rang-
ing from straightforward solutions (e.g., TruckPla-
toon) to highly sophisticated and technically com-
plex ones (e.g., AutoDroid ). When using the tax-
onomy to inspire innovative uses of LLMs, we recom-
mend supplementing it with descriptions of example
applicationstoenhanceitsillustrativeness. Thechar-
acteristics of the Skillsdimension are derived prag-
matically from the investigated example instances.
While they do not claim to be exhaustive or deeplyrooted in LLM theory or cognitive science, they add
relevant details to the categorizations and illustrate
design options and potentials for using LLMs as soft-
ware components.
It emerged as a key insight of this research that,
rather than analyzing an LLM-integrated application
in whole, analysis should start with the identifica-
tion and description of its distinct LLM components.
This is essential for gaining a clear understanding of
how the application utilizes the capabilities of LLMs.
The LLM-integrated application then manifests as a
combinationofitsLLMcomponents. Asshowninfig-
ure 1, the visualization effectively displays both the
quantity and the variety of LLM components in an
LLM-integrated application.
LLM components interact through prompt chaining,
where one LLM component’s output feeds into an-
other’s input [67]. When an LLM-integrated applica-
tion involves such an interaction, the taxonomy rep-
resents it as an LLMcharacteristic within a Prompt
dimension. The taxonomy can capture the variance
in these interactions. For instance, in AutoDroid
TaskExecutor andLowCode Executing , the
LLMcharacteristic appears in the Prompt State di-
mension, because their prompt components (knowl-
edge base excerpts and prompt definition, respec-
tively) are generated by other LLM components in a
preparatory stage. In contrast, the LLMcharacter-
istic appears in the Prompt Task dimension for Ma-
trixProduction Operator , because its prompt
part is generated individually by the MatrixPro-
duction Manager almost immediately before use.
Taxonomy dimensions that cover entire LLM-
integrated applications may be useful. Given their
complexity, these dimensions should be designed
basedonabroaderrangeofexamples, whichwillonly
become available as more LLM-integrated applica-
tions are developed and their architectures disclosed
in the future. Extensions to the taxonomy could
also include dimensions for describing the structure
of prompts in more detail, as well as dimensions ad-
dressing characteristics of the language models used.
18Table 4: LLM usage in the sample instances. “Evals” indicates evaluations of various LLMs.
Application Used or best LLM Evals Comments
Honeycomb GPT-3.5 yes GPT-4 far too slow
LowCode GPT-3.5-turbo
MyCrunchGpt GPT-3.5 then awaiting the publication of GPT-4
MatrixProduction text-davinci-003
WorkplaceRobot GPT-3
AutoDroid GPT-4 yes GPT-4 best for tasks requiring many steps
ProgPrompt GPT-3 CODEX better, but access limits prohibitive
FactoryAssistants GPT-3.5
SgpTod GPT-3.5 yes GPT-3.5 best more often than others combined
TruckPlatoon GPT-3.5-turbo
ExcelCopilot N/A combined LLMs in Copilot for Microsoft 365 [43]
7. Conclusion
This paper investigates the use of LLMs as soft-
ware components. Its perspective differs from cur-
rent software engineering research, which investigates
LLMs as tools for software development [14, 22] and
fromresearchexaminingLLMsasautonomousagents
[11, 62, 57, 21]. This paper defines the concept of an
LLM component as a software component that re-
alizes its functionality by invoking an LLM. While
LLM components implicitly appear in various works,
termed, for example, “prompters”, “prompted LLM”,
“prompt module”, or “module” [30, 71, 6, 7], to our
knowledge, this concept has not yet been formalized
or systematically investigated.
The main contribution of this study is a taxonomy
for the analysis and description of LLM components,
extending to LLM-integrated applications by charac-
terizing them as combinations of LLM components.
In addition to the dimensions and characteristics of
the taxonomy, the study contributes a taxonomy vi-
sualization based on feature vectors, which is more
compact than the established visualizations such as
morphological boxes [55] or radar charts. It repre-
sentsanLLM-integratedapplicationasonevisualen-
tity in a tabular format, with its LLM components
displayed as rows.
The taxonomy was constructed using established
methods, based on a set of example instances, and
evaluated with a new set of example instances. Thecombined samples exhibit broad variation along the
identified dimensions. For some instances, informa-
tion was not available, necessitating speculative in-
terpretation. However, since the sample is used for
identifying options rather than quantitative analysis,
this issue and the representativeness of the sample
are not primary concerns. The evaluation was con-
ducted by the developer of the taxonomy, consistent
with recent related work [21, 52, 48]. Using a new
sample for evaluation strengthens the validity of the
results.
A further significant contribution of the paper is a
systematic overview of a sample of LLM-integrated
applications across various industrial and technical
domains, illustrating a spectrum of conceptual ideas
and implementation options.
As the examples show, LLM components can re-
place traditionally coded functions in software sys-
tems and enable novel use cases. However, practi-
cal challenges persist. Developers report that new
software engineering methods are required, e.g., for
managing prompts as software assets and for test-
ing and monitoring applications. For instance, the
costs of LLM invocations prohibit the extensive au-
tomated testing that is standard in software devel-
opment practice [44, 7]. Challenges also arise from
the inherent indeterminism and uncontrollability of
LLMs. Small variations in prompts can lead to differ-
ences in outputs, while automated output processing
19in LLM-integrated applications requires the output
to adhere to a specified format.
Furthermore, the deployment mode of LLMs,
whether local (on the same hardware as the ap-
plication) or remote, managed privately or offered
as Language-Models-as-a-Service (LMaaS), has im-
pact on performance and usability. Table 4 gives an
overview of the LLMs used in our sample of appli-
cations. Where papers report evaluations of mul-
tiple LLMs, the table displays the chosen or best-
performing LLM. Although not representative, the
table provides some insights. LMaaS dominates,
likely due to its convenience, but more importantly,
due to the superior performance of the provided
LLMs.
Concerns regarding LMaaS include privacy, as sensi-
tive data might be transmitted to the LLM through
the prompt [64], and service quality, i.e., reliability,
availability, and costs. Costs typically depend on the
quantity of processed tokens. This quantity also af-
fects latency, which denotes the processing time of
an LLM invocation. A further important factor for
latency is the size of the LLM, with larger models
being slower [7].
When building LLM-based applications for real-
worlduse, thereliabilityandavailabilityofanLMaaS
are crucial. Availability depends not only on the
technical stability of the service, but also on factors
such as increased latency during high usage periods
or usage restrictions imposed by the provider of an
LMaaS, as reported for ProgPrompt [51]. Beyond
technical aspects, the reliability of an LMaaS also en-
compassesitsbehavior. Forinstance,providersmight
modify a model to enhance its security, potentially
impacting applications that rely on it.
Despite practical challenges, integrating LLMs into
systems has the potential to alter the way software
is constructed and the types of systems that can be
realized. Prompts are central to the functioning of
LLM components which pose specific requirements
such as strict format adherence. Therefore, an im-
portant direction for future research will be prompt
engineering specifically tailored for LLM-integrated
applications.In future work, the taxonomy will be extended to
distinguish finer-grained parts of prompts, allowing a
more detailed description and comparison of prompts
andrelatedexperimentalresults. Initialstudiesshare
results on the format-following behavior of LLMs [68]
as a subtopic of instruction-following [73], derived
with synthetic benchmark data. It is necessary to
complementtheirresultswithexperimentsusingdata
and tasks from real application development projects
because, in the early stages of this field, synthetic
benchmarks may fail to cover relevant aspects within
the wide range of possible options. Another crucial
research direction involves exploring how LLM char-
acteristics correspond to specific tasks, such as de-
termining the optimal LLM size for intent detection
tasks. Thetaxonomydevelopedinthisstudycansys-
tematize such experiments and their outcomes. Ad-
ditionally, it provides a structured framework for de-
lineating design choices in LLM components, making
it a valuable addition to future training materials.
Acknowledgements
SpecialthankstoAntoniaWeberandConstantinWe-
berforproofreadingandprovidinginsightfulandcon-
structive comments.
References
[1] EleniAdamopoulouandLefterisMoussiades. An
Overview of Chatbot Technology. In Ilias Ma-
glogiannis, Lazaros Iliadis, and Elias Pimeni-
dis, editors, Artificial Intelligence Applications
and Innovations , IFIP Advances in Information
and Communication Technology, pages 373–383,
Cham, 2020. Springer International Publishing.
doi:10.1007/978-3-030-49186-4_31.
[2] Sebastian Bader, Erich Barnstedt, Heinz Be-
denbender, Bernd Berres, Meik Billmann, and
Marko Ristin. Details of the asset adminis-
tration shell-part 1: The exchange of informa-
tion between partners in the value chain of in-
dustrie 4.0 (version 3.0 rc02). Working Paper,
Berlin: Federal Ministry for Economic Affairs
20and Climate Action (BMWK), 2022. doi.org/
10.21256/zhaw-27075 .
[3] Marcos Baez, Florian Daniel, Fabio Casati, and
Boualem Benatallah. Chatbot integration in few
patterns. IEEE Internet Computing , pages 1–1,
2020. doi:10.1109/MIC.2020.3024605.
[4] Tom Bocklisch, Thomas Werkmeister,
Daksh Varshneya, and Alan Nichol. Task-
Oriented Dialogue with In-Context Learn-
ing. (arXiv:2402.12234), February 2024.
doi:10.48550/arXiv.2402.12234.
[5] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Ze-
hua Wang, Yaobo Liang, Tao Ge, Chenfei Wu,
Wang You, Ting Song, Yan Xia, Jonathan Tien,
and Nan Duan. Low-code LLM: Visual Pro-
grammingoverLLMs. (arXiv:2304.08103), April
2023. doi:10.48550/arXiv.2304.08103.
[6] Lang Cao. DiagGPT: An LLM-based Chatbot
with Automatic Topic Management for Task-
Oriented Dialogue. (arXiv:2308.08043), August
2023. doi:10.48550/arXiv.2308.08043.
[7] Phillip Carter. All the Hard Stuff No-
body Talks About When Building Prod-
ucts with LLMs . Honeycomb, May
2023. https://www.honeycomb.io/blog/
hard-stuff-nobody-talks-about-llm .
[8] Phillip Carter. So We Shipped an AI Prod-
uct. Did It Work? Honeycomb, Octo-
ber 2023. https://www.honeycomb.io/blog/
we-shipped-ai-product .
[9] Banghao Chen, Zhaofeng Zhang, Nicolas
Langrené, and Shengxin Zhu. Unleash-
ing the potential of prompt engineering in
Large Language Models: A comprehensive
review. (arXiv:2310.14735), October 2023.
doi:10.48550/arXiv.2310.14735.
[10] Wang Chen, Yan-yi Liu, Tie-zheng Guo, Da-
peng Li, Tao He, Li Zhi, Qing-wen Yang,
Hui-han Wang, and Ying-you Wen. Sys-
tems engineering issues for industry appli-
cations of large language model. AppliedSoft Computing , 151:111165, January 2024.
doi:10.1016/j.asoc.2023.111165.
[11] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang,
Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao
Wang, Zekai Wang, Feng Yin, Junhua Zhao, and
Xiuqiang He. Exploring Large Language Model
based Intelligent Agents: Definitions, Methods,
and Prospects. (arXiv:2401.03428), January
2024. doi:10.48550/arXiv.2401.03428.
[12] Silvia Colabianchi, Andrea Tedeschi, and
Francesco Costantino. Human-technology in-
tegration with industrial conversational agents:
A conceptual architecture and a taxonomy for
manufacturing. Journal of Industrial Infor-
mation Integration , 35:100510, October 2023.
doi:10.1016/j.jii.2023.100510.
[13] Jonathan Evertz, Merlin Chlosta, Lea Schön-
herr, and Thorsten Eisenhofer. Whispers in
the Machine: Confidentiality in LLM-integrated
Systems. (arXiv:2402.06922), February 2024.
doi:10.48550/arXiv.2402.06922.
[14] Angela Fan, Beliz Gokkaya, Mark Harman,
Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo,
and Jie M. Zhang. Large Language Models
for Software Engineering: Survey and Open
Problems. (arXiv:2310.03533), November 2023.
doi:10.48550/arXiv.2310.03533.
[15] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing
Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei
Wang, Xiangyu Zhao, Jiliang Tang, and Qing
Li. Recommender Systems in the Era of Large
Language Models (LLMs). (arXiv:2307.02046),
August 2023. doi:10.48550/arXiv.2307.02046.
[16] David Fortin. Microsoft Copilot in Excel:
What It Can and Can’t Do . YouTube, Jan-
uary 2024. https://www.youtube.com/watch?
v=-fsu9IXMZvo .
[17] Martin Fowler. Patterns of Enterprise Applica-
tion Architecture . 2002. ISBN 978-0-321-12742-
6.
21[18] Shirley Gregor. The nature of theory in infor-
mation systems. MIS quarterly , pages 611–642,
2006. doi:10.2307/25148742.
[19] Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu
Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jin-
jie Gu, and Chenyi Zhuang. Intelligent Vir-
tual Assistants with LLM-based Process Au-
tomation. (arXiv:2312.06677), December 2023.
doi:10.48550/arXiv.2312.06677.
[20] Muhammad Usman Hadi, Qasem Al Tashi,
Rizwan Qureshi, Abbas Shah, Amgad Muneer,
Muhammad Irfan, Anas Zafar, Muhammad Bi-
lal Shaikh, Naveed Akhtar, Jia Wu, and Seyedali
Mirjalili. Large Language Models: A Compre-
hensive Survey of its Applications, Challenges,
Limitations, and Future Prospects, September
2023. doi:10.36227/techrxiv.23589741.v3.
[21] Thorsten Händler. A Taxonomy for Au-
tonomous LLM-Powered Multi-Agent Architec-
tures:. In Proceedings of the 15th Interna-
tional Joint Conference on Knowledge Discov-
ery, Knowledge Engineering and Knowledge
Management , pages 85–98, Rome, Italy, 2023.
SCITEPRESS - Science and Technology Publi-
cations. doi:10.5220/0012239100003598.
[22] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang,
KailongWang, LiLi, XiapuLuo, DavidLo, John
Grundy, and Haoyu Wang. Large Language
Models for Software Engineering: A Systematic
Literature Review. (arXiv:2308.10620), Septem-
ber 2023. doi:10.48550/arXiv.2308.10620.
[23] Vojtěch Hudeček and Ondrej Dusek. Are
Large Language Models All You Need for Task-
Oriented Dialogue? In Svetlana Stoyanchev,
Shafiq Joty, David Schlangen, Ondrej Dusek,
Casey Kennington, and Malihe Alikhani, edi-
tors,Proceedings of the 24th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue,pages216–228,Prague,Czechia,Septem-
ber 2023. Association for Computational Lin-
guistics. doi:10.18653/v1/2023.sigdial-1.21.[24] Kevin Maik Jablonka, Qianxiang Ai, Alexander
Al-Feghali, ShrutiBadhwar, JoshuaD.Bocarsly,
Andres M. Bran, Stefan Bringuier, Catherine L.
Brinson, Kamal Choudhary, Defne Circi, Sam
Cox, Wibe A. de Jong, Matthew L. Evans, Nico-
las Gastellu, Jerome Genzling, María Victoria
Gil, Ankur K. Gupta, Zhi Hong, Alishba Im-
ran, Sabine Kruschwitz, Anne Labarre, Jakub
Lála, Tao Liu, Steven Ma, Sauradeep Majum-
dar, Garrett W. Merz, Nicolas Moitessier, Elias
Moubarak, Beatriz Mouriño, Brenden Pelkie,
Michael Pieler, Mayk Caldas Ramos, Bojana
Ranković, Samuel Rodriques, Jacob Sanders,
Philippe Schwaller, Marcus Schwarting, Jiale
Shi, Berend Smit, Ben Smith, Joren Van Herck,
Christoph Völker, Logan Ward, Sean War-
ren, Benjamin Weiser, Sylvester Zhang, Xiaoqi
Zhang, Ghezal Ahmad Zia, Aristana Scour-
tas, K. Schmidt, Ian Foster, Andrew White,
and Ben Blaiszik. 14 examples of how LLMs
can transform materials science and chem-
istry: A reflection on a large language model
hackathon. Digital Discovery , 2(5):1233–1250,
2023. doi:10.1039/D3DD00113J.
[25] Jean Kaddour, Joshua Harris, Maximilian
Mozes, Herbie Bradley, Roberta Raileanu, and
Robert McHardy. Challenges and Applica-
tions of Large Language Models, July 2023.
doi:10.48550/arXiv.2307.10169.
[26] Samuel Kernan Freire, Mina Foosherian, Chao-
fan Wang, and Evangelos Niforatos. Harnessing
Large Language Models for Cognitive Assistants
in Factories. In Proceedings of the 5th Interna-
tional Conference on Conversational User Inter-
faces, CUI ’23, pages 1–6, New York, NY, USA,
July 2023. Association for Computing Machin-
ery. doi:10.1145/3571884.3604313.
[27] Anis Koubaa, Wadii Boulila, Lahouari Ghouti,
Ayyub Alzahem, and Shahid Latif. Explor-
ing ChatGPT Capabilities and Limitations: A
Survey.IEEE Access , 11:118698–118721, 2023.
doi:10.1109/ACCESS.2023.3326474.
[28] Varun Kumar, Leonard Gleyzer, Adar Ka-
hana, Khemraj Shukla, and George Em Karni-
22adakis. MyCrunchGPT:ALLMAssistedFrame-
work for Scientific Machine Learning. Jour-
nal of Machine Learning for Modeling and
Computing , 4(4), 2023. doi.org/10.1615/
JMachLearnModelComput.2023049518 .
[29] Dennis Kundisch, Jan Muntermann,
Anna Maria Oberländer, Daniel Rau, Maxi-
milian Röglinger, Thorsten Schoormann, and
Daniel Szopinski. An Update for Taxonomy
Designers. Business & Information Systems
Engineering , 64(4):421–439, August 2022.
doi:10.1007/s12599-021-00723-x.
[30] Gibbeum Lee, Volker Hartmann, Jongho
Park, Dimitris Papailiopoulos, and Kang-
wook Lee. Prompted LLMs as chatbot
modules for long open-domain conversation.
In Anna Rogers, Jordan Boyd-Graber, and
Naoaki Okazaki, editors, Findings of the as-
sociation for computational linguistics: ACL
2023, pages 4536–4554, Toronto, Canada, July
2023.AssociationforComputationalLinguistics.
doi:10.18653/v1/2023.findings-acl.277.
[31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng-
bao Jiang, Hiroaki Hayashi, and Graham Neu-
big. Pre-train, Prompt, and Predict: A Sys-
tematic Survey of Prompting Methods in Nat-
ural Language Processing. ACM Comput-
ing Surveys , 55(9):195:1–195:35, January 2023.
doi:10.1145/3560815.
[32] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang,
Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan
Zheng, and Yang Liu. Prompt Injection at-
tack against LLM-integrated Applications, June
2023. doi:10.48550/arXiv.2306.05499.
[33] Yuchen Liu, Luigi Palmieri, Sebastian
Koch, Ilche Georgievski, and Marco Aiello.
DELTA: Decomposed Efficient Long-Term
Robot Task Planning using Large Language
Models. (arXiv:2404.03275), April 2024.
doi:10.48550/arXiv.2404.03275.
[34] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan
Jia, and Neil Zhenqiang Gong. Prompt Injec-
tion Attacks and Defenses in LLM-IntegratedApplications. (arXiv:2310.12815), October2023.
doi:10.48550/arXiv.2310.12815.
[35] Shaoguang Mao, Qiufeng Yin, Yuzhe Cai,
and Dan Qiao. LowCodeLLM. https:
//github.com/chenfei-wu/TaskMatrix/
tree/main/LowCodeLLM , May 2023.
[36] Scott McLean, Gemma J. M. Read, Jason
Thompson,ChrisBaber,NevilleA.Stanton,and
Paul M. Salmon. The risks associated with Ar-
tificial General Intelligence: A systematic re-
view.Journal of Experimental & Theoretical
Artificial Intelligence , 35(5):649–663, July 2023.
doi:10.1080/0952813X.2021.1964003.
[37] Oier Mees, Jessica Borja-Diaz, and Wolfram
Burgard. Grounding Language with Visual Af-
fordances over Unstructured Data. In 2023
IEEE International Conference on Robotics
and Automation (ICRA) , pages 11576–11582,
London, United Kingdom, May 2023. IEEE.
doi:10.1109/ICRA48891.2023.10160396.
[38] Grégoire Mialon, Roberto Dessì, Maria
Lomeli, Christoforos Nalmpantis, Ram Pa-
sunuru, Roberta Raileanu, Baptiste Rozière,
Timo Schick, Jane Dwivedi-Yu, Asli Ce-
likyilmaz, Edouard Grave, Yann LeCun,
and Thomas Scialom. Augmented Lan-
guage Models: A Survey, February 2023.
doi:10.48550/arXiv.2302.07842.
[39] Melanie Mitchell. Debates on the na-
ture of artificial general intelligence. Sci-
ence, 383(6689):eado7069, March 2024.
doi:10.1126/science.ado7069.
[40] Quim Motger, Xavier Franch, and Jordi Marco.
Software-Based Dialogue Systems: Survey,
Taxonomy, and Challenges. ACM Comput-
ing Surveys , 55(5):91:1–91:42, December 2022.
doi:10.1145/3527450.
[41] Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan
Cai, Keng Siau, and Langtao Chen. Gen-
erative AI and ChatGPT: Applications, chal-
lenges, and AI-human collaboration. Jour-
23nal of Information Technology Case and Ap-
plication Research , 25(3):277–304, July 2023.
doi:10.1080/15228053.2023.2233814.
[42] Robert C Nickerson, Upkar Varshney, and
Jan Muntermann. A method for taxon-
omy development and its application in in-
formation systems. European Journal of In-
formation Systems , 22(3):336–359, May 2013.
doi:10.1057/ejis.2012.26.
[43] Camille Pack, Cern McAtee, Samantha Robert-
son, Dan Brown, Aditi Srivastava, and Kweku
Ako-Adjei. Microsoft Copilot for Microsoft
365 overview. https://learn.microsoft.
com/en-us/copilot/microsoft-365/
microsoft-365-copilot-overview , March
2024.
[44] Chris Parnin, Gustavo Soares, Rahul Pan-
dita, Sumit Gulwani, Jessica Rich, and
Austin Z. Henley. Building Your Own Prod-
uct Copilot: Challenges, Opportunities, and
Needs. (arXiv:2312.14231), December 2023.
doi:10.48550/arXiv.2312.14231.
[45] Rodrigo Pedro, Daniel Castro, Paulo Car-
reira, and Nuno Santos. From Prompt In-
jections to SQL Injection Attacks: How Pro-
tected is Your LLM-Integrated Web Appli-
cation? (arXiv:2308.01990), August 2023.
doi:10.48550/arXiv.2308.01990.
[46] Ken Peffers, Tuure Tuunanen, Marcus A.
Rothenberger, and Samir Chatterjee. A De-
sign Science Research Methodology for Infor-
mation Systems Research. Journal of Man-
agement Information Systems , 24(3):45–77, De-
cember 2007. ISSN 0742-1222, 1557-928X.
doi:10.2753/MIS0742-1222240302.
[47] Mohaimenul Azam Khan Raiaan, Md. Sad-
dam Hossain Mukta, Kaniz Fatema, Nur Mo-
hammad Fahad, Sadman Sakib, Most Mar-
ufatul Jannat Mim, Jubaer Ahmad, Mo-
hammed Eunus Ali, and Sami Azam. A Review
on Large Language Models: Architectures, Ap-
plications, Taxonomies, Open Issues and Chal-lenges. IEEE Access , 12:26839–26874, 2024.
doi:10.1109/ACCESS.2024.3365742.
[48] Jack Daniel Rittelmeyer and Kurt Sandkuhl.
Morphological Box for AI Solutions: Evalua-
tion and Refinement with a Taxonomy Develop-
mentMethod. InKnutHinkelmann, FranciscoJ.
López-Pellicer, and Andrea Polini, editors, Per-
spectives in Business Informatics Research , Lec-
ture Notes in Business Information Process-
ing, pages 145–157, Cham, 2023. Springer Na-
ture Switzerland. doi:10.1007/978-3-031-43126-
5_11.
[49] Shubhra Kanti Karmaker Santu and Dongji
Feng. TELeR: A General Taxonomy of
LLM Prompts for Benchmarking Complex
Tasks. (arXiv:2305.11430), October 2023.
doi:10.48550/arXiv.2305.11430.
[50] Thorsten Schoormann, Frederik Möller, and
Daniel Szopinski. Exploring Purposes of Us-
ing Taxonomies. In Proceedings of the Inter-
national Conference on Wirtschaftsinformatik
(WI), Nuernberg, Germany, February 2022.
[51] Ishika Singh, Valts Blukis, Arsalan Mousa-
vian, Ankit Goyal, Danfei Xu, Jonathan Trem-
blay, Dieter Fox, Jesse Thomason, and Ani-
mesh Garg. ProgPrompt: Generating Situated
Robot Task Plans using Large Language Mod-
els. In2023 IEEE International Conference on
Robotics and Automation (ICRA) , pages 11523–
11530, London, United Kingdom, May 2023.
IEEE. doi:10.1109/ICRA48891.2023.10161317.
[52] Gero Strobel, Leonardo Banh, Frederik Möller,
and Thorsten Schoormann. Exploring Gener-
ative Artificial Intelligence: A Taxonomy and
Types. In Proceedings of the 57th Hawaii Inter-
national Conference on System Sciences , Hon-
olulu, Hawaii, January 2024. https://hdl.
handle.net/10125/106930 .
[53] Hendrik Strobelt, Albert Webson, Victor Sanh,
Benjamin Hoover, Johanna Beyer, Hanspeter
Pfister, and Alexander M. Rush. Interac-
tive and Visual Prompt Engineering for Ad-
hoc Task Adaptation With Large Language
24Models. IEEE Transactions on Visualization
and Computer Graphics , pages 1–11, 2022.
doi:10.1109/TVCG.2022.3209479.
[54] Daniel Szopinski, Thorsten Schoormann, and
DennisKundisch. CriteriaasaPreludeforGuid-
ing Taxonomy Evaluation. In Proceedings of the
53rd Hawaii International Conference on Sys-
tem Sciences , 2020. https://hdl.handle.net/
10125/64364 .
[55] Daniel Szopinski, Thorsten Schoormann, and
Dennis Kundisch. Visualize different: To-
wards researching the fit between taxon-
omy visualizations and taxonomy tasks. In
Tagungsband Der 15. Internationalen Tagung
Wirtschaftsinformatik (WI 2020) , Potsdam,
2020. doi:10.30844/wi_2020_k9-szopinski.
[56] Manisha Thakkar and Nitin Pise. Unified Ap-
proach for Scalable Task-Oriented Dialogue Sys-
tem.International Journal of Advanced Com-
puter Science and Applications , 15(4), 2024.
doi:10.14569/IJACSA.2024.01504108.
[57] Oguzhan Topsakal and Tahir Cetin Akinci. Cre-
ating Large Language Model Applications Uti-
lizing Langchain: A Primer on Developing LLM
Apps Fast. In International Conference on
Applied Engineering and Natural Sciences , vol-
ume 1, pages 1050–1056, 2023.
[58] Michael Unterkalmsteiner and Waleed Adbeen.
A compendium and evaluation of taxonomy
quality attributes. Expert Systems , 40(1):
e13098, 2023. doi:10.1111/exsy.13098.
[59] Bryan Wang, Gang Li, and Yang Li. En-
abling Conversational Interaction with Mo-
bile UI using Large Language Models. In
Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems , CHI
’23, pages 1–17, New York, NY, USA, April
2023. Association for Computing Machinery.
doi:10.1145/3544548.3580895.
[60] Can Wang, Bolin Zhang, Dianbo Sui, Zhiying
Tu, Xiaoyu Liu, and Jiabao Kang. A Survey onEffective Invocation Methods of Massive LLM
Services. (arXiv:2402.03408), February 2024.
doi:10.48550/arXiv.2402.03408.
[61] Jun Wang, Guocheng He, and Yiannis Kan-
taros. Safe Task Planning for Language-
Instructed Multi-Robot Systems using Confor-
mal Prediction. (arXiv:2402.15368), February
2024. doi:10.48550/arXiv.2402.15368.
[62] Lei Wang, Chen Ma, Xueyang Feng, Zeyu
Zhang, Hao Yang, Jingsen Zhang, Zhiyuan
Chen, Jiakai Tang, Xu Chen, Yankai Lin,
Wayne Xin Zhao, Zhewei Wei, and Jirong
Wen. A survey on large language model
based autonomous agents. Frontiers of Com-
puter Science , 18(6):186345, March 2024.
doi:10.1007/s11704-024-40231-1.
[63] Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu
Zhang, Ying Nian Wu, Song-Chun Zhu, and
Hangxin Liu. LLM3:Large Language Model-
based Task and Motion Planning with Motion
Failure Reasoning. (arXiv:2403.11552), March
2024. doi:10.48550/arXiv.2403.11552.
[64] Hao Wen, Yuanchun Li, Guohong Liu, Shan-
hui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang,
Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Em-
powering LLM to use Smartphone for Intelligent
Task Automation. (arXiv:2308.15272), Septem-
ber 2023. doi:10.48550/arXiv.2308.15272.
[65] Hao Wen, Yuanchun Li, and Sean KiteFly-
Kid. MobileLLM/AutoDroid. MobileLLM,Jan-
uary 2024. https://github.com/MobileLLM/
AutoDroid .
[66] Jules White, Quchen Fu, Sam Hays, Michael
Sandborn, Carlos Olea, Henry Gilbert, Ashraf
Elnashar, Jesse Spencer-Smith, and Dou-
glas C. Schmidt. A Prompt Pattern Cat-
alog to Enhance Prompt Engineering with
ChatGPT. (arXiv:2302.11382), February 2023.
doi:10.48550/arXiv.2302.11382.
[67] Tongshuang Wu, Michael Terry, and Car-
rie Jun Cai. AI Chains: Transparent and
25Controllable Human-AI Interaction by Chain-
ing Large Language Model Prompts. In
Proceedings of the 2022 CHI Conference on
Human Factors in Computing Systems , CHI
’22, pages 1–22, New York, NY, USA, April
2022. Association for Computing Machinery.
doi:10.1145/3491102.3517582.
[68] Congying Xia, Chen Xing, Jiangshu Du, Xinyi
Yang, Yihao Feng, Ran Xu, Wenpeng Yin,
and Caiming Xiong. FOFO: A Benchmark
to Evaluate LLMs’ Format-Following Capa-
bility. (arXiv:2402.18667), February 2024.
doi:10.48550/arXiv.2402.18667.
[69] Yuchen Xia, Manthan Shenoy, Nasser Jazdi,
and Michael Weyrich. Towards autonomous
system: Flexible modular production sys-
tem enhanced with large language model
agents. In 2023 IEEE 28th International Con-
ference on Emerging Technologies and Fac-
tory Automation (ETFA) , pages 1–8, 2023.
doi:10.1109/ETFA54631.2023.10275362.
[70] I. de Zarzà, J. de Curtò, Gemma Roig,
and Carlos T. Calafate. LLM Adaptive
PID Control for B5G Truck Platooning Sys-
tems. Sensors, 23(13):5899, January 2023.
doi:10.3390/s23135899.
[71] Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan
Zhou, and Helen Meng. SGP-TOD: Build-
ing Task Bots Effortlessly via Schema-Guided
LLMPrompting. (arXiv:2305.09067), May2023.
doi:10.48550/arXiv.2305.09067.
[72] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi
Tang, Xiaolei Wang, Yupeng Hou, Yingqian
Min, Beichen Zhang, Junjie Zhang, Zican Dong,
Yifan Du, Chen Yang, Yushuo Chen, Zhipeng
Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun
Nie, and Ji-Rong Wen. A Survey of Large Lan-
guage Models. (arXiv:2303.18223), May 2023.
doi:10.48550/arXiv.2303.18223.
[73] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra,
Siddhartha Brahma, Sujoy Basu, Yi Luan,Denny Zhou, and Le Hou. Instruction-
Following Evaluation for Large Language Mod-
els. (arXiv:2311.07911), November 2023.
doi:10.48550/arXiv.2311.07911.
26

Title: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable
Abstract: The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.
Full Text: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable
Chaofan Lin1∗, Zhenhua Han2, Chengruidong Zhang2, Yuqing Yang2
Fan Yang2, Chen Chen1∗, Lili Qiu2
1Shanghai Jiao Tong University,2Microsoft Research
Abstract
The rise of large language models (LLMs) has enabled
LLM-based applications (a.k.a. AI agents or co-pilots), a new
software paradigm that combines the strength of LLM and
conventional software. Diverse LLM applications from differ-
ent tenants could design complex workflows using multiple
LLM requests to accomplish one task. However, they have
to use the over-simplified request-level API provided by to-
day’s public LLM services, losing essential application-level
information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end
performance of LLM applications.
This paper introduces Parrot, an LLM service system that
focuses on the end-to-end experience of LLM-based applica-
tions. Parrot proposes Semantic Variable , a unified abstrac-
tion to expose application-level knowledge to public LLM
services. A Semantic Variable annotates an input/output vari-
able in the prompt of a request, and creates the data pipeline
when connecting multiple LLM requests, providing a natu-
ral way to program LLM applications. Exposing Semantic
Variables to the public LLM service allows it to perform con-
ventional data flow analysis to uncover the correlation across
multiple LLM requests. This correlation opens a brand-new
optimization space for the end-to-end performance of LLM-
based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement
for popular and practical use cases of LLM applications.
1 Introduction
Large language models (LLMs) have demonstrated a remark-
able language understanding capability [7, 41]. This enables
a paradigm shift in application development. In this new
paradigm, one or multiple application entities, known as AI
agents or co-pilots, communicate with LLMs via natural lan-
guage, known as “prompts”, to accomplish a task collabo-
ratively. For example, Meeting applications like Microsoft
Teams or Google Meet can summarize meeting discussions
through LLMs [33]. Search engines like Google and Bing
can be enhanced with Chat ability through LLMs [14, 34].
It is believed such LLM-based applications will become the
mainstream applications in the near future [13].
∗This work is partially done while Chaofan Lin’s internship and Dr. Chen
Chen’s visting scholar in Microsoft Research.To accomplish a task, LLM-based applications typically
require multiple rounds of conversation. The conversation, im-
plemented through multiple API calls to LLM, demonstrates
complex workflow patterns. Figure 1 illustrates several popu-
lar conversation patterns. For example, a meeting summary
application [8, 33] often divides a lengthy document into mul-
tiple shorter sections, each satisfying the length constraint
of the LLM conversation and thus can be summarized and
combined into the final summary through the Map-Reduce
or chaining summary patterns. Chat-based applications, e.g.,
Bing Copilot [34], call LLM APIs multiple times to generate
answers based on user queries. Multiple agents, each repre-
senting a different role played by different LLM calls, can
collaborate to achieve a task [22, 47, 54].
Public LLM service providers have to face diverse tenants
and applications, each with different workflows and perfor-
mance preference. However, existing API design for LLM
service provision is still request-centric. Public LLM services
only observe tons of individual requests, without knowing any
application-level information, e.g., which requests belong to
the same application, how different requests are connected, or
whether there are any similarities. The lost application-level
information makes public LLM service blindly optimize the
performance of individual requests, leading to sub-optimal
end-to-end performance of LLM applications. In this paper,
we observe there exist significant opportunities to improve
theend-to-end experience of LLM applications by exploiting
the application-level information, especially the correlation
of multiple LLM requests.
First, multiple consecutive LLM requests may be depen-
dent: the result of one request could be the direct input of
the next request. Therefore, it is desirable to colocate those
Chunk 1
Chunk 2……
Chunk NLLM
LLM
LLM……S1
S2
SN ……LLMFinal 
Summary
(a) Map-Reduce Summary
Chunk 1
Chunk 2……
Chunk NLLM
S1 + LLM
S1 + S2 +
……SN-1 LLMFinal 
SummaryLLM Request
Message Passing (b) Chain Summary
Query Rewriter
LLM -powered Search
QA w/ search result
Safety CheckerLLM
Final 
AnswerUser 
Query
(c) LLM-Powered Search
Product Manger
Architect
Engineer
QA TesterCode ReviewerLLM
Final 
CodeTask (d) Multi-agent Coding
Figure 1: The workflow of popular LLM-based applications.
The final result requires multiple LLM requests.arXiv:2405.19888v1  [cs.LG]  30 May 2024requests together and execute them consecutively on the LLM
service side. However, unaware of their dependencies, these
requests have to be executed interactively between the client
side of LLM-based applications and the public LLM ser-
vices. These clients, often located on the other end of the
Internet, can only issue the second request after they receive
the result of the first request. This unnecessarily incurs extra
overhead of consecutive requests on network latency as well
as losing the opportunity of co-scheduling these consecutive
requests (§3).
Second, LLM requests may have diverse scheduling pref-
erence , even within a single application. For example, in Fig-
ure 1a, to reduce the end-to-end latency, the requests represent-
ing multiple Map tasks should be batched more aggressively
to increase the throughput of the Map tasks; while the Re-
duce task, due to its scarcity, should be optimized for latency.
Unfortunately, public LLM services cannot discriminate the
difference between the two types of tasks. As a result, the
current practice is to blindly optimize the latency for individ-
ual requests, which might not be desirable for the end-to-end
experience.
Third, there exists a high degree of commonality across
LLM requests. Popular LLM applications (e.g., Bing Copi-
lot [32], GPTs [42]) use a long system prompt, including task
definitions, examples, and safety rules, to guide the behavior
of LLM applications. The long system prompt is usually static
and common for all users. As existing public LLM services
treat each request individually, these common prefix prompts
are provided repeatedly in each request, leading to a great
waste of storage, computation, and memory bandwidth. Our
analysis of a production LLM-based search engine shows
that over 94% of tokens in the requests are repeated across
different users.
Although we have seen some emerging engine-level tech-
niques [25,56,63] proposed to optimize the above three cases,
they all work based on certain application-level knowledge,
which is lost in nowadays public LLM services. In a nut-
shell, due to the lack of understanding of the correlations of
LLM requests, existing LLM services cannot leverage the
three opportunities, leading to high end-to-end service latency
and reduced throughput. Based on the above facts and in-
sights, we introduce Parrot, an LLM service system that treats
LLM applications as first-class citizens. Parrot retains most of
application-level information by a simple abstraction Seman-
tic Variable, achieving a perfect balance between increasing
system complexity and bringing new information for opti-
mization. A Semantic Variable is a text region in the prompt
with a specific semantic purpose, such as a task instruction, a
list of few-shot examples, an input, or an output. A Semantic
Variable can also work as the data pipeline that connects mul-
tiple LLM requests. Semantic Variable naturally exposes the
information of prompt structures and correlations of requests
to LLM services. By inspecting Semantic Variable at runtime,
Parrot can perform conventional data flow analysis to derive
Figure 2: The communication of consecutive LLM requests
in multi-agent applications.
the data dependency between LLM requests just-in-time.
By analyzing the application-level information, Parrot’s
unified abstraction naturally enables joint optimizations,
which bring better global optimality. The same data pipeline
built by Semantic Variables can enable multiple optimizations
simultaneously, including hiding data pipeline’s latency, ob-
jective deduction for a better scheduling and commonality
analysis to perform de-duplication. Parrot’s scheduling also
takes different opportunities into accounts under the unified
abstraction. Our extensive evaluation of Parrot on popular
LLM-based applications, including the production and open-
source projects, shows Parrot achieves up to 11.7×speedup
or12×higher throughput compared with the state-of-the-art
solutions.
2 Background
LLM Service. Most LLM services are provisioned as a
conditional generation service via a text completion API.
Completion (prompt : str )− →generated_text : str .
The application client provides a text prompt, and the LLM
service responds with the generated text. Behind the API,
an LLM service provider runs one or multiple clusters of
LLM inference engines. A request scheduler dispatches LLM
requests from a queue to an LLM inference engine, which
uses a set of GPUs to conduct the LLM inference.
LLM-based Applications. Figure 1 highlights the repre-
sentative workflows of how LLM is used in the applications.
Due to the limited context window of LLMs (e.g., 4,096 for
GPT-3.5-Turbo [40]), data analytics on long documents fol-
low a map-reduce style (Figure 1a) or chain style (Figure 1b)
workflow to generate the final results. It splits the long tran-
script into chunks, uses multiple requests to generate partial
results for each chunk (the Map task), and combines them
altogether (a Reduce task) or incrementally (the chain style)
to generate the final result. Chat-based search engine in Fig-
ure 1c may use consecutive LLM requests to discern query0 1000 2000 3000 4000
Prompt Length (# of tokens)010002000300040005000Time (ms)
End-to-end Time (P99))
GPU Inference Time
Other Overhead (median)(a) Latency Breakdown
LLM Step  A
LLM Step B
SchedulerInternetLLM  App
Other LLM Apps
A B
LLM Engine LLM Engine LLM Engine
AQueue
B①
②③
Query
Response
④ (b) Current LLM Services
LLM Engine LLM Engine LLM EngineSchedulerLLM Step A
LLM Step B
①
②Internet
A BA B Queue
Query
ResponseLLM  App
Other LLM Apps (c) Our system: Parrot
Figure 3: The end-to-end latency breakdown of current LLM services. The source of the overhead comes from network and
queuing due to chatty interaction between LLM application and LLM services, which is eliminated in our system Parrot.
intention, enrich the query with supplementary information,
retrieve related data, undergo a safety check, and finally gen-
erate the response. Multi-agent in Figure 1d and Figure 2 is
another type of workflow using multiple LLM requests, each
with a designated role. Different roles work collaboratively on
the same task, e.g., AutoGen [54] and MetaGPT [22] use the
roles like product manager, architect, engineer, and QA tester.
They communicate with each other on a software project.
Each role is supported by one or multiple LLM requests to
act as the designed role to generate their responses.
3 Problems of Serving LLM Applications
Although LLM’s text completion API provides a flexible way
of building LLM applications, it loses the application-level
information to public LLM services, leading to the following
challenges.
Excessive Overhead of Consecutive Requests. As demon-
strated in Figure 1, LLM applications frequently make multi-
ple LLM calls to complete a single task. Due to the request-
centric design of existing public LLM services, which gener-
ate responses for each request individually, developers have to
parse the output of an LLM request and compose the prompts
for subsequent LLM requests on the client side. Figure 3a
shows our empirical study of the latency breakdown of the
LLM calls from a popular LLM application in our production,
which uses a chain-style workflow. The prompt lengths range
from 150 to 4000 tokens and the output length is around 50
tokens. We find there is a significant portion of the latency of
LLM API call originates outside the LLM engine ( 30∼50%
on average and over 70% in the worst cases). The overhead in-
creases with the growing length of prompts. The high latency
can sometimes result in API timeouts and resubmissions.
Such overhead is due to the chatty interaction between
LLM services and clients. Figure 3b illustrates the overhead
of a simple two-step LLM application (e.g., chain-style sum-
mary of two text chunks). Existing LLM services are unaware
of the dependency among such requests, where the output of
the previous request may be the direct input of the next one.
For such consecutive and dependent requests, the client has
Chunk 1Chunk 2
Chunk 3Chunk 4……Final Summary Chunk 5Chunk 6
Chunk 15Chunk 16Batch =2
Time
Chunk 1 Final SummaryBatch=8 Chunk 9Time(1) Per-request latency optimized
(2) End -to-end latency  optimizedMaximize Throughput
Minimize LatencyLatency=2700 ms
Latency=1100 msReduce StageMap Stage
Reduce Stage
Chunk 2 Chunk 10Chunk 7 Chunk 15Chunk 8 Chunk 16Map Stage ……
……Figure 4: Request-centric scheduling v.s. application-centric
scheduling for the map-reduce style document summary task.
to wait for the arrival of the response to the first LLM request
(2) before submitting the next LLM request ( 3). This un-
necessarily incurs heavy network latency because clients and
LLM services are typically in different data centers. More-
over, the next LLM request has to suffer extra queuing delays
(4), because requests from other applications may arrive
between the consecutive LLM requests.
In Table 1, we evaluated four popular LLM applications.
The first two are from our production, and the last two are
popular open-source projects. They all require tens of LLM
calls to complete a single task, which results in high user-
perceived latency. Our evaluation in §8.2 shows LLM services
that treat requests individually could slow down the end-to-
end latency by over 2×. An LLM service can eliminate the
overhead if it can handle consecutive requests in a batch.
Parrot adopts such an approach. As shown in Figure 3c, the
two steps of the same application are scheduled together, thus
allowing the output of Step A to be fed directly into Step
B—with the network and queuing overhead bypassed.
Misaligned Scheduling Objectives. Due to the lost appli-
LLM-based App. # Calls Tokens Repeated (%)∗
Long Doc. Analytics 2∼40 3.5k∼80k 3%
Chat Search 2∼10 5k 94%
MetaGPT [22] 14 17k 72%
AutoGen [54] 17 57k 99%
∗We count a paragraph as repeated if it appears in at least two LLM requests.
Table 1: Statistics of LLM calls of LLM applications.[system](#instructions) 
## You are the chat mode 
of Microsoft Bing search: 
- You identify as Microsoft 
Bing search to users, 
**not** an assistant. 
- You should ……[system](#context) 
- New conversation with user A. 
- Time at the start of this conversation is 
Sun, 30 Oct 2022 16:13:49 GMT. The 
user is located in  Redmond, Washington, 
United States. 
[user](#message) Hi. ……[system](#context) 
- New conversation with user B. 
- Time at the start of this conversation is 
Mon, 20 Nov 2023 16:13:49 GMT. The 
user is located in  London, UK. 
[user](#message)
Explain AI agent  for a kid.
Task Role (static) Few -shot Examples (quasi -static) User Input (dynamic)+ +Figure 5: The prompt structure of Bing Copilot shows a long
prompt reused by different user queries.
cation information (workflow and application performance
objective), existing public LLM services have to blindly use
a universal treatment for all requests, e.g., optimizing per-
request latency [44]. However, LLM-based applications are
more concerned about the end-to-end experience, rather than
individual requests. This misaligned optimization objectives
may negatively impact end-to-end performance. Considering
the map-reduce document summary in Figure 1a, the system
should minimize the end-to-end time it takes to receive the
final summary, rather than the latency of individual requests.
The LLM services optimized for individual requests are not
optimal for end-to-end latency.
As depicted in Figure 4, current LLM services must limit
the number of concurrent requests running on each LLM en-
gine to control the latency of individual requests. However,
there is a trade-off between latency and throughput in LLM in-
ference. Increasing the batch size can bring up to 8.2×higher
throughput but lead to 95% higher latency [9]. Yet, if we un-
derstand the application-level performance objective, which
in this case is the end-to-end latency, we can determine that
the ideal scheduling strategy should maximize the throughput
(using higher batch sizes) during the map stage and minimize
request latency during the reduce stage. This strategy reduces
end-to-end latency by 2.4×. Moreover, it uncovers the po-
tential to enhance cluster throughput without compromising
the end-to-end latency of LLM applications. This insight is
essential for addressing the conflict between rising demand
and limited hardware resources. It underscores the necessity
of scheduling LLM requests from the perspective of LLM
applications, but it also presents the challenge of managing
diverse LLM requests with varying performance objectives.
Redundant Computations. Currently, most LLM-based
applications exhibit a high degree of redundancy in the
prompts of their requests. For instance, Bing Chat [32] has
handled more than 1 billion chat prompts. These prompts
share the same system prompts that defines the functionality
of Bing Chat. OpenAI introduces GPTs [42] to let users cus-
tomize a ChatGPT for a specific purpose whose prompt tem-
plate is the same across users. The commonality in prompts
is crucial as it delineates the functionality and restrictions
of LLM-based applications. The prompt structure in Fig-
ure 5 [52] includes a role definition, several examples to
enhance the precision of LLM’s behaviors and user query
details. While the user input is dynamic, the task role is al-
Parrot LLM EngineParrot LLM EngineParrot LLM EngineParrot APIs w/ 
 Semantic Variables
Parrot Manager  w/ Inter -Request Analysis
Parrot 
App -centric  
LLM ServiceApplications
InternetApplications (front -end)
Parrot Front -end Others (LangChain ,SK, etc.)
Perf. Objective Deduction
Sharing Prompt Prefix App -centric Scheduling
Efficient GPU Kernels Context ManagementContextual Fill / GenInter -Request Comm.Figure 6: Parrot system overview.
ways fixed, and the few-shot examples could be quasi-static in
that the same type of tasks use the same examples. This is why
more than 94% of prefix tokens could be repetitively used
across LLM requests for various users (Table 1). Such com-
monality also exists in multi-agent applications. For example,
MetaGPT [22] and AutoGen [54] recurrently incorporate con-
versation history into the prompt over several rounds of LLM
requests, leading to 72% and99% redundancy respectively.
These redundant sections excessively utilize GPU memory
bandwidth and are computed for multiple times. Earlier re-
sults have proposed optimizations in LLM engines to avoid
redundant GPU memory of shared prompt [25]. However, it is
hard for public LLM services to swiftly detect and co-locate
the prompt-sharing requests, which be dynamically generated,
from tons of diverse requests from diverse applications. With-
out knowledge about the prompt structure, extensive token-
by-token matching for every LLM request is expensive at the
cluster level. Hence, if the cluster scheduler of public LLM
service cannot dispatch prompt-sharing requests to the same
engine, the engine-level redundancy avoidance optimizations
would be hard to take effect.
4 Parrot Design
Figure 6 depicts the overview of Parrot’s design. Parrot pro-
vides a natural way of programming LLM applications with
Semantic Variable annotations (§4.1), which is compatible of
existing LLM orchestration frameworks, e.g., LangChain [8].
Centering on this abstraction, Parrot Manager is designed
to schedule LLM requests at a cluster-level, by deriving the
application-level knowledge (§4.2) and optimizing end-to-end
performance of application (§5). The manager will schedule
the LLM requests to LLM Engine , which is formed by a GPU
server (or a group of servers) in the cluster that can serve LLM
requests independently.
4.1 Semantic Variable
Parrot treats an LLM request as a semantic function1im-
plemented using natural language and executed by LLMs.
1The term semantic function is borrowed from Semantic Kernel [36].import Parrot asP
from Parrot .PerformanceCriteria import LATENCY
@P.SemanticFunction
def WritePythonCode(task: P.SemanticVariable):
""" You are an expert software engineer.
Write python code of {{input:task}}.
Code: {{output:code}}
"""
@P.SemanticFunction
def WriteTestCode(
task: P.SemanticVariable,
code: P.SemanticVariable):
""" You are an experienced QA engineer.
You write test code for {{input:task}}.
Code: {{input:code}}.
Your test code: {{output:test}}
"""
def WriteSnakeGame():
task = P.SemanticVariable("a snake game")
code = WritePythonCode(task)
test = WriteTestCode(task, code)
return code.get(perf=LATENCY), test.get(perf=LATENCY)
Figure 7: Example: a multi-agent application in Parrot.
A Semantic Variable is defined as a input or output vari-
able of a semantic function, which is referred as a place-
holder in the prompt. Figure 7 shows a simplified example of
multi-agent application like MetaGPT [22]. It contains two
SemanticFunction s, one for the software engineer to write
code and one for the QA engineer to write test code. It has
three Semantic Variables: task ,code , and test , for task de-
scription, the code to be developed by the software engineer,
and the test code to be developed by the QA engineer, re-
spectively. Although existing LLM orchestration frameworks
(e.g., LangChain [8]) also allow placeholders in a prompt,
however, the placeholders are rendered with real data before
the submission, hence public LLM services cannot detect such
a structure. Instead, Parrot relies on Semantic Variables to
preserve the prompt structure for further inter-request analysis
in public LLM services side.
In addition to the semantic functions, LLM application
developers can further define orchestration functions that con-
nect multiple semantic functions (e.g., WriteSnakeGame in
Figure 7). The Semantic Variables connecting multiple se-
mantic functions form the data pipeline of multiple LLM
requests in the public LLM service. A simple data flow
analysis of the semantic functions can be done to reveals
the connections of multiple LLM requests. E.g., in Figure 7,
thecode variable connects the two LLM requests originat-
ing from WritePythonCode andWriteTestCode , showing
their sequential dependency. Different from traditional com-
pletion API, Parrot splits a completion request to submit
operation and get operation (§7). A function calling of
SemanticFunction will trigger the submit API to submit a
LLM request with its prompt and input Semantic Variables.
The execution of a SemanticFunction is asynchronous
thus it returns the futures of the output Semantic Variables.
task
codeWritePythonCode
WriteTestCode
testYou are an expert software engineer. Write python code of
You are an expert ...... code of: {{input:task }}. Code:Hash( )
Hash( )① PrefixHash ()
④ GetPerfObj () Latency   ③ GetConsumers () [Request( )]② GetProducer ()  Request( ) WritePythonCode
WriteTestCodeFigure 8: Primitives (selected) for Inter-Request Analysis.
Through the get API, applications can fetch the value of
an output Semantic Variable from the public LLM service
in an on-demand manner. This asynchronous design allows
Parrot-powered LLM service to receive all LLM requests not
blocked by native functions and analyze their relationships
just-in-time.
Thegetoperation supports annotation of performance cri-
teria, showing the end-to-end performance requirement of
an application, which can be end-to-end latency or through-
put (extensible to more criteria like per-token latency when
streaming, and time-to-first-token). For example, the final out-
puts, code andtest in Figure 7, are fetched using getwith
an objective of end-to-end latency. Criteria of middle vari-
ables will be automatically deduced and propagated from final
outputs (§5.2). After propagation, each variable is attached to
a criterion, which finally works by serving as a hint to Parrot’s
scheduler (§5.4).
4.2 Primitives of Inter-Request Analysis
In general, Parrot perform inter-request analysis mainly by
two types of application-level information deduced from Se-
mantic Variable: DAG of requests and prompt structure. Fig-
ure 8 illustrates the DAG workflow of the example shown in
Figure 7 and the primitives used for inter-request analysis and
optimizations.
DAG-based analysis. As requests, or SemanticFunction s,
are submitted beforehand, Parrot can receive them all at once
and analyze their correlations just-in-time on the service side.
Parrot maintains a DAG-like data structure in each user’s
registered session. Each node is either a request or a Seman-
tic Variable that connects different requests. When a request
comes, Parrot inserts it to DAG by linking edges with Seman-
tic Variables it refers through placeholders in the prompts.
Parrot can perform conventional dataflow analysis [1, 38]
using the primitives to get the producer and consumers of Se-
mantic Variables (i.e., GetProducer andGetConsumers ) to
recover dependency of LLM requests. Using the request DAG
and the annotated performance criteria (via GetPerfObj ) of
final output Semantic Variables, Parrot can deduct the request-
level scheduling preference by analyzing the DAG and the
performance objective of final outputs (§5.2).Prompt structure-based analysis. Based on the prompt
structure declared by Semantic Variables, Parrot supports ex-
tracting the hash values of an LLM request at multiple po-
sitions split by Semantic Variables (i.e., PrefixHash ). For
example, the prompt of WritePythonCode has two potential
sharing prefix: the text before {{input:task}} and the text
before {{output:code}} , thus there will be two prefix hash
values generated. The prefix hashes of LLM requests will
be used by swift detection of commonality across multiple
requests, supporting both static and dynamically generated
contents, as well as within the same type of application or
even across applications (§5.3).
5 Optimizations with Semantic Variable
5.1 Serving Dependent Requests
To avoid the unnecessary client-side execution, it requires
the dependency of requests at the application level, which
is lost in today’s public LLM services. With the DAG and
primitives illustrated in §4.2, Parrot serves dependent requests
efficiently through a graph-based executor. The executor polls
constantly and sends it to corresponding engine once ready
(i.e. producer requests are all finished), which allows instant
execution and maximizes batching opportunities. For con-
secutive execution of dependent requests, materialized value
is transmitted through a message queue allocated for cor-
responding Semantic Variable, avoiding unnecessary chatty
communication between clients and LLM services.
The value of a Semantic Variable in a request may require
transformation before being exchanged, e.g., the value of a
Semantic Variable is extracted from the JSON-formatted out-
put of an LLM request, which is then fed into consecutive
LLM requests. Similar to existing message queue systems
that support message transformation (e.g., Kafka [5]), Parrot
also supports string transformation to manipulate Semantic
Variables during value exchanging among LLM requests. Par-
rot supports most output parsing methods of LangChain [8],
which covers most use cases of LLM applications.
5.2 Performance Objective Deduction
To optimize the end-to-end performance of applications, we
need to know the application-level performance criteria. To
help deriving the request-level scheduling preference from the
end-to-end application’s performance requirement, we need
to understand the workflow of the LLM application, which is
the DAG of LLM requests derived by Parrot’s primitives.
When an application annotates a Semantic Variable to pre-
fer higher throughput, all requests generating this Seman-
tic Variable (both directly or indirectly) will be marked as
throughput-preferred when scheduling. This scheduling pref-
erence is usually beneficial for offline data processing, such
as bulk document analysis.
1
3 54 6
7x.get (perf =LATENCY )Task 
Group 0Task 
Group 1
2 y.get (perf =LATENCY )Figure 9: Performance deduction for an LLM-based applica-
tion generating two latency-sensitive Semantic Variable.
Handling latency-sensitive applications is more intricate.
As demonstrated in Figure 4, achieving low end-to-end la-
tency may sometimes require prioritizing throughput at the
Mapping stage. The latency of individual requests can sacri-
ficed so as to reduce the completion time of the entire DAG of
requests. Parrot analyzes LLM requests in reverse topological
order, beginning with those linked to latency-critical Semantic
Variable, as depicted in Figure 9. With the extracted DAG,
LLM requests that directly result in latency-critical Seman-
tic Variables are labeled as latency-sensitive (Request 1 and
2), as are their immediate predecessors (Request 3). Parallel
LLM requests at the same stage are grouped into a task group
(Task Groups 0 and 1). The scheduler should minimize the
latency of the entire task group, often leading to a higher batch
capacity for higher throughput of token generation.
5.3 Sharing Prompt Prefix
When an LLM request is scheduled to an LLM engine, a con-
text on the engine is created to store the state of the model
execution for this request (mainly KV cache). Existing works
have proposed to share the KV cache of common prefix of
prompts in LLM engines to save the GPU memory. However,
as we have explained in §3, today’s public LLM service face
diverse applications and requests, which is hard to identify
the commonality at the cluster level. Token-by-token compar-
ison is impractical due to high time complexity, especially for
very long context with massive requests. In Parrot, by expos-
ing Semantic Variables to LLM service, we can understand
the prompt structure to automatically detect the commonality
more efficiently at the granularity of Semantic Variables.
Using Parrot’s primitive of PrefixHash , Parrot only needs
to check the hash value at positions after each Semantic Vari-
able in a request’s prompt. Parrot maintains a key-value store,
where each entry maps a (hashed) prefix of tokens to a list of
requests, thus the scheduler can quickly check the opportunity
in an online manner, supporting both static and dynamically-
generated prompt within one application or even across dif-
ferent applications.
Furthermore, we propose better GPU kernel for the atten-
tion computation of the requests with a common prefix. We
first leverage vLLM’s paged memory management [25] to
save the redundent GPU memory. But vLLM’s kernel still
suffers from redundant computation and memory loading
of the shared tokens. Therefore, we design a new Attention
decoding algorithm by combining FlashAttenation [12] and
PagedAttention [25] that treat the shared and non-shared to-Algorithm 1: Parrot’s Request Scheduling.
Data: Q: the request queue
1Q.sort() ; /* Topological order */
2forr∈Qdo
3 SharedReqsInQueue, CtxInEngine =
FindSharedPrefix( r);
4 ifr.TaskGroup ̸=∅then
5 r∗= FindEngine(r.TaskGroup);
6 else if SharedReqsInQueue ̸=∅then
7 r∗= FindEngine(SharedReqsInQueue);
8 else if CtxInEngine ̸=∅then
9 r∗= FindEngine(r, filter=CtxInEngine);
10 ifr∗=∅then
11 r∗= FindEngine( r);
12 Q.remove( r∗);
ken separately. This significantly accelerates the attention of
shared contexts (implementation details in §7).
5.4 Application-Centric Scheduling
To fix the problem of existing public LLM service that blindly
optimize diverse individual requests, Parrot’s scheduling pol-
icy leverages the application-level knowledge to optimize the
end-to-end performance. Specifically, the primary goal of Par-
rot’s scheduler is to meet the varied performance goals of
LLM applications while optimizing GPU cluster utilization.
As explained in §3, a conflict arises when combining through-
put and latency oriented requests: large batch sizes increase
throughput and GPU efficiency but degrade latency, and vice
versa. Transformer-based LLM inference is largely memory-
bound, with latency influenced by the count of concurrent
tokens within the engine. To meet performance targets of
LLM applications, particularly latency, an LLM engine must
regulate the token count below a specified threshold, which
is determined by the LLM request with the most strict la-
tency constraint. Therefore, Parrot’s scheduling principles are
twofold: (1) group LLM requests with similar performance
requirements to circumvent the conflict, and (2) maximize
opportunities for sharing across requests.
Algorithm 1 outlines the scheduling process of Parrot. With
the extracted DAG, the system arranges the LLM requests
according to their topological order (line 1). Parrot tends to
schedule requests belonging to the same application together
to avoid the slowing down of interleaved scheduling (§8.2).
For requests identified as part of a task group through Parrot’s
performance objective deduction, the scheduler attempts to
allocate the entire task group together (line 4-line 5). Addi-
tionally, if Parrot detects other queued requests or running
contexts with a common prefix, it tries to assign them to
the same LLM engine (line 3, line 6-line 9), to utilize Par-
rot’s context fork to reduce the redundant computation andGPU memory transactions. For an LLM request without the
above opportunity, Parrot schedules the request independently
(line 10-line 11). Due to limited space, we omit the details of
how Parrot chooses LLM engines (i.e., FindEngine ). Briefly,
Parrot finds the engine that satisfies the scheduling preference
of a request while minimizing the negative impacts. For in-
stance, if a latency-sensitive request is scheduled to an LLM
engine that can run up to 64,000 tokens of throughput-driven
requests, its capacity will be significantly reduced to 2,000 to
satisfy its strict latency requirement. But, if it is scheduled to
an engine that has already been running a latency-sensitive
request, the capacity reduction is negligible.
6 Discussion
Dynamic Applications and Function Calling. Currently,
Parrot only supports cloud-side orchestration of LLM requests
without involving dynamic control flow and native functions
(e.g., Python Code). They still require client-side execution.
We intentionally disable the offloading of these functions
to public LLM services to minimize the security risks of
malicious injection. For private LLM services whose LLM
applications are trusted or there is a trusted zone to execute
these functions, Parrot’s APIs can be easily extended with
conditional connections and native code submission. More-
over, these extensions further enable new optimizations, e.g.,
we can speculatively pre-launch high-probability branches in
dynamic applications based on past profiles. This also proves
the potential of Parrot’s design when facing new types of
applications. We leave these extensions as future works.
Other Applications of Inter-Request Analysis. The inter-
request analysis in Parrot enables a new optimization space
not limited to the ones we introduced in §5. A large-scale
service has more scheduling features to consider, including
handling outliers [3], job failures [58], delay scheduling [57],
fairness [15,61], starvation [17], or supporting heterogeneous
clusters [24, 37], which have been widely studied in other
systems. Parrot provides a new view from the perspective
of LLM-based applications: we need to understand the inter-
connection and commonality of LLM requests to optimize
applications’ end-to-end performance. These features can be
revisited in the LLM service system by considering the new
characteristics of LLM applications. In this paper, we focus
on Parrot’s mechanisms and a few use cases, leaving other
optimizations as promising future works.
Parrot with LLM Orchestration Frameworks. There
have been several frameworks for developers to build LLM-
based applications, e.g., LangChain [8], SemanticKernel [36],
and PromptFlow [35]. The key function of these frameworks
is to “glue” different LLM calls to accomplish a complex
task (aka. LLM orchestration). Parrot can be integrated withthese frameworks by extending their calling of LLM service
APIs with Semantic Variables. Most of these frameworks
have already used a template-based approach in which devel-
opers can design a template with placeholders, and render the
placeholders at runtime. These placeholders naturally have
the same concept as Parrot’s Semantic Variable. However,
because these frameworks will render the template prompt
before the submission, LLM services lose the information on
the prompt structure. To make these frameworks compatible
with Parrot, both the template itself and the variables to render
the template (using Semantic Variable in Parrot) need to be
wrapped as a SemanticFunction so the necessary informa-
tion is exposed to Parrot’s LLM service.
7 Implementation
Parrot is an end-to-end LLM service for LLM applications,
implemented on Python with about 14,000 lines of code. Its
front-end provides the abstraction of Semantic Variable, and
SemanticFunction , which is transformed into Parrot’s APIs
(implemented with FastAPI [48]) to be submitted as LLM
requests. A centralized Parrot manager handles the manage-
ment of LLM requests, including Semantic Variables, com-
munication, and scheduling. We also build an LLM engine
based on efficient kernels from vLLM [25], xFormers [26],
and ourselves. The engine supports advanced features for
LLM serving, including paged memory management [25] and
continues batching [56]. Parrot’s front-end and manager are
implemented in 1,600 and 3,200 lines of Python, respectively.
Parrot’s LLM engine is implemented in 5,400 lines of Python
and 1,600 lines of CUDA. We have implemented OPT [60]
and LLaMA [51] with PyTorch [45] and Transformers [53].
APIs. Applications programmed by SemanticFunction s
or other frontends are finally lowered to requests to universal
APIs through different adapters. Parrot provides OpenAI-like
APIs with the extension of Semantic Variables. The request
body of two operations mentioned in §4.1 is shown as follows:
(submit) {"prompt": str, "placeholders": [{"name":
str, "in_out": bool, "semantic_var_id": str,
"transforms": str}, ...], "session_id": str},→
,→
(get) {"semantic_var_id": str, "criteria": str,
"session_id": str} ,→
In addition to the static string prompt, Parrot preserves the
input and output placeholders. A placeholder is associated
with a semantic variable either for rendering the input or
parsing the output. As introduced in §5.1. Parrot supports
transformations before the input or after the output. Parrot
also supports other APIs for setting and fetching the value of
Semantic Variables. The error message will be returned when
fetching an Semantic Variable, whose intermediate steps fail
(including engine, communication, and string transformation).Kernel Optimization. vLLM’s GPU kernel, while capable
of reusing results cached in GPU memory for shared prefix to-
kens in a prompt, sometimes excessively reloads these tokens
from global to shared memory, impeding attention score com-
putations. Using OpenAI Triton [43] and CUDA, we have
developed a novel GPU kernel, integrating concepts from
PagedAttention [25] and FlashAttention [11, 12], to acceler-
ate attention decoding computation involving shared prefixes.
This kernel retains PagedAttention’s approach of storing the
key-value (KV) cache in disparate memory segments and
utilizes a page table per request to monitor block status and
placement. Furthermore, employing FlashAttention princi-
ples, the kernel maximizes data reuse within shared memory.
Unlike reloading tiles repeatedly in the PagedAttention’s im-
plementation, it loads KV cache tiles for the shared prefix
to shared memory only once, diminishing memory transac-
tions between the L2 Cache and Shared Memory. The kernel
initially calculates interim attention metrics (including atten-
tion scores, qk_max ,exp_sum ) for the shared prefix using the
loaded tiles and records these back to HBM. Subsequently, it
processes the new tokens’ partial attention beyond the prefix,
amalgamating this with the prefix’s interim results to derive
the ultimate attention output.
Universal Engine Abstraction. Parrot’s cluster manager
controls multiple engines running various models, tokeniz-
ers, KV cache layouts, etc. To enable Parrot’s optimizations,
LLM engines need to support (1) stateful generation (e.g.,
guidance [18]) and (2) sharing KV cache states across dif-
ferent requests. Hence we propose a universal abstraction to
describe the minimal capability required to LLM engines to
be integrated into Parrot.
def Fill(token_ids: List[int], context_id: int,
parent_context_id: int) ,→
def Generate(sampling_configs: Dict, context_id:
int, parent_context_id: int) ,→
def FreeContext(context_id: int)
These three methods not only cover the basic completion
functionality of LLM inference engine, but also provide a
flexible context management interface. The Fill method pro-
cesses the initial prompt tokens, calculates and fills the KV
cache into corresponding context. The Generate method pro-
duces tokens via generative decoding that produces one token
per iteration until it reaches the length limit, user-defined
termination character or EOS (end-of-sequence) token, un-
der certain sampling configurations (e.g. temperature). Fill s
andGenerate s are scheduled and batched by engine’s sched-
uler per iteration using continuous batching [56]. Creating
and forking contexts can also be realized with these two
methods by setting context_id andparent_context_id ,
respectively. The FreeContext method explicitly frees a con-
text (i.e. free its KV cache in GPU memory). Separating
Fill andGenerate not only fits Semantic Variable naturally:constant text and input values are processed by Fill ; the out-
put values are generated by Generate , but also breaks the
request-level dependency into a finer granularity, enabling
more parallel execution opportunities [2, 21, 46, 64].
8 Evaluation
8.1 Experimental Setup
Testbed. We evaluate Parrot with two separate setups for
single-GPU and multi-GPU experiments. The single-GPU
evaluations use a server with a 24-core AMD-EPYC-7V13
CPUs equipped with one NVIDIA A100 (80GB) GPU. The
multi-GPU evaluations use a server with 64-core EPYC AMD
CPU and four NVIDIA A6000 (48GB) GPUs. Both servers
run CUDA 12.1 and cuDNN 8.9.2.
Workloads. Our evaluations are performed to run four rep-
resentative LLM applications. Each LLM engine uses one
GPU and runs a LLaMA 13B or LLaMA 7B model [51] .
For LLM-based data analytics on long documents, we use the
Arxiv dataset [27], executing chain and map-reduce summa-
rizations on an extensive collection of academic papers. To
investigate the sharing opportunities of LLM-based applica-
tions with many users, we run the prompts from Bing Copilot
and GPTs [42] with synthesized user queries. For multi-agent
applications, we build a multi-agent programming application
using MetaGPT [22], which contains a system architect to
design APIs, multiple programmers to write code for different
files, reviewers to share review comments. The programmers
will also revise the code based on comments. For chat ser-
vice workloads, we derived scenarios from the ShareGPT
dataset [50], which mirrors real LLM chat conversations. Ac-
cording to the distribution of our measurement, we introduced
a random delay of 200∼300ms to LLM requests to emulate
typical network overhead seen over the Internet. To create
realistic workloads, we documented the LLM responses us-
ing GPT-4 [41], ensuring the LLaMA models generated text
of similar length for system performance analysis. Table 2
presents the workloads and their optimizations in Parrot.
Baseline. We benchmark Parrot against sate-of-the-art so-
lutions for building LLM applications and serving LLM re-
quests. The majority of LLM applications used in our baseline
WorkloadServing
Dependent
Requests.Perf. Obj.
DeductionSharing
PromptApp-centric
Scheduling
Data Analytics ✓ ✓ ✓
Serving Popular
LLM Applications✓ ✓
Multi-agent App. ✓ ✓ ✓ ✓
Mixed Workloads ✓ ✓ ✓
Table 2: The workloads and the optimizations taking effect.
5 10 15 20 25
Requests/s2030405060Mean Latency (ms)
Capacity=2048
Capacity=4096
Capacity=6144
Capacity=8192
Capacity=10240
Capacity=12288(a) Mean Latency
5 10 15 20 25
Requests/s2030405060P90 Latency (ms)
Capacity=2048
Capacity=4096
Capacity=6144
Capacity=8192
Capacity=10240
Capacity=12288 (b) P90 Latency
Figure 10: Latency (per output token) of vLLM with varying
token capacities and request rates. Requests are sampled from
ShareGPT [50] and their arrival time follows Poisson distri-
butions.
comparisons are developed using LangChain [8], which is the
predominant framework for LLM application development.
The LLM applications in baselines leverage OpenAI-style
chat completion APIs as provided by FastChat [62]. FastChat
is a widely recognized open-source LLM serving system
with over 30,000 stars on its repository. Incoming requests to
FastChat are allocated to LLM engines that run either Hug-
gingFace’s Transformers library [53] or vLLM [25], both of
which incorporate cutting-edge enhancements for LLM exe-
cution, such as FlashAttention [12], PagedAttention [25], and
continuous batching techniques [56]. The default scheduling
strategy employed by FastChat assigns incoming requests
to the LLM engine with the smallest current queue. Since
existing LLM services typically expose their functionality
through "chat" completion APIs, baseline assessments treat
all requests as independent and assume a high sensitivity to
latency. To manage token generation response times, each
LLM engine is subject to a capacity threshold, which is the
aggregate token count from all active requests on the engine.
Since existing LLM token generation is usually bound by
memory bandwidth, the per-token generation latency of an
engine is mainly affected by the number of running tokens in
a batch. As depicted in Figure 10, our experiments indicate
that the latency per output token, i.e. TPOT (Time-per-output-
token) for vLLM, with continuous batching enabled, experi-
ences a notable uptick when the engine’s workload using a
batch capacity beyond 6144. In our evaluation, we use the
setting that an LLM engine can keep its generation latency
under 40 ms/s for latency-sensitive requests, consistent with
our experience of OpenAI’s LLM services. When all LLM
engines hit their maximum capacity, any additional LLM re-
quests are queued in a FIFO (First In, First Out) manner,
awaiting the completion and release of resources by ongoing
tasks. Serving longer context (e.g., 32k or even 1M tokens)
within a satisfactory latency require either more GPUs using
tensor-parallel [49] or sequence-parallel [6] approaches, or
approximate attention (e.g., StreamingLLM [55]), which is
beyond the scope of this paper.25 50 75 100
Output Length (# tokens)050100150200250Average Latency (s)1.38x1.21x1.14x1.11x
1.88x1.64x1.55x1.52x Parrot
Baseline (vLLM)
Baseline (HuggingFace)(a) Output lengths
512 1024 1536 2048
Chunk Size (# tokens)050100150200250Average Latency (s)1.21x
1.21x
1.20x
1.19x1.63x
1.62x
1.60x
1.61xParrot
Baseline (vLLM)
Baseline (HuggingFace) (b) Chunk sizes
Figure 11: Average E2E latency of chain summarization with
varying output lengths and chunk sizes.
8.2 Data Analytics on Long Documents
Our experimental analysis within data analytics randomly
picks ten long documents from the Arxiv-March dataset [27],
using chain-summary and map-reduce summary. Each docu-
ment has over 20,000 tokens. The results measures the mean
end-to-end latency across all documents.
Chain-style Applications. Our evaluation demonstrates
how Parrot enhances chain summarization by mitigating the
excessive communication overhead stemming from client in-
teractions. Figure 11 presents the average end-to-end latency
for summarizing a single document using one LLM engine
(A100, LLaMA 13B) . We adjust the chunk size (the count of
tokens per chunk) and the output length, with results shown in
Figure 11a and Figure 11b, respectively. Parrot achieves a re-
duction in end-to-end latency by as much as 1.38×and1.88×
compared to the baselines employing vLLM and Hugging-
Face, respectively. The efficiency of Parrot primarily stems
from the decreased network latency, which is a consequence
of reduced client interaction. As the output length increases,
the time spent on generation becomes more significant, lead-
ing to a diminishing advantage for Parrot over the baseline. By
increasing the chunk size, we decrease the number of chunks,
yet the extent of the speedup is contingent upon the network
latency savings for each chunk. Given that token generation is
substantially more time-consuming than prompt processing,
we observe a consistent speedup with variable chunk sizes
and a fixed output length ( 1.2×and1.66×relative to vLLM
and HuggingFace, respectively). This indicates that Parrot’s
optimization for dependent LLM requests is particularly bene-
ficial for shorter outputs, which are prevalent in various LLM
applications such as summarization, short answer generation,
scoring, and choice provision. Due to HuggingFace’s slower
performance relative to vLLM, subsequent evaluations focus
solely on the comparison between Parrot and vLLM.
Figure 12a extends the evaluation by introducing back-
ground LLM requests at varying rates to examine the capa-
bility of Parrot in mitigating additional queuing delays for
dependent requests. Parrot slashes the end-to-end latency by a
factor of 2.38×in comparison to the baseline (vLLM). With
Parrot, as soon as the summary for the first chunk is completed,
0.00.51.01.52.02.53.03.5
Request Rate (reqs/s)50100150200250Average Latency (s)
1.21x1.19x1.31x1.79x2.38xParrot
Baseline (vLLM)(a) With background requests
10 15 20 25
Number of Apps0100200300Average Latency (s)1.38x1.52x1.63x1.68xParrot
Baseline (vLLM) (b) Multiple summary apps.
Figure 12: Average E2E latency of chain-summary with back-
ground requests or other chain-summary applications.
12345678910111213141516171819202122232425
Application No.050100150200250Latency in Baseline - Latency in Parrot (s)
Figure 13: The difference in E2E latency of the 25 chain-
summary application between Baseline and Parrot. All appli-
cations finish earlier in Parrot.
the subsequent chunk is processed immediately by incorporat-
ing the summaries of previous chunks into the prompt, which
aids in generating the summary for the next chunk. In con-
trast, the baseline treats all LLM requests individually. As a
result, in addition to the network latency from client interac-
tions, subsequent requests must re-enter the queue, leading
to added queuing delays. Figure 12b further illustrates the
end-to-end latency when multiple chain-summary applica-
tions are submitted concurrently, with each application tasked
with generating a summary for a separate document. Parrot
manages to reduce the average end-to-end latency for all ap-
plications by 1.68×without slowing down any applications
compared to the baseline according to Figure 13. The base-
line, by interleaving the execution of different applications,
exacerbates the slowdown of the end-to-end latency for all
applications. These experiments validate that recognizing the
interconnections of LLM requests can significantly enhance
end-to-end performance, as opposed to processing requests
in isolation.
Map-Reduce Applications. An alternative implementation
of the document summarization application follows the map-
reduce paradigm as depicted in Figure 1a. This approach
consists of multiple parallel mapping LLM requests, where
each request summarizes a distinct segment of the document,
followed by a reducing LLM request that aggregates these
individual summaries into a final summary. As shown in
Figure 14, Parrot realizes a 2.37×acceleration over the base-25 50 75 100
Output Length (# tokens)010203040Average Latency (s)1.70x2.04x2.22x2.37xParrot
Baseline (vLLM)(a) Output lengths
512 1024 1536 2048
Chunk Size (# tokens)0102030Average Latency (s)1.96x
2.07x2.07x 2.16xParrot
Baseline (vLLM) (b) Chunk sizes
Figure 14: Average E2E latency of Map-Reduce document
summary with varying output lengths and chunk sizes.
line with one LLM engine (A100, LLaMA 13B). Since the
mapping LLM requests are independent, they are dispatched
concurrently by both Parrot and the baseline. The primary ad-
vantage of Parrot stems from its deduction of a performance
objective that identifies the mapping tasks as a task group.
By recognizing this relationship, Parrot is capable of optimiz-
ing the latency of the entire task group through larger batch
sizes, which in turn enhances throughput. In contrast, the
baseline processes each LLM request in isolation, operating
under the presumption that they are all sensitive to latency.
This constrains the baseline to utilize a limited token capacity
(4096 tokens) on the LLM engine to achieve optimal latency
for individual tasks, which is detrimental to the end-to-end
performance of applications. It underscores the necessity for
LLM services to distinguish LLM requests to optimize the
end-to-end performance of varied LLM applications.
8.3 Serving Popular LLM Applications
Production applications need to face massive users. As ex-
plained in Figure 5, developers often need to use a very long
system prompt to define the behavior of LLMs. Therefore,
users of the same LLM application often use the shared
prompt, which can benefit from Parrot’s context fork mech-
anism and Parrot’s scheduling policy that co-locates LLM
requests sharing a long prompt prefix. Because we do not
have access to the intermediate steps of Bing Copilot, we only
evaluate the final request generating the response to users.
We synthesized 64 requests from the length distribution we
measured using Bing Copilot. The system prompt length is
about 6000 tokens. The output lengths ranges from 180 to
800 tokens. Figure 15 shows the average request latency of
Bing Copilot of Parrot and the baselines. Because the LLM
service in the baseline system does not know the prompt struc-
ture, it is hard to infer the shared prompt from massive LLM
requests. Compared to the baseline without sharing prompt,
Parrot achieves 1.8× ∼ 2.4×speedup for batch sizes of 8 and
16. Further increasing the batch size leads to out-of-memory
due to the massive KV cache of shared system prompt. We
also build an advanced baseline using vLLM’s paged atten-
tion to support sharing the prompt with a static prefix. Both
8 16 32 64
Batch Size010203040Avg. Latency (s)1.1x1.3x1.4x1.7x
1.8x2.4x
x xParrot
Baseline w/ Sharing
Baseline w/o SharingFigure 15: Latency of Bing Copilot with varying batch sizes.
200 400 600 800
Output Length (# tokens)0.000.020.040.060.080.100.12Latency per token (s)
1.44x
1.53x 1.56x 1.58x
Parrot
Baseline w/ Sharing
(a) Batch Size = 32
100 200 300 400 480
Output Length (# tokens)0.000.050.100.150.200.25Latency per token (s)
1.44x
1.64x1.74x 1.81x1.84x
Parrot
Baseline w/ Sharing (b) Batch Size = 64
Figure 16: Latency per output token of Bing Copilot.
Parrot and vLLM use the paged memory management [25],
thus both systems can hold the same number of tokens in
an LLM engine (A100, LLaMA 7B). Parrot further achieves
1.1× ∼ 1.7×speedup over vLLM because of the better GPU
kernel. Although vLLM can save extra memory usage of the
shared prompt, its GPU kernel still has to reload the tokens
repeatedly. Given that the token generation of LLMs is bound
by memory bandwidth, such redundant memory loading slows
down the end-to-end inference. By combining FlashAtten-
tion and PagedAttention, Parrot only needs to load the tokens
of the shared prompt once, when computing the attention
from the diverged tokens of different users. Parrot’s speedup
of shared prompt mainly comes from the token generation,
thus the longer output length leads to higher improvement.
Figure 16 shows Parrot achieves 1.58×and1.84×speedup
compared to vLLM using paged attention, showing 40ms
per-output-token latency at a batch size of 32.
In Figure 17, we further evaluated the serving of multiple
GPTs applications [42], each of which has multiple users, in
a multi-GPU cluster. Four A6000 (48GB) GPUs are deployed
with four LLM engines (LLaMA 7B). We select four GPTs
applications in four popular categories including productivity,
programming, image generation, and data analysis. The LLM
requests are randomly generated from the four categories with
equal probability. LLM requests arrive at fixed rates following
Poisson distribution. Parrot can sustain 12×higher request
rates compared to the baseline without sharing. Because the
baseline’s scheduling policy is not aware of the shared prompt
within each LLM application, the requests are mixed in all
LLM engines making it impossible to reuse the common
prompt prefix. Parrot’s scheduling policy co-locates LLM
requests of the same applications to maximize the sharing op-012345678910111213141516
Request rate (req/s)0100200300Normalized latency
 (ms/token)
Parrot
Parrot w/ PagedAttention
Parrot w/o Scheduling
Baseline (vLLM)Figure 17: Serving multiple GPTs applications.
portunity, achieving both lower inference latency and higher
cluster throughput. After turning off such affinity scheduling
policy, Parrot only exhibits 3×higher request rates compared
to the baseline, because the requests with shared prefix are
often dispatched to different engines thus reduced the sharing
opportunities. Moreover, Parrot’s attention kernel helps Parrot
to achieve 2.4×higher rate compared to Parrot using vLLM’s
PagedAttention, by avoiding the redundant memory loading
for attention of shared prompts.
8.4 Multi-agent Applications
We assess the performance of multi-agent systems utiliz-
ing MetaGPT [22] within Parrot. A workflow is constructed
with three distinct roles. Initially, the Architect outlines the
project’s file structures and specifies APIs within each file
for a given task. Subsequently, multiple Coders undertake the
project implementation, with each focusing on a specific file.
Following the integration of the code from all files, several
Reviewers engage in the process, each examining and com-
menting on a single file. The Coders then revise their code
based on these comments. This review-and-revision cycle
is iterated three times to produce the final code. Figure 18
illustrates the latency and memory consumption of Parrot
compared to baseline systems on one A100 running LLaMA
13B. Parrot achieves a speedup of up to 11.7×compared
with the latency-centric baseline. The primary improvement
is attributed to Parrot’s capability to deduct the performance
objectives for LLM requests based on the end-to-end perfor-
mance criteria. For this specific multi-agent scenario, the goal
is to minimize the time taken to deliver the final code. Parrot
identifies multiple task groups within the parallel processes of
coding, reviewing, and revising, facilitating larger batch sizes
to enhance throughput and reduce the completion time of task
groups. We also contrast Parrot with an throughput-centric
baseline that uses larger batch on purpose to optimize cluster
throughput, which also shows higher concurrency and better
completion time than the latency-centric baseline.
Even when compared to the throughput-centric baseline,
Parrot demonstrates superiority, being faster by up to 2.45×.
This enhancement mainly stems from Parrot’s ability to
24002600 11.7x
4 8 12 16
Number of Files050010001500Average Latency (s)1.00x 1.04x 1.14x 1.16x 1.22x1.61x1.88x2.35x
1.27x1.58x2.03x2.45x7.19x
4.9x
3.0xParrot
Parrot w/ PagedAttention
Parrot w/o Sharing
Baseline (vLLM, Throughput)
Baseline (vLLM, Latency)(a) End-to-end Latency
4 8 12 16
Number of Files01020304050GPU Memory of
KV Cache (GB)GPU Memory Capacity
Parrot
Parrot w/o Sharing
(b) GPU Memory of KV Cache
Figure 18: The latency and memory usage for multi-agent
programming, with varying number of files to program.
decrease redundancy through its prompt structure analysis,
which contributes a 2.35×acceleration. Given the interactive
nature of the roles in MetaGPT, there is considerable overlap
in the context among different roles, which Parrot capitalizes
on by sharing this common context as a prompt prefix. The
static prefix sharing mechanism from vLLM does not work
in this dynamic scenario. Without a grasp of the prompt’s
structure, it cannot identify dynamically generated Semantic
Variables that could also be shared during runtime. As de-
picted in Figure 18b, Parrot without this sharing capability
would hit the GPU memory ceiling. Additionally, Parrot’s spe-
cialized GPU kernel for processing the shared prefix achieves
a further 1.2×speedup when there are 16 files, compared to
using vLLM’s PagedAttention, due to the reduced memory
transactions.
8.5 Scheduling of Mixed Workloads
To assess the performance of Parrot on a multi-GPU setup, we
configure a cluster with four A6000 (48GB) GPUs, each host-
ing a separate LLM engine (LLaMA 7B), resulting in a total
of four LLM engines. We emulate a real-world scenario where
LLM services encounter a variety of demands by injecting a
mix of requests from chat applications at a rate of 1 req/s and
from data analytic tasks (i.e., map-reduce applications) previ-
ously analyzed in §8.2. Requests from the chat applications
are characterized by their need for low latency, whereas the
map-reduce applications prioritize high throughput, creating a
challenge when they are concurrently processed by the same
LLM engine. We benchmark Parrot against two reference
implementations: one tailored for latency, limiting engine ca-
pacity to reduce decoding time, and another for throughput,0200400600800
149.1184.6827.6Average Chat
 Normalized Latency (ms)
020406080
45.177.8
41.4Average Chat
Decode Time (ms)
020406080100
23.2 24.586.4Average
Map-Reduce JCT (s)
Parrot Baseline (Throughput) Baseline (Latency)Figure 19: The mixture of chat and map-reduce applications.
utilizing full engine capacity to maximize GPU utilization.
The results depicted in Figure 19 demonstrate that Par-
rot attains a 5.5×and1.23×improvement in normalized
latency (measured as request latency per number of output
tokens) [25, 56] for chat applications in comparison to the
latency-focused and throughput-focused baselines, respec-
tively. In terms of token generation speed for chat applications,
Parrot delivers performance on par with the latency-centric
baseline and outperforms the throughput-centric baseline by
1.72×. For map-reduce applications, Parrot reaches a 3.7×
speedup over the latency-centric baseline and is 1.05×more
efficient than the throughput-centric baseline. Parrot excels
by providing both low latency for chat applications and high
throughput for map-reduce applications. It mitigates the con-
tention between chat and map-reduce workloads by intelli-
gently scheduling them on separate engines. These findings
underscore the significance of specialized handling for diverse
requests to enhance the overall performance of LLM services.
9 Related Works
Deep Learning Serving Systems. The field of model serv-
ing has seen a surge of research activity in recent years,
with many systems developed to address the different chal-
lenges of deep learning model deployment. The systems in-
clude Clipper [10], TensorFlow Serving [39], Clockwork [19],
REEF [20], AlpaServe [28], which have explored many as-
pects including batching, caching, placement, scheduling,
model parallelism for the serving of single or multiple models.
These systems were proposed for serving general deep learn-
ing models, which have less consideration about the unique
requirements of large language models, e.g., autoregressive
decoding. Orca [56] proposed a fine-grained scheduling mech-
anism that can batch multiple LLM requests at the iteration
level, which is also known as continuous batching. vLLM
proposes PagedAttention [25] allows the batching of LLM
requests with different lengths using non-contiguous memory,
increasing memory utilization. These systems for LLM serv-
ing still treat LLM requests separately, missing the opportuni-
ties to understand the interconnections within an application
and exploit the commonality of different requests. Parrot isorthogonal to them. With more application-level knowledge
exposed by Semantic Variables, Parrot can do data flow analy-
sis on LLM requests, which enables a brand new optimization
space with the final goal of optimizing the end-to-end perfor-
mance of applications, rather than individual requests.
LLM Orchestrator Frameworks. LLM orchestration
frameworks help developers create and manage applications
powered by LLMs. They simplify the process of prompt de-
sign, and orchestration of multiple LLM requests, which en-
able developers to interact with LLMs easily. LangChain [8]
is a Python framework that provides many workflow patterns,
e.g., chain, map-reduce so that developers can easily cus-
tomize their own LLM applications. Semantic Kernel [36]
introduces Planners are semantic agents that can automati-
cally generate plans based on the needs of the users. Prompt-
Flow [35] supports chains of native and semantic functions
and visualizes them as a graph. LlamaIndex [29] allows de-
velopers to use natural language queries to retrieve relevant
documents. Parrot is orthogonal to these frameworks and can
be easily integrated with these frameworks to support Parrot’s
APIs with Semantic Variable abstraction, as discussed in §6.
DAG-aware System Optimizations. Dependency graphs
or DAGs (Directed Acyclic Graphs) widely exist in many
kinds of systems, and many optimizations have been proposed
to optimize the systems by exploiting the DAG information.
Tez [4], Dryad [23], and Graphene [16] use the task depen-
dency to optimize the scheduling and packing of parallel data
analytic workloads. SONIC [30], Caerus [59], and Orion [31]
optimize serverless functions from the aspects of communica-
tion, latency, and cost. Parrot learns from the previous system
works and realizes the importance of correlations of LLM
requests to optimize the end-to-end performance of LLM ap-
plications. This motivates Parrot to build APIs for exposing
such dependency information. Moreover, it is unique to LLM
applications to understand the prompt structure in addition to
request-level dependency, which is necessary for communica-
tion and identifying commonality across LLM requests. This
motivates us to propose the Semantic Variable abstraction,
instead of just using a DAG of requests.
10 Conclusion
This paper proposes Parrot that treats LLM applications as
first-class citizens and targets to optimize the end-to-end per-
formance of LLM applications, instead of only optimizing
individual LLM requests. We propose Semantic Variable as
the key abstraction that exposes the dependency and common-
ality of LLM requests, enabling a new optimization space.
Our evaluation shows Parrot can optimize LLM-based ap-
plications by up to 11.7×. We envision this new angle of
efficiency improvement of LLM applications brings a broadfuture direction to study other scheduling features like the
fairness of end-to-end performance of LLM applications.
Acknowledgments
We thank the anonymous reviewers and the shepherd for their
constructive feedback and suggestions. Zhenhua Han, Yuqing
Yang and Chen Chen are the corresponding authors.
References
[1]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, Man-
junath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek G. Murray, Benoit Steiner, Paul Tucker,
Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: A system for Large-
Scale machine learning. In 12th USENIX Symposium on
Operating Systems Design and Implementation (OSDI
16), pages 265–283, Savannah, GA, November 2016.
USENIX Association.
[2]Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree
Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tu-
manov, and Ramachandran Ramjee. Taming throughput-
latency tradeoff in llm inference with sarathi-serve.
arXiv preprint arXiv:2403.02310 , 2024.
[3]Ganesh Ananthanarayanan, Srikanth Kandula, Albert
Greenberg, Ion Stoica, Yi Lu, Bikas Saha, and Edward
Harris. Reining in the outliers in Map-Reduce clusters
using mantri. In 9th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 10) , Van-
couver, BC, October 2010. USENIX Association.
[4]Apache. Tez. https://tez.apache.org/ , November
2019.
[5]Apache. Kafka. https://kafka.apache.org/ , Octo-
ber 2023.
[6]Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen
Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang
You. Colossal-ai: A unified deep learning system for
large-scale parallel training. CoRR , abs/2110.14883,
2021.
[7]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.
Sparks of artificial general intelligence: Early experi-
ments with gpt-4, 2023.[8]Harrison Chase. LangChain. https://github.com/
langchain-ai/langchain , October 2022.
[9]Lequn Chen. Dissecting batching effects in gpt infer-
ence. https://le.qun.ch/en/blog/2023/05/13/
transformer-batching/ , May 2023.
[10] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J.
Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper:
A Low-Latency online prediction serving system. In
14th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 17) , pages 613–627, Boston,
MA, March 2017. USENIX Association.
[11] Tri Dao. Flashattention-2: Faster attention with bet-
ter parallelism and work partitioning. arXiv preprint
arXiv:2307.08691 , 2023.
[12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh, editors, Advances in Neural Information Process-
ing Systems , volume 35, pages 16344–16359. Curran
Associates, Inc., 2022.
[13] Bill Gates. Ai is about to completely change how you
use computers and upend the software industry. https:
//www.gatesnotes.com/AI-agents , Nov 2023.
[14] Google. Google bard. https://bard.google.com/ ,
Nov 2023.
[15] Robert Grandl, Mosharaf Chowdhury, Aditya Akella,
and Ganesh Ananthanarayanan. Altruistic scheduling
in Multi-Resource clusters. In 12th USENIX Sympo-
sium on Operating Systems Design and Implementa-
tion (OSDI 16) , pages 65–80, Savannah, GA, November
2016. USENIX Association.
[16] Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya
Akella, and Janardhan Kulkarni. GRAPHENE: Packing
and Dependency-Aware scheduling for Data-Parallel
clusters. In 12th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 16) , pages
81–97, Savannah, GA, November 2016. USENIX Asso-
ciation.
[17] Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin,
Yibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu,
and Chuanxiong Guo. Tiresias: A GPU cluster manager
for distributed deep learning. In 16th USENIX Sympo-
sium on Networked Systems Design and Implementation
(NSDI 19) , pages 485–500, Boston, MA, February 2019.
USENIX Association.
[18] guidance ai. Guidance. https://github.com/
guidance-ai/guidance , November 2023.[19] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao,
Antoine Kaufmann, Ymir Vigfusson, and Jonathan
Mace. Serving DNNs like clockwork: Performance
predictability from the bottom up. In 14th USENIX Sym-
posium on Operating Systems Design and Implementa-
tion (OSDI 20) , pages 443–462. USENIX Association,
November 2020.
[20] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo
Chen. Microsecond-scale preemption for concurrent
GPU-accelerated DNN inferences. In 16th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 22) , pages 539–558, Carlsbad, CA,
July 2022. USENIX Association.
[21] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Am-
mar Ahmad Awan, Jeff Rasley, Samyam Rajbhan-
dari, Reza Yazdani Aminabadi, Heyang Qin, Arash
Bakhtiari, Lev Kurilenko, et al. Deepspeed-fastgen:
High-throughput text generation for llms via mii and
deepspeed-inference. arXiv preprint arXiv:2401.08671 ,
2024.
[22] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,
Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt:
Meta programming for multi-agent collaborative frame-
work. arXiv preprint arXiv:2308.00352 , 2023.
[23] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell,
and Dennis Fetterly. Dryad: Distributed data-parallel
programs from sequential building blocks. In Proceed-
ings of the 2nd ACM SIGOPS/EuroSys European Con-
ference on Computer Systems 2007 , EuroSys ’07, page
59–72, New York, NY , USA, 2007. Association for Com-
puting Machinery.
[24] Suhas Jayaram Subramanya, Daiyaan Arfeen, Shouxu
Lin, Aurick Qiao, Zhihao Jia, and Gregory R. Ganger.
Sia: Heterogeneity-aware, goodput-optimized ml-cluster
scheduling. In Proceedings of the 29th Symposium on
Operating Systems Principles , SOSP ’23, page 642–657,
New York, NY , USA, 2023. Association for Computing
Machinery.
[25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,
Hao Zhang, and Ion Stoica. Efficient memory man-
agement for large language model serving with page-
dattention. In Proceedings of the 29th Symposium on
Operating Systems Principles , SOSP ’23, page 611–626,
New York, NY , USA, 2023. Association for Computing
Machinery.
[26] Benjamin Lefaudeux, Francisco Massa, Diana
Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean
Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang,Patrick Labatut, and Daniel Haziza. xformers: A modu-
lar and hackable transformer modelling library. https:
//github.com/facebookresearch/xformers ,
2022.
[27] Yucheng Li. Unlocking context constraints of llms: En-
hancing context efficiency of llms with self-information-
based content filtering, 2023.
[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent
Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,
Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Al-
paServe: Statistical multiplexing with model parallelism
for deep learning serving. In 17th USENIX Sympo-
sium on Operating Systems Design and Implementa-
tion (OSDI 23) , pages 663–679, Boston, MA, July 2023.
USENIX Association.
[29] Jerry Liu. LlamaIndex, November 2022.
[30] Ashraf Mahgoub, Karthick Shankar, Subrata Mitra,
Ana Klimovic, Somali Chaterji, and Saurabh Bagchi.
SONIC: Application-aware data passing for chained
serverless applications. In 2021 USENIX Annual Tech-
nical Conference (USENIX ATC 21) , pages 285–301.
USENIX Association, July 2021.
[31] Ashraf Mahgoub, Edgardo Barsallo Yi, Karthick
Shankar, Sameh Elnikety, Somali Chaterji, and Saurabh
Bagchi. ORION and the three rights: Sizing, bundling,
and prewarming for serverless DAGs. In 16th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 22) , pages 303–320, Carlsbad, CA,
July 2022. USENIX Association.
[32] Microsoft. Bing chat. https://www.bing.com/chat ,
Nov 2023.
[33] Microsoft. Meeting recap in microsoft
teams. https://www.microsoft.com/en-us/
microsoft-teams/premium , May 2023.
[34] Microsoft. Microsoft 365 copilot. https:
//www.microsoft.com/en-us/microsoft-365/
enterprise/microsoft-365-copilot , Mar 2023.
[35] Microsoft. PromptFlow. https://github.com/
microsoft/promptflow , November 2023.
[36] Microsoft. Semantic Kernel. https://github.com/
microsoft/semantic-kernel , November 2023.
[37] Deepak Narayanan, Keshav Santhanam, Fiodar
Kazhamiaka, Amar Phanishayee, and Matei Zaharia.
Heterogeneity-Aware cluster scheduling policies for
deep learning workloads. In 14th USENIX Symposium
on Operating Systems Design and Implementation
(OSDI 20) , pages 481–498. USENIX Association,
November 2020.[38] Flemming Nielson, Hanne R Nielson, and Chris Hankin.
Principles of program analysis . Springer, 2015.
[39] Christopher Olston, Fangwei Li, Jeremiah Harmsen, Jor-
dan Soyke, Kiril Gorovoy, Li Lao, Noah Fiedel, Sukriti
Ramesh, and Vinu Rajashekhar. Tensorflow-serving:
Flexible, high-performance ml serving. In Workshop on
ML Systems at NIPS 2017 , 2017.
[40] OpenAI. Chatgpt. https://chat.openai.com/ , Nov
2023.
[41] OpenAI. Gpt-4 technical report, 2023.
[42] OpenAI. Introducing gpts. https://openai.com/
blog/introducing-gpts , Nov 2023.
[43] OpenAI. OpenAI Triton. https://github.com/
openai/triton , November 2023.
[44] OpenAI. Production best practices - ope-
nai api. https://platform.openai.com/
docs/guides/production-best-practices/
improving-latencies , Nov 2023.
[45] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Auto-
matic differentiation in pytorch. 2017.
[46] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo
Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bian-
chini. Splitwise: Efficient generative llm inference using
phase splitting. arXiv preprint arXiv:2311.18677 , 2023.
[47] Chen Qian, Xin Cong, Cheng Yang, Weize Chen,
Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong
Sun. Communicative agents for software development.
arXiv preprint arXiv:2307.07924 , 2023.
[48] Sebastián Ramírez. FastAPI. https://github.com/
tiangolo/fastapi .
[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
Megatron-lm: Training multi-billion parameter language
models using model parallelism. CoRR , abs/1909.08053,
2019.
[50] ShareGPT Team. Sharegpt dataset. https://
sharegpt.com/ , Nov 2023.
[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-
tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,
Aurelien Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample. Llama: Open and efficient foun-
dation language models, 2023.[52] Unknown. Prompt of bing chat. https:
//www.make-safe-ai.com/is-bing-chat-safe/
Prompts_Conversations.txt , Nov 2023.
[53] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Perric
Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen
Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers:
State-of-the-Art Natural Language Processing. pages
38–45. Association for Computational Linguistics, Oc-
tober 2020.
[54] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xi-
aoyun Zhang, and Chi Wang. Autogen: Enabling next-
gen llm applications via multi-agent conversation frame-
work. arXiv preprint arXiv:2308.08155 , 2023.
[55] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. Efficient streaming language
models with attention sinks. arXiv , 2023.
[56] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-
jeong Kim, and Byung-Gon Chun. Orca: A distributed
serving system for Transformer-Based generative mod-
els. In 16th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 22) , pages 521–538,
Carlsbad, CA, July 2022. USENIX Association.
[57] Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma,
Khaled Elmeleegy, Scott Shenker, and Ion Stoica. Delay
scheduling: A simple technique for achieving locality
and fairness in cluster scheduling. In Proceedings of
the 5th European Conference on Computer Systems ,
EuroSys ’10, page 265–278, New York, NY , USA, 2010.
Association for Computing Machinery.
[58] Matei Zaharia, Andy Konwinski, Anthony D Joseph,
Randy H Katz, and Ion Stoica. Improving mapreduce
performance in heterogeneous environments. In 8th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 08) , San Diego, CA, 2008.
[59] Hong Zhang, Yupeng Tang, Anurag Khandelwal, Jin-
grong Chen, and Ion Stoica. Caerus: NIMBLE task
scheduling for serverless analytics. In 18th USENIX
Symposium on Networked Systems Design and Imple-
mentation (NSDI 21) , pages 653–669. USENIX Associ-
ation, April 2021.
[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,
and Luke Zettlemoyer. Opt: Open pre-trained trans-
former language models, 2022.[61] Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang,
Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau,
Yuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing
a GPU cluster for deep learning with guarantees. In 14th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 20) , pages 515–532. USENIX
Association, November 2020.
[62] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-
han Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.
Gonzalez, and Ion Stoica. Judging llm-as-a-judge with
mt-bench and chatbot arena, 2023.
[63] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff
Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Chris-
tos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark
Barrett, and Ying Sheng. Efficiently programming large
language models using sglang, 2023.
[64] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,
Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-
serve: Disaggregating prefill and decoding for goodput-
optimized large language model serving. arXiv preprint
arXiv:2401.09670 , 2024.

Title: A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
Abstract: General large language models (LLMs), represented by ChatGPT, have
demonstrated significant potential in tasks such as code generation in software
engineering. This has led to the development of specialized LLMs for software
engineering, known as Code LLMs. A considerable portion of Code LLMs is derived
from general LLMs through model fine-tuning. As a result, Code LLMs are often
updated frequently and their performance can be influenced by the base LLMs.
However, there is currently a lack of systematic investigation into Code LLMs
and their performance. In this study, we conduct a comprehensive survey and
analysis of the types of Code LLMs and their differences in performance
compared to general LLMs. We aim to address three questions: (1) What LLMs are
specifically designed for software engineering tasks, and what is the
relationship between these Code LLMs? (2) Do Code LLMs really outperform
general LLMs in software engineering tasks? (3) Which LLMs are more proficient
in different software engineering tasks? To answer these questions, we first
collect relevant literature and work from five major databases and open-source
communities, resulting in 134 works for analysis. Next, we categorize the Code
LLMs based on their publishers and examine their relationships with general
LLMs and among themselves. Furthermore, we investigate the performance
differences between general LLMs and Code LLMs in various software engineering
tasks to demonstrate the impact of base models and Code LLMs. Finally, we
comprehensively maintained the performance of LLMs across multiple mainstream
benchmarks to identify the best-performing LLMs for each software engineering
task. Our research not only assists developers of Code LLMs in choosing base
models for the development of more advanced LLMs but also provides insights for
practitioners to better understand key improvement directions for Code LLMs.
Full Text: A Survey of Large Language Models for Code: Evolution,
Benchmarking, and Future Trends
ZIBIN ZHENG, Sun Yat-sen University, China
KAIWEN NING, Sun Yat-sen University, China
YANLIN WANG∗,Sun Yat-sen University, China
JINGWEN ZHANG, Sun Yat-sen University, China
DEWU ZHENG, Sun Yat-sen University, China
MINGXI YE, Sun Yat-sen University, China
JIACHI CHEN, Sun Yat-sen University, China
General large language models (LLMs), represented by ChatGPT, have demonstrated significant potential
in software engineering tasks such as code generation. This led to the development of specialized LLMs for
software engineering, called Code LLMs. Further, Code LLMs are often derived from general LLMs through
fine-tuning and their performance can be influenced by the base LLMs. However, there is a lack of systematic
investigation into Code LLMs. In this study, we conduct a comprehensive survey of Code LLMs to address three
questions: (1) What LLMs are specifically designed for software engineering tasks, and their relationship? (2)
Do Code LLMs outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficient
in different software engineering tasks? To answer these questions, we first collect relevant literature and work
from four databases and categorize Code LLMs based on their publishers. Next, we investigate the performance
differences between general LLMs and Code LLMs in software engineering tasks to demonstrate future trends.
Finally, we comprehensively maintained the performance of LLMs to identify the best-performing LLMs for
each software engineering task. Our research helps researchers understand the evolution and performance of
Code LLMs and provides insights for practitioners to improve Code LLMs.
Additional Key Words and Phrases: large language models (LLMs), Code LLMs, software engineering tasks
ACM Reference Format:
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2024. A
Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends. ACM Trans. Softw.
Eng. Methodol. 1, 1 (January 2024), 44 pages. https://doi.org/10.1145/3488245
1 INTRODUCTION
Large Language Models (LLMs) are blurring the boundaries between human languages and machine
languages with their powerful text understanding and generation capabilities [ 34]. LLMs have
also had a significant impact on various fields, including software engineering [ 115]. Currently, a
∗Yanlin Wang is the corresponding author (wangylin36@mail.sysu.edu.cn).
Authors’ addresses: Zibin Zheng, Sun Yat-sen University, Zhuhai, China, zhzibin@mail.sysu.edu.cn; Kaiwen Ning, Sun
Yat-sen University, Zhuhai, China, ningkw@mail2.sysu.edu.cn; Yanlin Wang, Sun Yat-sen University, Zhuhai, China,
wangylin36@mail.sysu.edu.cn; Jingwen Zhang, Sun Yat-sen University, Zhuhai, China, zhangjw273@mail2.sysu.edu.cn;
Dewu Zheng, Sun Yat-sen University, Zhuhai, China, zhengdw5@mail2.sysu.edu.cn; Mingxi Ye, Sun Yat-sen University,
Zhuhai, China, yemx6@mail2.sysu.edu.cn; Jiachi Chen, Sun Yat-sen University, Zhuhai, China, chenjch86@mail.sysu.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
©2024 Association for Computing Machinery.
1049-331X/2024/1-ART $15.00
https://doi.org/10.1145/3488245
ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article . Publication date: January 2024.arXiv:2311.10372v2  [cs.SE]  8 Jan 20242 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
considerable amount of work focuses on evaluating and optimizing the performance of LLMs on
software engineering tasks such as code generation [ 123], code summarization [ 2], vulnerability
mining [ 114], and more [ 15,117]. However, many studies adopt a negative stance on the current
performance of LLMs in the field of software engineering [ 51,70,125]. This is largely due to
the fact that LLMs may not effectively comprehend the structure and semantics of code, lacking
domain-specific knowledge in software engineering [ 59,85]. Fortunately, we can enhance the
performance and adaptability of LLMs in specific tasks within the software engineering domain
through fine-tuning and additional training [11], thereby designing LLMs specifically tailored for
software engineering tasks. We refer to these LLMs designed for software engineering tasks as
Code LLMs [10].
Code LLMs can not only be generated through fine-tuning but also through traditional pre-
training methods [ 50]. As language models specifically developed for the field of software engineer-
ing, ideally, LLMs can have their capabilities further enhanced and possess stronger abilities in code
generation, vulnerability mining, and other tasks [ 55]. Therefore, there is currently a considerable
amount of work focusing on this area, resulting in the development of many milestone Code LLMs
such as Codex [60], Code Llama [78].
However, due to the uncertainties brought by fine-tuning, training data, and other factors [ 82,119],
as well as the remarkable performance of state-of-the-art general LLMs like GPT-4 in various
software engineering domains [ 98], it is challenging to determine whether the current state-of-
the-art Code LLMs outperform the current state-of-the-art general LLMs in software engineering
tasks. Moreover, there is a wide range of Code LLMs available, many of which are derived through
fine-tuning on general LLMs or other Code LLMs [ 21,62,72,120]. They may possess different
structures and fine-tuning techniques [ 3,64], and it is likely that they excel in different software
engineering tasks. However, there is currently a lack of systematic investigation into Code LLMs
and their performance, particularly in comparison to general LLMs and among different Code
LLMs.
In this study, our goal is to provide a comprehensive review of Code LLMs and their performance.
To achieve this objective, we initially collected relevant literature and works from four major
databases known for publishing new models: GitHub1, dblp2, Google Scholar3, and arXiv4. We
employed a card sorting method [ 14] to remove duplicate and irrelevant papers and further expanded
our list of research targets through a snowballing approach. After the screening process, we
obtained 149 relevant and valid papers. Through this research, we aim to address the following
three questions:
RQ1: What LLMs are specifically designed for software engineering tasks, and what is the
relationship between them?
We classified Code LLMs based on the types of institutions to which their main developers
belong, such as companies, universities, etc. We not only conducted a chronological review of
Code LLMs but also provided a comprehensive summary of their development relationships. These
development relationships include but are not limited to, iterations, fine-tuning, and improvements.
RQ2: Do Code LLMs really outperform general LLMs in software engineering tasks?
To address this question, we selected relevant content from the aforementioned literature,
specifically focusing on experimental sections or evaluation work that compared general LLMs
and Code LLMs. We conducted a detailed statistical analysis and presentation of the experimental
1https://github.com/
2https://dblp.uni-trier.de/
3https://scholar.google.com
4https://arxiv.org/A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 3
findings or evaluation results. Our findings indicate that, for the same model, the newly fine-
tuned models specifically tailored for software engineering tasks outperform their base models.
Furthermore, when the number of parameters is comparable, Code LLMs tend to outperform
general LLMs. Overall, the current state-of-the-art Code LLMs, such as CodeFuse-CodeLlama-34B,
show better performance than the current state-of-the-art general LLMs, such as GPT-4, in code
generation tasks. However, GPT-4 still demonstrates strong competitiveness in other tasks.
RQ3: Which LLMs are more proficient in different software engineering tasks?
We have comprehensively summarized the experimental part of the aforementioned work, manu-
ally annotating the performance of 130 Code LLMs on major benchmarks such as HumanEval [ 16].
Additionally, a considerable amount of research has utilized alternative evaluation methods or
proposed new benchmarks and evaluation metrics. We have also organized and presented the
experimental results from these studies. We found that different Code LLMs do exhibit certain
performance differences across various software engineering tasks. Some of these findings can
assist developers of Code LLMs in making better choices regarding base models and fine-tuning
approaches to develop more advanced LLMs tailored to different software engineering tasks.
The main contributions of this paper are:
•To the best of our knowledge, this is the first systematic review of Code LLMs. We manually
selected 149 relevant works from a large number of articles in four major open-source
communities or databases5;
•We classified Code LLMs based on the types of institutions to which their main developers
belong and organized them chronologically and in terms of their development relationships.
This can help practitioners better understand and utilize Code LLMs;
•We conducted a comprehensive analysis and compilation of the performance of general
LLMs and Code LLMs in software engineering tasks. We provided statistical results and ana-
lyzed interesting phenomena. This can assist developers in understanding key improvement
directions for Code LLMs;
•We maintained the scores of 126 Code LLMs on major benchmarks and conducted a detailed
analysis of their performance across different software engineering tasks. This can aid
Code LLM developers in making informed decisions regarding base models and fine-tuning
approaches.
The organization of this paper is as follows. In Section 2, we describe the methods of literature
collection and the criteria for screening; In Section 3, we summarize and categorize the Code LLMs
and propose an answer to research question one (RQ1); In Section 4, we address research question
two (RQ2); In Section 5, we compiled the performance of different LLMs across various benchmarks
and literature to address research question there (RQ3); In Section 6, we elucidate related work;
Finally, in Section 7, we summarize the entire paper.
2 METHODOLOGY
In this section, we introduce the detailed steps of conducting a literature review. We follow the
method provided by [ 41] for the literature review with consists of three steps: literature search,
literature screening, and data analysis. As shown in Fig. 1.
2.1 Literature Search
Based on the previous literature review [ 41], we selected four databases and communities: dblp,
Google Scholar, GitHub and arXiv. From these search engines, we can not only find publications
5https://github.com/KevinHeiwa/LLM_SE_Papers_List4 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Key Words SearchLiterature 
SearchLiterature 
Selection
Seven  Exclusion 
CriteriaData 
Analysis
masterSKevin1Literature collection and screening process: large -scale collection of 
literature and accurate screening of relevant literature
4 Search Engines
Including  
published 
literature and 
preprint literature
Manually Context
Analysis
Using card 
classification 
methods to check 
data
Paper Reading
Literature 
classification
Discussion and 
Unified 
Conclusion
masterSKevin1Rolling the snowball to expand is the bibliography list and 
problem distillation.
Fig. 1. Overview of methodology design.
Table 1. Number of keyword searches returned by each search engine.
SE LLM Software
Engineer-
ing Large
Language
ModelSoftware
Engineer-
ing LLMSE Large
Language
ModelCode LLM Code Large
Language
Model
GitHub 399 272 4 85 711 328
dblp 22 92 1 139 15 62
Google
Scholar128000 4480000 24100 532000 75000 5280000
arXiv 7 364 26 17 705 2484
in journals, conferences, workshops, and numerous preprint papers but also access the latest
advancements in the industry and open-source projects.
We conducted searches using the following six keywords: “SE LLM", “Software Engineering
Large Language Model", “Software Engineering LLM", “SE Large Language Model", “Code LLM",
and “Code Large Language Model" on the aforementioned four databases. The obtained results
are presented in Table 1. It is worth noting that there might be a significant number of duplicate
papers and irrelevant articles resulting from different keyword searches within the same engine or
the same keyword across different engines. Therefore, we need to manually screen and select these
papers, which is known as literature screening or literature selection.
2.2 Literature Selection
During this stage, we not only need to remove duplicate papers but also filter out irrelevant ones.
For example, there are papers that primarily focus on LLMs or the field of software engineering
but do not include LLM/Large Language Model keywords in their abstracts. Additionally, Google
Scholar returns a large number of unrelated results. According to the industry’s definition of
LLMs [ 115], we excluded works published before 2021. As for GitHub, simple keyword matchingA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 5
may result in numerous irrelevant issues and pull requests. Therefore, we only focused on the work
done in repositories. In summary, we applied the following eight exclusion criteria to filter the
literature:
Exclusion Criteria
•Studies are not written in English.
•Master or Ph.D. theses.
•Keynote papers.
•Studies not related to LLMs.
•Studies not related to software engineering.
•Duplicate papers.
•Studies up to 2021 (not including 2021).
•Results in GitHub except repositories.
In this study, we only focus on the performance of Code LLMs and General LLMs in software
engineering tasks. Therefore, many works that do not involve the demonstration and evaluation
of LLMs’ performance in software engineering tasks are beyond the scope of our research. We
specifically focus on the following topics:
•Code LLMs.
•Application of Different LLMs in Software Engineering.
•Evaluation of LLMs on Software Engineering Tasks
To improve the accuracy of the literature screening process, we will use the card sorting method
to evaluate the collected data. Card sorting is a common technique used to assess data and derive
categories from it. There are three types of card sorting methods: open card sorting, closed card
sorting, and hybrid card sorting. Among these three types, closed card sorting involves predefined
categories [ 14]. In our case, we applied closed card sorting to select relevant papers since we have
only two categories: relevant and irrelevant. Each card will have a title (paper title) and a description
(paper abstract). By using this method, we can systematically evaluate and categorise the papers
based on their relevance to our research.
Three experienced researchers, including one non-coauthors, independently conducted a thor-
ough review of the search engine results from the four databases. After individually organizing the
papers, they engaged in a collaborative discussion to align their findings. Through this rigorous
process, we ultimately identified 121 relevant works that met the criteria for inclusion in our study.
Furthermore, we expanded the paper list using a snowballing approach [ 14]. Specifically, we
manually checked the references of the identified 121 papers and found an additional 13 papers
that met our selection criteria. Therefore, we ultimately selected 134 papers for analysis. The list of
papers can be found at https://github.com/.
2.3 Data Analysis
We used an open card sorting approach to help find the answers to these three research questions.
We carefully read the articles and actively searched for answers related to the two questions shown
in Table 2. If we couldn’t find any answers in a particular paper, it was removed from our list.
For the answers to (1), we primarily examined whether and observed whether the work released
brand new Code LLMs or mentioned other unknown Code LLMs.We organized this information
and categorized the work according to the type of organization (e.g., company, university) in which
the main developers are located, as shown in Fig. 2. We can see that there are 18 corporate-led
Code LLMs, with Microsoft publishing the most, accounting for 8. The report publishes a total of
18 Code LLMs, while research teams and communities publish the most Code LLMs in comparison,
with 21. The specifics of these tasks are presented in 3. For the answers to (2), We mainly read6 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Fig. 2. Number of Code LLMs issued by organizations.
Table 2. Data Collection for Each RQ.
RQs Type of Data We Collected
RQ1 What LLMs are specifically designed for software engineering tasks, and what is the
relationship between them? For example, which LLMs are fine-tuned based on other
LLMs?
RQ2 Do Code LLMs really outperform general LLMs in software engineering tasks? For in-
stance, do Code LLMs exhibit better performance than general LLMs in code generation
tasks?
RQ3 Which LLMs are more proficient in different software engineering tasks? Different
LLMs may excel in different software engineering tasks.
whether the articles include in the experimental section a comparison of the performance of Code
LLMs with general LLMs on software engineering tasks. Especially those example studies, and
research articles.
For the answers to (3), we organized the performance of LLMs according to different software
engineering tasks, including code generation, code summarization, code translation, and vulnera-
bility detection. We primarily looked for experimental sections in various papers that involved the
comparison of different LLMs. Additionally, we compiled the performance of different LLMs on
benchmarks commonly used to assess software engineering tasks. In this section, we focused on
evaluating articles and papers introducing new Code LLMs.
3 LLMS FOR SOFTWARE ENGINEERING
In this section, we answer and summarize mainly for RQ1 . We organize our collection of LLMs
designed specifically for software engineering tasks and show the relationships between them.
According to the different release times, we can obtain Fig. 3. We can see that with the increase
in years, more and more companies, organizations, and research teams have been involved in the
development of Code LLMs [ 17,27,40,122]. This fully demonstrates the high level of attention
from both the industry and academia to the performance of LLMs in software engineering tasks. In
order to better illustrate the development process of different LLMs, we will elaborate on the types
of affiliations of the main developers of Code LLMs in the following text. These include compa-
nies, organizations, research teams & open-source communities, and individuals & anonymous
contributors.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 7
3.1 Company-led LLMs
Microsoft: Microsoft has been at the forefront of exploring numerous LLMs in the field of software
engineering and has produced several LLMs specifically designed for software engineering tasks.
In 2020, Microsoft introduced the first LLM for programming, GPT-C [ 88], followed by the release
of PyMT5 [ 20] and CodeGPT [ 56]. GPT-C is a variant of GPT-2 [ 37], belonging to the class of
multilayer generative Transformer models. GPT-C underwent retraining on a large-scale unsuper-
vised multilingual source code dataset and achieved an average edit similarity of 86.7% on code
completion tasks in the Python programming language. On the other hand, PyMT5 is a code-based
LLM tailored for Python and natural language. Its architecture is built on the encoder-decoder
transformer framework. PyMT5 was trained on a massive parallel corpus containing 26 million
Python methods and 7.7 million method-docstring (documentation strings) pairs. Its primary focus
is on translation tasks between Python methods and method-docstrings, such as predicting Python
methods based on docstrings or generating summaries for Python code. The article also showcases
the performance of PyMT5 on the CodeSearchNet [ 36]. CodeGPT is another LLM developed by
Microsoft for software engineering. It shares the same model architecture and training objectives
as GPT-2. CodeGPT is pretrained on the CodeSearchNet dataset and primarily used for code com-
pletion and code generation tasks. There are two variants of CodeGPT. The first variant, CodeGPT,
involves retraining the model with randomly initialized parameters. The second variant, CodeGPT-
adapted, involves training directly on top of the GPT-2 model. The article evaluates CodeGPT’s
performance in code completion tasks on two datasets: PY150 [ 74] and Github Java Corpus [ 4]. The
results indicate that both CodeGPT and CodeGPT-adapted outperform methods such as LSTM [ 33]
and Transformer [95], with CodeGPT-adapted achieving the highest scores.
Phi-1 [ 31] is another code-based LLM proposed by Microsoft Research. Phi-1 utilizes a decoder-
only transformer architecture and has a size of 1.3B. It was trained on a high-quality dataset of
7B samples. Despite its smaller model size and dataset, Phi-1 outperforms some models trained
with larger parameters and larger-scale datasets on multiple benchmarks. This demonstrates that a
high-quality dataset can significantly enhance model performance.
Given the strong potential demonstrated by phi-1, Microsoft continues to explore the power of
small-scale Transformer-based language models with the subsequent version, phi-1.5 [ 52]. Phi-1.5
is a 1.3 billion parameter LLM trained primarily on a specially curated “textbook-quality" synthetic
dataset. The paper indicates that phi-1.5 achieves performance on par with models five times its size
in natural language tasks and surpasses most non-cutting edge LLMs in more complex inference
tasks like elementary math and basic coding. Regarding the impressive performance of phi-1.5, the
article emphasizes the importance of data quality. While the model still lags behind the capabilities
of the largest LLMs, it showcases features previously only seen in larger models and demonstrates
the feasibility of achieving high-level functionality in smaller LLMs.
Microsoft also released a code-based LLM called JuPyT5 [ 12], which was trained on all publicly
available Jupyter Notebook GitHub repositories. Chandel et al.[ 12] aimed to explore a Data Science
assistant based on the Seq2Seq transformer architecture and introduced a new metric called Data
Science Problems (DSP) in this paper. This metric evaluates the model’s proficiency in using Python
for mathematics and data science by utilizing a curated set of 1,119 questions from educational
notebooks. The results of the study show that JuPyT5 was able to solve 77.5% of the DSP problems
in 100 sampling tests.
PyCodeGPT [ 109] is a library-oriented code generation model developed by Microsoft using
unlabeled code corpora for training. However, the main focus of the article is on introducing CERT,
a novel two-stage approach for library-oriented code generation. On the HumanEval benchmark,
PyCodeGPT (110M) achieves a competitive pass@1 rate of 8.33%.8 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
WizardCoder [ 57] is an open-source large model co-developed by Microsoft and Hong Kong
Baptist University. It applies the Evol-Instruct method mentioned in WizardLM [ 102] to the field of
code and utilizes the Code Alpaca’s 20K instruction-following code dataset to obtain 78K fine-tuning
data containing code instructions of varying complexity. WizardCoder uses StarCoder [ 50] as the
base model and fine-tunes it on the aforementioned 78K instruction-based fine-tuning dataset. The
article also tests WizardCoder on four widely recognized code generation benchmark metrics.
Google: Google has been exploring ways to improve the safety and quality of LLMs’ output. They
propose addressing this challenge by fine-tuning models using annotated data and enabling models
to query external sources of knowledge. Based on this approach, Google introduced LaMDA [92],
a series of decoder-only Transformer-based LLMs ranging from 2B to 137B parameters. LaMDA
undergoes pretraining on 1.56 trillion words from public dialogue data and web text. It is then
fine-tuned using annotated data and equipped with the ability to invoke external retrieval tools’
APIs.
Google also explores the impact of model scale on few-shot learning [ 18]. They introduce a new
machine learning system called Pathways, which allows efficient training of large models across
multiple TPU Pods. Utilizing this system, they trained a dense activation Transformer language
model with 540B parameters called Pathways Language Model (PaLM). PaLM uses a standard
decoder-only Transformer model architecture with some modifications, such as using the SwiGLU
activation function in MLPs, employing parallel forms in each Transformer block, and utilizing
multi-query attention.
For code-related tasks, researchers collected a Python code dataset called ExtraPythonData. They
fine-tuned PaLM on this dataset for code tasks, resulting in the model PaLM-Coder. PaLM-Coder
achieves excellent performance on multiple code generation benchmarks and the DeepFix code
repair test [18].
However, current LLMs do not perform well when it comes to evaluating more complex and
unknown problems that require surpassing the mere translation of instructions into code [ 53]. For
instance, understanding algorithmic and complex natural language competition-style programming
problems. To address this, Google DeepMind proposed AlphaCode [ 53], which aims to tackle these
programming problems that require deep reasoning to find novel solutions.
AlphaCode utilizes an asymmetric Encoder-Decoder Transformer architecture, consisting of a
shallow encoder and a deep decoder. This modification significantly improves training efficiency
without compromising inference speed. When evaluated on Codeforces, AlphaCode’s performance
is considered average, roughly on par with the median competitor. Interestingly, the authors
note that scaling upsampling and filtering samples to cluster them into a small set, as well as
introducing new efficient transformers that support large-scale sampling, can effectively enhance
the performance of LLMs.
Meta: Facebook AI Research has introduced InCoder [ 26], a unified generative model that can
perform program synthesis through left-to-right generation and code editing through masking
and infilling. InCoder is the first large-scale generative code model capable of filling arbitrary code
regions. It is trained on a large corpus of code files with permissive licenses, where random code
regions are masked and moved to the end of each file, allowing bidirectional context for code
completion.
InCoder adopts the MoE (Mixture of Experts) model architecture [ 5]. The paper highlights that
training code generation models with causal masking objectives can achieve strong zero-shot
performance on challenging and practically meaningful code completion and editing tasks. It also
lays the groundwork for future work on supervised filling and editing via model fine-tuning and
iterative decoding.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 9
Meta has also introduced Code Llama [ 78], a large-scale code language model series based on
Llama 2. Code Llama offers multiple versions, including a base model (Code Llama), a Python
specialization model (Code Llama - Python), and an instruction-following model (Code Llama -
Instruct), each available with 7B, 13B, and 34B parameters. All models are trained on 16k token
sequences and demonstrate improvements on inputs with up to 100k tokens. The paper provides
extensive experimental evidence to demonstrate the superiority of Code Llama. Meta has also made
Code Llama available for research and commercial use under licensing terms.
Huawei: PanGu-Coder [ 19] is a pre-trained decoder language model proposed by Huawei Noah’s
Ark Lab. It adopts the PANGU- 𝛼architecture [ 111] and is specifically designed for text-to-code
generation. Pangu-Coder utilizes a two-stage training strategy: in the first stage, it undergoes
causal language modelling (CLM) pre-training on raw programming language data, while in the
second stage, a combination of CLM and masked language modelling (MLM) training objectives
is used, focusing on downstream tasks of text-to-code generation. It is trained on loosely filtered
natural language program definitions and code-function pairs, using a dataset of 147GB of code
data collected from GitHub.
The results of the study show that the two-stage training approach helps achieve comparable or
even better performance than models of similar scale, with smaller context windows and fewer
training data requirements. Additionally, the experiments with PANGU-CODER-FT demonstrate
that by carefully selecting and fine-tuning with data closely related to the target task, the base
model’s scores can be improved more quickly, as the model is sensitive to data distribution mismatch
and shifts during fine-tuning. Building upon PanGu-Coder, Huawei Cloud has launched the Huawei
Cloud Intelligent Programming Assistant CodeArts Snap to enhance developers’ programming
efficiency.
PanGu-Coder2 [ 81] is an iterative upgrade of PanGu-Coder. PanGu-Coder2 achieved a pass@1
rate of 62.20% in the OpenAI HumanEval benchmark test. Furthermore, through extensive eval-
uations on CoderEval and LeetCode benchmarks, the paper demonstrates that PanGu-Coder2
consistently outperforms all previous Code LLMs. Interestingly, PanGu-Coder2 is developed based
on a novel framework called RRTF (Rank Responses to align Test & Teacher Feedback). The paper
claims that this framework can efficiently enhance large-scale pre-trained language models for
code generation.
Baidu: Currently, LLMs in the field of software engineering tend to be English-centric, which
leads to limited comprehension of other natural languages. Therefore, Baidu has proposed a large
code model called ERNIE-Code N90 that supports multiple natural languages and programming
languages. ERNIE-Code is based on the T5 architecture and utilizes two approaches for universal
cross-lingual pre-training. Firstly, it undergoes unsupervised learning using a monolingual pro-
gramming language (PL) and natural language (NL) data. Then, it undergoes supervised learning
on cross-lingual NL-PL or NL-NL pairs to enable the model to learn cross-lingual/modal align-
ment and zero-shot capabilities. The paper acknowledges that while ERNIE-Code demonstrates
strong performance on various tasks involving computer programs and natural language, the lack
of corresponding benchmarks prevents the authors from evaluating ERNIE-Code’s performance
systematically across a wide range of multi-language NLs.
Replit: Replit-Code [ 76] is an open-source code model developed by Replit, primarily targeting
code completion tasks. Replit-Code has a scale of 2.7B and is trained on a subset of the Stack Dedup
v1.2 dataset [ 42]. It offers code completion capabilities for 20 programming languages. However,
the current lack of technical and testing reports for Replit-Code makes it difficult for us to know
the specific performance of Replit-Code on software engineering tasks.
BAAI: AquilaCode-multi [ 119] is a bilingual code model developed by BAAI (Beijing Academy
of Artificial Intelligence). It is trained from scratch on a high-quality corpus of both Chinese and10 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
2021 2022 20232020
MultiPL -TPhi-1GPT-CPyMT5
CodeGPT
 PLBART
 CodeParrot
 CodeT5
 Codex
 GPT-Neo
JuPyT5 PyCodeGPT
 LAMDA PaLM -Coder AlphaCode
PanGu -CoderGPT-J
FIM
CodeRLCodeT5Mix
 GPT-CC
BLOOM
ERNIE -Code
 OpenCoderPlus
 PolyCoder
 MultiCoder
Phi-1.5WizardCoder
 InCoder Code Llama
 PanGu -Coder2
 Code Alpca
CodeT5+ CodeGen CodeGen2
 SantaCoder StarCoderOctoCoder/
OctoGeeX
 CodeGeeX CodeGeeX2
VeriGen
 CodeEditor CodeFuse CodeFuse -Codellama
 Phind -Codellama
 CodeGenAPI
Replit -Code
 AquilaCode
UniXcoder366M 374M
124M 140M -406M 110M -1.5B 220M -770M 12M-175B 125M -1.3B 6B
50M 110M Unknown 8B-540B 284M -41.1B 560M -176B 50M-6.9B
125M -1.3B 560M 15B 160M -2.7B 8B-540B770M2B-137B
1.3B-6.7B
220M -16B350M -16.1B 1.1B7B-13B
1B-16B 13B
2.7B 7B15.5B15B 1.3B
16BUnknown
350M -6B7B-34B
Unknown15B
34B16B/6B 6B
13B 34B1.3B 15B
Fig. 3. Evolution of Code LLMs
English languages, with approximately 40% of the data consisting of Chinese text. This approach
ensures that the model accumulates native Chinese knowledge during the pre-training phase,
rather than relying on translation. The training data for code generation is derived from high-
quality filtered code samples that adhere to open-source licenses. Currently, there is also a lack
of comprehensive analytical data on the capabilities of AquilaCode-multi. Further exploration is
needed to uncover the specific performance of AquilaCode-multi.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 11
3.2 University-led LLMs
PLBART [ 1] N65 is a code-based large language model jointly released by the University of Cali-
fornia and Columbia University. It is built upon the BART architecture and is pre-trained using
denoising autoencoding on a large corpus of Java and Python functions along with their associated
natural language texts. Experimental results demonstrate that PLBART outperforms methods
such as Seq2Seq [ 58], Transformer [ 95], RoBERTa [ 54], and CodeBERT [ 75] on tasks such as code
summarization, code generation, and code translation across seven programming languages.
Carnegie Mellon University conducted a systematic evaluation of code-based large models such
as Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot [ 103]. The article also highlights the
lack of a large-scale open-source model exclusively trained on a multilingual code corpus among
these models. As a result, Carnegie Mellon University introduced PolyCoder, an LLM trained on
a single machine using 249GB of code from 12 programming languages. PolyCoder is available
in three versions: 160M, 400M, and 2.7B. The experimental results in the article demonstrate that
PolyCoder outperforms all the models at that time, including Codex, in terms of performance on
the C programming language. Moreover, it is noteworthy that PolyCoder is open-source, which
adds to its value.
CodeGeeX [ 116] is an open-source multilingual LLM led by Tsinghua University. CodeGeeX
adopts a decoder-only autoregressive (programming) language modelling approach, with a core
architecture consisting of a 39-layer Transformer decoder. The training corpus includes open-source
code datasets such as the Pile, and CodeParrot, as well as publicly available code scraped directly
from GitHub repositories, totaling 850 billion tokens from 23 different programming languages.
The experimental results in the article demonstrate that CodeGeeX consistently outperforms other
open-source LLMs of similar scale in code generation and translation tasks. Furthermore, the
extension capabilities built by CodeGeeX bring significant benefits in improving coding efficiency.
Tsinghua University has released the second generation of CodeGeeX, called CodeGeeX2 [ 93].
Unlike the first generation, CodeGeeX2 is built on the ChatGLM2 architecture and incorporates code
data for pretraining. Leveraging the improved performance of ChatGLM2, CodeGeeX2 demonstrates
performance improvements across multiple metrics. With just 6 billion parameters, CodeGeeX2
achieves nearly a 10% improvement over StarCoder-15B, which has over 15 billion parameters.
Due to inheriting the characteristics of the ChatGLM2-6B model, CodeGeeX2-6B provides better
support for both Chinese and English inputs. It also supports a maximum sequence length of
8192 and significantly improves inference speed compared to the first-generation CodeGeeX-13B
model. After quantization, CodeGeeX2-6B only requires 6GB of GPU memory to run and supports
lightweight local deployment.
MultiCoder [29] is a collaborative development by Harbin Institute of Technology and Huawei
Noah’s Ark Lab. It addresses the challenge of code completion on low-resource programming
languages (PL) that are widely used by developers but present difficulties for data-driven approaches.
To enhance code completion capabilities for low-resource PLs, Gong et al. [ 29] proposes MultiCoder,
which leverages MultiPL pre-training and a MultiPL Mixture-of-Experts (MoE) layer. The article
also introduces a novel PL-level MoE routing strategy (PL-MoE) to improve code completion across
all PLs. Performance analysis of MultiCoder is conducted on CodeXGLUE and MultiCC datasets.
The results indicate that MultiCoder significantly outperforms the MonoPL baseline in low-resource
programming languages. Importantly, the PL-MoE module further enhances performance across
six programming languages.
Cassano et al. [ 10] also focus on low-resource languages and propose an effective method called
MultiPL-T to improve the performance of low-resource language Code LLM using semi-synthetic
data. By utilizing the data generated by MultiPL-T, the article presents a fine-tuned version of12 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
StarCoderBase and achieves better performance on benchmark problems for Racket, OCaml, and
Lua languages.
Thakur et al. [ 91] explored the ability of LLMs to generate high-quality Verilog code and fine-
tuned a new LLM, called VeriGen, based on the CodeGen-16B architecture using Verilog datasets
collected from GitHub and Verilog textbooks. The experimental results of the paper demonstrate that
VeriGen outperforms the state-of-the-art commercial GPT-3.5-turbo model in terms of generating
Verilog code, showing an overall improvement of 1.1%. Although the improvement is not significant,
it showcases the potential of small-scale LLMs in hardware design automation.
Due to the frequent usage of private library APIs by programmers when writing proprietary
code, LLMs lack the capability in this aspect. This is because, during pre-training, LLMs have
limited exposure to these private libraries. To address this issue, Zan et al. [ 108] propose a novel
framework to simulate the process of programmers writing private code. The framework consists
of two modules: APIFinder, which retrieves potentially useful APIs from API documentation,
and APICoder, which utilizes these retrieved APIs to generate private code. Furthermore, Zan et
al. [108] introduce an enhanced version of the APICoder called CodeGenAPI, developed based on
GodeGen, an LLM. The paper also creates four benchmark metrics for private libraries, including
TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNumEval. Test cases are written for
each benchmark test to support the comprehensive evaluation of CodeGenAPI.
Zhang et al. [ 112] focuses on the task of code translation, which involves converting code changes
from one programming language to another. A language model called Codeeditor is designed and
implemented for this purpose. Codeeditor explicitly models code changes as an editing sequence
and learns cross-language associations for the changes. To evaluate Codeeditor, the paper collects
6,613 consistent code changes from 8 pairs of open-source software projects that have similar
functionality in two programming languages: Java and C#. The results of the study show that
Codeeditor outperforms both Codex and CodeT5 by a significant margin across various commonly
used metrics. The performance of Codeeditor on a single software engineering task provides new
insights for the development of Code LLMs and complements the capabilities of existing Code
LLMs. The combination of these models can ensure higher performance overall.
3.3 Research teams & Open source communities-led LLMs
EleutherAI: EleutherAI has conducted equivalent reproductions of the non-open-source GPT-3
N71 model and released two open-source equivalents: GPT-Neo (125M 2.7B) N59 and GPT-J (6B)
N60, corresponding to GPT-3 Ada (2.7B) and GPT-3 Babbage (6.7B), respectively. To reproduce
GPT-3, EleutherAI first proposed a 825GB English text corpus called “the Pile" N72, which includes
95GB of code data sourced from GitHub. GPT-Neo is a GPT2-like causal language model trained on
the Pile dataset and utilizes the mesh-tensorflow library for parallelism. GPT-J is an autoregressive
text generation model with 6 billion parameters trained on the Pile using Mesh Transformer JAX.
Although GPT-Neo and GPT-J outperform GPT-3 models with equivalent parameters on the tested
NLP reasoning benchmarks, they still have a significant gap when compared to the largest GPT-3
model, GPT-3 Davinci (175B). Additionally, due to the inclusion of code data in the training set, both
GPT-Neo and GPT-J show some performance in code-related tasks. GPT-NeoX [ 8], an open-source
LLM by EleutherAI, is a 20B model and currently the largest publicly available model in terms of
weight. It is trained on the Pile dataset with minor modifications to the GPT-J architecture. However,
the paper does not provide experimental evaluations of the programming capabilities of GPT-NeoX.
Instead, it primarily showcases the impressive performance of GPT-NeoX on mathematical tasks.
OpenAI: In 2021 and 2022, OpenAI introduced two LLMs, Codex [ 17] and FIM [ 7], designed to
assist with programming tasks. Codex [ 17] is a GPT-3 model fine-tuned using publicly available code
from GitHub, with a maximum parameter count of 12B. The research paper also introduces a newA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 13
evaluation benchmark called HumanEval, which involves synthesizing programs from docstrings.
HumanEval consists of 164 handwritten programming questions and has become an important
tool for evaluating the performance of large models on software engineering tasks. Experimental
results show that in a single sample from HumanEval, Codex solved 28.8% of the problems, while
GPT-3 solved 0% and GPT-J solved 11.4%.
Building upon Codex, GitHub collaborated with OpenAI to develop a code generation tool
called GitHub Copilot. GitHub Copilot leverages context and prompts to automatically generate
code snippets, comments, and more. It provides programming suggestions and code completion
functionality to assist developers in their coding tasks.
Indeed, all model classes, including Codex and GPT-3, have limitations when it comes to infilling,
which involves generating text at a specific location within a prompt while conditioning on both a
prefix and a suffix [ 17]. Left-to-right models can only condition on the prefix and cannot handle this
type of task effectively. To address this limitation, OpenAI proposed the addition of fill-in-the-middle
(FIM) capability within the paradigm of LLMs [7].
The FIM framework allows for simple modifications to the training data without altering the
model structure, enabling the models to handle fill-in-the-middle tasks more effectively. The research
paper showcases eight causal transformer decoder models trained on the FIM framework, which
have architectures similar to Codex and GPT-3. To prevent contamination from the HumanEval
dataset, which was used for evaluation, FIM utilized the same 159GB code dataset as Codex for
pre-training. The models were initialized with random parameters similar to GPT-3. Due to the
efficiency of the FIM training framework, subsequent models such as Incoder and SantaCoder also
adopted this approach.
Salesforce Research: Salesforce Research has released three LLM models in the field of software
engineering: CodeT5 [100], CodeRL [45], and CodeGen [67].
CodeT5 [ 100] is a pre-trained encoder-decoder model based on the T5 architecture. The developers
of CodeT5 recognized that previous code models treated source code as token sequences similar
to natural language, overlooking the rich structural information present in code. To address this,
the authors proposed a novel identifier-aware mechanism that enables the model to distinguish
which tokens are identifiers and recover them when they are masked. CodeT5 was trained on the
CodeSearchNet dataset and further fine-tuned on the CodeXGLUE [ 56] benchmark. Experimental
results demonstrate that CodeT5 outperforms previous works on most tasks within the CodeXGLUE
benchmark, showcasing its improved performance in the software engineering domain.
CodeRL [ 45] introduces a training framework that leverages reinforcement learning to improve
the performance of pre-trained language models in program synthesis tasks. Based on this frame-
work, the authors used CodeT5 [ 100] as the base model and trained the CodeRL model on the GCPY
dataset, which focuses on Python code. During training, the code generation language model is
treated as an actor-network, and a critic network is introduced to predict the functional correct-
ness of the generated programs and provide dense feedback signals to the actor. Experimental
results presented in the paper demonstrate that CodeRL surpasses CodeT5 not only on the APPS
benchmark test and MBPP benchmark test but also exhibits strong zero-shot transfer learning
capabilities. This indicates that CodeRL can effectively generalize its learning to new tasks and
datasets without explicit training on them.
By increasing the model size and data scale, the language modelling capability can be improved,
enabling LLMs to understand long contexts and generate coherent responses. Therefore, utilizing
this capability can lead to a better understanding of user intent and achieve improved program
synthesis results [ 67]. To validate this approach’s effectiveness, the authors propose a multi-step
program synthesis method and introduce a new LLM called CodeGen [ 67] for verification. CodeGen14 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
is also used for program synthesis tasks, adopting the form of an autoregressive transformer.
CodeGen is trained on three datasets: the Pile, BigQuery, and BigPython.
The authors initially trained CodeGen on the Pile dataset. The Pile dataset is a large-scale
English text corpus open-sourced by EleutherAI, which includes a substantial amount of GitHub
code data. This training results in the first version of CodeGen-NL. Subsequently, developers
utilize the parameters of CodeGen-NL as initial parameters for training on BigQuery. BigQuery
is a publicly available multi-programming language code dataset provided by Google, and six
programming languages are selected for this purpose, leading to the second version, CodeGen-
MULTI. Finally, developers employ the parameters of CodeGen-MULTI as initial parameters for
training on BigPython. BigPython is a single-language code dataset focused on Python. Upon
completion of training, CodeGen-MONO is obtained.
The experiments in the article demonstrate that multi-step program synthesis can enhance the
model’s program synthesis capability. Furthermore, the multi-step program synthesis capability
exhibits a linear relationship with the model size and data scale.
Salesforce Research also explored improving the efficiency of LLMs for program synthesis training
by unifying four key components: model architecture, learning methods, infill sampling, and data
distribution. In this exploration, researchers trained four models with parameter sizes of 1B, 3.7B,
7B, and 16B, referred to as CodeGen2 [ 66]. Regarding model architecture, CodeGen2 attempted
to unify the encoder and decoder models into a single prefix-based language modelling (Prefix-
LM) [ 73]. This approach aims to unify bidirectional encoder representations and unidirectional
decoder representations.
In terms of learning methods, CodeGen2 employed an algorithm that combines a causal language
modelling objective with span corruption. And it also explored the “free lunch" hypothesis. In the
aspect of data distribution, CodeGen2 investigated the impact of mixing programming language
and natural language distributions on model performance.
The experimental results showed the following findings:
•The Prefix-LM architecture did not demonstrate measurable improvements in this task.
•Training a model with infill sampling did not provide a free lunch.
•A simple mixture of causal language modeling and span corruption, limited to within-file
spans, was sufficient.
•A mixture distribution of programming and natural languages showed promising results.
Salesforce Research also introduced an upgraded version of CodeT5 called CodeT5+ [ 98]. CodeT5+
allows flexible combinations of its component modules to adapt to various downstream code tasks.
The authors of the article proposed the use of mixed pretraining objectives to alleviate the dis-
crepancy between pretraining and fine-tuning stages. They covered multiple pretraining tasks,
including span denoising, contrastive learning, text-code matching, and causal language modeling,
across both single-modal and multimodal code corpora. Furthermore, the authors suggested ini-
tializing CodeT5+ with frozen pre trained LLMs, without the need for training from scratch. This
approach effectively scales up these models and explores instruction conditioning to align with
natural language instructions.
CodeT5Mix [ 99] is based on CodeT5. Building upon CodeT5, CodeT5Mix introduces an encoder
component, RoBERTa, and two decoder components, GPT-2 and CodeGPT. By combining these
components (or using them individually), CodeT5Mix is able to handle various code-related tasks.
The training incorporates diverse pre-training tasks and employs an efficient weight-sharing
strategy. CodeT5Mix comes in two variants: base (220M) and large (770M). The results of the
study demonstrate that it can fully support a semi-parametric retrieval-augmented code generation
approach, as the model effectively performs in both code retrieval and generation tasks.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 15
CodedotAI: CodedotAI attempted to replicate GitHub Copilot and proposed the GPT-CC (GPT-
Code-Clippy) [ 65] model. GPT-CC utilized the GPT-Neo model as its base model and was trained
on Code Clippy Data, with fine-tuning performed on APPS and CodeSearchNet Challenge Data.
Code Clippy Data is a 132GB dataset created by developers by deduplicating and merging code
data collected from GitHub with code data from the Pile. However, GPT-CC did not provide a
comparison with other LLMs in terms of performance on software engineering tasks, nor did it
further analyze the effectiveness and superiority of Code Clippy Data.
Huaggingface: Huaggingface has made significant contributions to the open-source landscape
of LLMs in the field of software engineering. They have provided a training tutorial similar to Codex,
allowing users to build a large model called CodeParrot) [ 35] from scratch. CodeParrot is based
on the GPT-2 architecture, and users can train both 110M and 1.5B versions of CodeParrot using
just 150 lines of code. Huaggingface has also introduced the BigCode project, which is a scientific
collaborative effort dedicated to developing large code models. Within the BigCode project, they
proposed SantaCoder [ 3], an LLM based on the FIM (Filling in the Missing) model. SantaCoder’s
Tokenizer draws inspiration from InCoder and uses the Hugging Face Tokenizer [ 63]. It employs
the Byte-Pair Encoding (BPE) algorithm to train the original bytes. The Stack v1.1 is a dataset
released by Huaggingface in the BigCode community, containing 6.4TB of source code data from
384 programming languages. This dataset has been filtered to remove sensitive information such
as usernames while generating it.
In addition, the BigCode community has also released an open-sourced StarCoder [ 50], claiming
it to be one of the most advanced LLMs for assisted programming currently available. StarCoder
is divided into two versions: StarCoder and StarCoderBase. The architecture of StarCoderBase
is similar to SantaCoder, with a parameter size of 15.5B and the utilization of techniques such
as FIM (Filling in the Missing) and MQA (Multi-Question Answering). StarCoderBase is trained
on 1 trillion tokens from the Stack dataset and then fine-tuned on 35B Python code data to
obtain StarCoder. StarCoderBase outperforms other existing open-source code LLMs in popular
programming benchmarks. Additionally, it has a context length of over 8,000 tokens, allowing
StarCoder to handle larger inputs than any other open-source LLM.
In Muennighoff et al. [ 64], the BigCode community introduced COMMITPACK, a 4TB Git commit
dataset covering 350 programming languages. This dataset includes paired information between
code changes and human instructions, which aids in better instruction-based fine-tuning of LLMs.
The article also introduces HUMANEVALPACK, which extends the HumanEval benchmark to three
coding tasks (code fixing, code interpretation, and code synthesis) across six languages (Python,
JavaScript, Java, Go, C++, Rust). Additionally, the article presents two Code LLM models, OctoCoder
and OctoGeeX. Experimental results demonstrate that these two LLMs achieve optimal performance
on HUMANEVALPACK and outperform GPT-4 in terms of performance.
BigScience Workshop: The BigScience Workshop introduced an open-source 176B language
model called BLOOM [ 79]. It is a decoder-only Transformer language model trained on the ROOTS
corpus. ROOTS is a composite collection consisting of 498 Hugging Face datasets [ 46] with a
total text size of 1.61TB, covering 46 natural languages and 13 programming languages. BLOOM
achieves competitive performance in various benchmark metrics and shows stronger results after
multi-task prompt fine-tuning. Unlike considering a mixture of expert models (MoE) or the state-
space model [ 30], BLOOM employs causal decoder-only models. However, the article does not
provide a comprehensive evaluation of BLOOM on software engineering tasks. Only a few test
results indicate that BLOOM performs weaker than Codex in code generation. This suggests that
multitasking fine-tuned BLOOMZ models do not significantly improve over BLOOM models.
OpenChat: OpenCoderPlus [ 96]is a code-oriented large language model in the OpenChat series,
fine-tuned on a dataset of code. The base model for OpenCoderPlus is StarCoderPlus. According to16 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
the data provided by the authors, OpenCoderPlus achieves 102.5% of the ChatGPT score on the
Vicuna GPT-4 evaluation and has a win rate of 78.7% on AlpacaEval.
CodeFuse: CodeFuse-MFTCoder [ 62]is an open-source project by CodeFuse that focuses on
multi-task code large language models (Code-LLMs). It includes models, datasets, training code
repositories, and inference guidelines. MFTCoder supports most mainstream open-source large mod-
els, particularly those related to Code-LLMs, such as Code-LaMA and Starcoder. Currently, CodeFuse
has released two high-quality code-related instruction fine-tuning datasets: Evol-instruction-66k
and CodeExercise-Python-27k. They have also introduced two models: CodeFuse-13B and CodeFuse-
CodeLlama-34B. According to the results presented in MFTCoder [ 62], CodeFuse-CodeLlama-34B
outperforms GPT-4 in terms of performance on HumanEval.
Phind: Indeed, there are several Code LLMs that have been fine-tuned based on Code Llama,
including Phind-CodeLlama [ 72]. Phind-CodeLlama offers three versions that are fine-tuned from
Code Llama: Phind-CodeLlama-34B-v1, Phind-CodeLlama-34B-v2, and CodeLlama-34B-Python.
Among them, CodeLlama-34B and CodeLlama-34B-Python achieved 67.6% and 69.5% pass@1 on
the HumanEval benchmark using an internal Phind dataset. Additionally, Phind-CodeLlama-34B-v2
achieved an even higher score of 73.8% pass@1 on the HumanEval benchmark. These scores surpass
the performance of GPT-4, which achieved 67% pass@1 on the same benchmark.
3.4 Individuals & Anonymously-led LLMs
CodeAlpaca [ 13] is an instruction-guided LLaMA [ 94] model trained based on code generation
instructions. LLaMA is a set of efficient open-source large language models trained on publicly
available datasets using the standard transformer architecture. Alpacaon the other hand, combines
the Self-Instruct [ 97] method with LLaMA to create a dataset for instruction-based fine-tuning.
LLaMA is then fine-tuned on this instruction-based dataset to create an LLM. Furthermore, CodeAl-
paca focuses on code generation, editing, optimization tasks, and similar code-related tasks by
placing emphasis on instruction-based fine-tuning using the Alpaca model. The LLaMA model is
fine-tuned on instruction-based data in these code domains, resulting in a large model capable of
following code instructions.
We have also mapped out the relationships between these Code LLMs as shown in Fig. 4. The
arrows indicate that the model being pointed to is derived or developed from the model pointing to
it. This development can include, but is not limited to, improvements, fine-tuning, or borrowing of
techniques.
4DO CODE LLMS REALLY PERFORM BETTER THAN GENERAL LLMS IN SOFTWARE
ENGINEERING TASKS?
In this section, we primarily focus on addressing and summarizing RQ2 . First, we will filter out the
papers from our collection that evaluate Code LLMs, introduce new evaluation benchmarks, or
propose new Code LLMs. Then, we will thoroughly examine all the literature and identify papers
that compare Code LLMs with general LLMs in their evaluation sections. We will organize this
information and focus on presenting the evaluation results.
Indeed, it is important to consider the parameter scale when comparing different LLMs, as
variations in parameter size can lead to significant performance differences [ 110]. When comparing
LLMs, it is crucial to take into account their specific parameter sizes.
Jiang et al. [ 38] compares the performance of code-davinci-002-175B and text-davinci-002 in three
metrics: Pass@1 of HumanEval, CodeBLEU, and AvgPassRatio. Since text-davinci-002is based on
code-davinci-002 (175B), there is no significant performance difference between the two. However,
compared to text-davinci-003, the performance difference becomes more apparent. After RLHF,
text-davinci-003 outperforms code-davinci-002 in all aspects. From the results of N2, it may beA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 17
Phi-1.5
GPT-2
GPT-3
T5
CodeGen
VeriGen
CodeT5
Llama 2
CodeGeeX CodeGeeX2
SantaCoderStarCoderOpenCod
erPlus
MultiCoderWizardCoder
Phi-1Llama
Alpaca
Code AlpacaCode 
Llama
CodeFuse -
CodellamaPhind -
Codellama
GPT-CGPT-NeoGPT-
NeoXGPT-CC
PyCodeGPTCode
GPT
CodeP
arrotPolyCoder
CodexPyMT5
JuPyT5ERNIE -
Code
CodeRL
CodeT5MixCodeT5+
PaLM -Coder
PanGu -
CoderPanGu -Coder2CodeGenAPI
Replit -Code
AquilaCodeCodeGen2
UniXco
der
Fig. 4. The relationships between Code LLMs
difficult to draw useful conclusions. We can conclude that the current code generation capability of
text-davinci-003 (175B) is stronger than code-davinci-002.
Maddigan et al. [ 60] evaluate the performance of ChatGPT, Codex, and GPT-3 in the task of
natural language generation data visualization. The article evaluates them through five case studies,
and in all five case studies, ChatGPT performs better than Codex and GPT-3.
Shirafuji et al. [ 82] evaluate the robustness of source code, Codex, CodeGen, InstructGPT, and
ChatGPT, popular LLMs, in solving programming problems by changing prompt formats and
modifying problem descriptions. The results of the article show that the new models combined
with RLHF techniques have stronger robustness towards the format of problem descriptions. In
other words, ChatGPT’s ability exceeds the other three models. Li et al. [ 48] mainly compare Codex
and ChatGPT in three code generation benchmark metrics, SCoT prompts, and Pass@k of the
benchmarks. The results also show that ChatGPT is more powerful than Codex in code generation
tasks.
Li et al. [ 47] introduce BIRD, a benchmark test for English large-scale cross-domain text-to-SQL.
T5, ChatGPT, and Codex are tested on this benchmark. The experimental results show that ChatGPT
and Codex perform better than T5 in both The Valid Efficiency Score and The Execution Accuracy.
However, there is not much difference in performance between ChatGPT and Codex, and relatively
speaking, ChatGPT is slightly better. However, they still fall far short of human performance.
Yang et al. [ 105] present the performance of CodeParrot-1.5B, CodeParrot-small-110M, PolyCoder-
160M, PolyCoder-400M, GPT-Neo-125M, and GPT-Neo-1.3B on OpenAI’s HumanEval benchmark.
We can see that different parameter sizes of the same model do have an impact on performance but
to varying degrees. Among the six models, GPT-Neo-1.3B performs the best, even surpassing the
larger parameter size of CodeParrot-1.5B. However, GPT-Neo-125M performs poorly, even worse
than the smaller parameter size of CodeParrot-small-110M.
Thakur et al. [ 91] include an experimental section evaluating the ability of LLMs to generate
high-quality Verilog code. The experimental results show that the fine-tuned CodeGen-16B model
outperforms the GPT-3.5-Turbo model, with an overall improvement of 1.1%. However, the article
also mentions that GPT-4 performs exceptionally well on these tasks, especially in advanced18 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
problem-solving. Due to limited API access to GPT-4, the article does not provide a detailed
comparative evaluation of GPT-4.
Siddiq et al. [ 83] includes an experiment that quantifies the percentage of compilable code
snippets in Java and Python code generation tasks for 11 LLMs. It is observed that CodeGen-2B-
multi performs better for Java code, while GPT-3.5 performs relatively better for Python code
generation. The article also shows the performance of the 11 LLMs on NDCG@10. Interestingly,
GPT-3.5 achieves a higher score in the Java task, while CodeGen-2B-mono obtains a higher score
in the Python code generation task. From this article, we can not determine which model performs
better between CodeGen-2B and GPT-3.5.
Sun et al. [ 86] presents a performance comparison of several LLMs on the Spider Dev Split test
suite, where GPT-4 performs relatively the best.
Sun et al. [ 87] tests the performance of ChatGPT against NCS, CodeBERT, and CodeT5 on the
CSN-Python dataset for code summarization. The results show that ChatGPT performs relatively
poorly, while CodeT5 achieves the best performance.
Wang et al. [ 98] introduce CodeT5+ and compare it to current major LLMs. Compared to
current major Code LLMs, CodeT5+ demonstrates better performance in pass@k(%) on HumanEval.
However, there is still a performance gap between CodeT5+ and models like GPT-4, GPT-3.5, and
code-davinci-002.
Li et al. [ 50] compare StarCoder with most mainstream Code LLMs. The results of the article
show that StarCoder outperforms general LLMs, PaLM, and various versions of LaMDA in Python
program metrics, demonstrating better performance.
Li et al. [ 49] compare the performance of Code LLMs (Codex) and NL LLMs (GPT-3) on the task
of information extraction (IE). The article primarily metrics named entity recognition (NER) and
relation extraction (RE). The experimental results show that Code LLMs outperform GPT-3 in both
tasks.
Zhang et al. [ 113] focuses on the performance of LLMs when executing API operations. The
experiments in the article are conducted on three Public library benchmarks and two Private
library benchmarks. The results show that CodeGen-2B performs worse than GPT-3.5 in all five
metrics. The article also fine-tunes the ToolCoder model based on CodeGen-2B to better handle
API operations during code generation. Although ToolCoder achieves results close to GPT-3.5 in
some metrics, overall performance still falls short of GPT-3.5.
Siddiq et al. [ 84] validate the performance of LLMs in generating unit test cases. The article
conducts experiments with different prompts and compares the results based on compilation rate,
test correctness, coverage, and test smells. The experimental results show that CodeGen performs
the worst, while Codex performs the best among the LLMs. This could be attributed to CodeGen
being a smaller model with only 350 million parameters, much smaller in terms of training data
and parameter size compared to Codex and ChatGPT-3.5, which have 12 billion and 175 billion
parameters respectively. ChatGPT-3.5’s performance falls between the two, but in most cases, it is
noticeably inferior to Codex.
Zhuo et al. [ 124] introduce a novel testing framework for code generation tasks and evaluate
multiple models on two different aspects (human preferences and execution success) and four
programming languages. The results show that the method based on GPT-3.5-turbo performs better.
However, there are two caveats to consider regarding the results of this article: (1) The article
primarily focuses on example-level Kendall-Tau and Spearman (rs) correlation tests with HumanEval
based on executable functional correctness, which may not directly indicate the performance of
different language models. (2) The other language models compared to GPT-3.5-turbo in the article,
such as CodeBERTScore, are far behind in terms of parameter size and cannot even be consideredA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 19
as LLMs. Therefore, while the work in this article is interesting, we cannot draw useful conclusions
from it.
Zheng et al. [ 116] introduce CodeGeeX-13B and extensively evaluate its performance on code
generation tasks in different programming languages using HumanEval-X. The experimental results
of the article show that CodeGeeX-13B and CodeGen-Multi-16B outperform general LLMs, GPT-
J-6B, and GPT-NeoX-20B in almost all metrics. The article also provides a comparison between
PaLM-540B and PaLMCoder-540B, showing that PaLMCoder-540B performs better than PaLM-540B
to some extent on the MBPP dataset.
Rozière et al. [ 78] introduce Code Llama and compare it in detail with SOTA LLMs on the
benchmarks of HumanEval, MBPP, and APPS. The article’s results demonstrate that Code Llama
achieves the best results in almost every evaluation metric. By observing the experimental settings
and results, we can draw the following conclusions: (1) The evaluation of GPT-3.5 and GPT-4 in the
article is not comprehensive, but based on the evaluations involving GPT-3.5 and GPT-4, they still
remain highly competitive and outperform Code LLMs like StarCoder. (2) When comparing Llama
2 with Code Llama and Code Llama with Code Llama Python, Code Llama Python performs better.
This also indicates that specialized instruction fine-tuning improves code generation capabilities
for the same model. (3) Model size is crucial, as larger parameter models like Code Llama exhibit
stronger performance.
Fu et al. [ 27] introduce CodeApex, a bilingual benchmark dataset focused on testing LLMs’
programming comprehension and code generation abilities. CodeApex consists of three types of
multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop
reasoning. The results of the article show that GPT-3.5-turbo performs better in most of the metrics.
Zan et al. [ 110] categorize LLMs based on model size and present their performance accordingly.
The data results indicate that Code LLMs have better performance when the parameter sizes are
similar. At the same time, the impact of parameter size on the model’s capabilities may vary among
different LLMs. It is observed that the three versions of CodeGen-Mono with different parameter
sizes (2.7B, 6.1B, and 16.1B) show minimal differences on the MBPP and HumanEval benchmarks.
On the other hand, Codex exhibits relatively significant differences between its two versions: 2.5B
and 12B. The marginal effect of model parameter size on performance and the sensitivity of different
models to parameter size are important factors worth discussing.
Lu et al. [ 56] introduce CodeGPT, which has the same model architecture and training objective
as GPT-2. CodeGPT performs better than GPT-2 in tasks such as code completion and code search.
Similarly, Clement et al. [ 20] compare the proposed PyMT5 with GPT-2 and reach the same
conclusion. CodeT5 also outperforms GPT-2 with better performance [100].
Xu et al. [ 103] claim that PolyCoder achieves lower complexity than all models, including Codex.
However, based on the HumanEval benchmark results provided in the article, PolyCoder falls behind
Codex by a considerable margin. It also performs worse than GPT-Neo with similar parameter
sizes.
Li et al. [ 53] introduce AlphaCode, which already surpasses GPT-Neo (APPS test) on all difficulty
levels with a small 1B parameter model, and it outperforms Codex-12B on interview and contest
difficulty levels. The article further analyzes the test results and finds that all encoder-decoder
models, including the final AlphaCode model, perform significantly worse in HumanEval com-
pared to decoder-only models. The article attributes this performance difference to the fact that
the encoder-decoder models align well with competitive programming settings. In competitive
programming settings, there is typically a dataset with clear inputs (programming contest problem
descriptions) and outputs (solution code), along with example tests for effective filtering. Therefore,
encoder-decoder models can learn effectively and sample efficiently. However, encoder-decoder
models are not aligned with the HumanEval setting, where the only training data is GitHub code,20 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
which cannot be easily split into meaningful inputs and outputs. Thus, the fact that decoder-only
models compute loss for all tokens (instead of only one-third of the tokens) enables them to achieve
stronger results in the HumanEval setting. Therefore, more diverse benchmark tests may be needed
to quantify the true capabilities of Code LLMs, as a single benchmark may not fully reflect their
performance.
Fried et al. [ 26] evaluated the performance of INCODER-6.7B, as proposed in the article, on
the HumanEval and MBPP benchmarks. We can see that INCODER outperforms GPT-J-6B, which
has a similar number of parameters, by a significant margin, and even performs better than the
larger model GPT-NeoX-20B. However, there is still a notable gap between INCODER-6.7B and
CodeGen-Mono, Codex, and other Code LLMs.
Wang et al. [ 99] extensively evaluated CodeT5Mix on seven code-related tasks across 20 datasets
and demonstrated that it achieves state-of-the-art performance on most tasks. However, CodeT5Mix
was not directly compared to general-purpose LLMs in these metrics. In some metrics, the smaller-
scale CodeT5Mix outperformed larger models like LaMDA and GPT-2. Unfortunately, the article did
not provide a comparison between CodeT5Mix and GPT-3.5/GPT-4. On the other hand, WizardCoder
proposed by Luo et al. [ 57] was compared to GPT-3.5 and GPT-4 on HumanEval (pass@1(%)) and
MBPP. The experimental results showed that WizardCoder outperformed GPT-3.5 but was inferior
to GPT-4. However, it still performed significantly better than larger models like PaLM-540B and
PaLM-Coder-540B.
Li et al. [ 52] also demonstrated impressive performance in leapfrogging, performing well on
Common Sense Reasoning Benchmarks and surpassing general LLMs like Llama2-7B. However, it
was outperformed by general LLMs on Language Understanding and Knowledge Benchmarks. The
article presents results that make it difficult to assess the superiority of Code LLMs over general
LLMs. It challenges the popular notion that the ability of LLMs is solely determined by their scale,
indicating that data quality plays a more important role than previously believed. The article further
demonstrates the feasibility of achieving high-level functionality in smaller LLMs. Perhaps the
results presented in the article are far more important than assessing the performance of Code
LLMs compared to general LLMs.
Some recent Code LLMs have shown better performance. OpenCoderPlus [ 96] claims to achieve
102.5% of ChatGPT score on Vicuna GPT-4 evaluation and a 78.7 win rate on AlpacaEval. CodeFuse-
CodeLlama-34B [ 62] also outperformed GPT-4 in HumanEval (Pass@1). Additionally, MFTCoder [ 62]
published the performance of WizardCoder-Python-34B-V1.0 in HumanEval (Pass@1), which
is slightly better than GPT-4. CodeGeeX2-6B [ 93] also surpassed LLaMA2-70B in HumanEval
(Pass@1).
Some evaluation work has focused on analyzing the performance of Code LLMs. Among them,
some work has compared the performance differences between Code LLMs and general LLMs. Tang
et al. [ 90] evaluate results on BIOCODER showed that ChatGPT has a better understanding of the
application of imported toolkits or functions contained in other files. Xia et al. [101] explored the
vulnerability repair ability of LLMs and found that as the model size increases, there is a scaling
effect of performance improvement. Codex exhibited a more prominent performance. Palacio et
al. [68] evaluated LLMs’ features and their corresponding AsC-Eval performance, and in all the
test results, CodeGen demonstrated better performance compared to GPT-Neo and CodeParrot.
Pan et al. [ 69] evaluated the performance of LLMs on code translation tasks and found that, except
for GPT-4 and StarCoder, all other LLMs performed poorly. Among the two, GPT-4 performed
better. Kou et al. [ 43] assessed the attention differences between LLMs and programmers in code
generation. The article indicated that compared to models like GPT-J-6B and INCODER, CodeGen’s
attention is closer to that of human programmers. However, the article also found that LLMs payA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 21
more attention to different types of keywords, while programmers are more sensitive to adjectives
and numbers in task descriptions.
Du et al. [ 25] provided Pass@k with Nucleus Sampling on ClassEval. ClassEval is a class-level
code generation benchmark test. The experimental results showed that GPT-4 and GPT-3.5 achieved
better performance compared to Code LLMs like WizardCoder, Instruct-StarCoder, and CodeGen.
We organize the results from the aforementioned paper into Table 3.
From the Table 3, we can see that out of the 38 papers, 11 experiments from various works
showed that general LLMs perform better on software engineering tasks, while in 20 experiments,
Code LLMs demonstrated superior performance. There are 7 papers where we couldn’t obtain a
clear conclusion regarding whether Code LLMs or general LLMs perform better based on their
experimental sections or conclusions.
Based on the above findings, it may be challenging to arrive at a definitive conclusion regarding
whether Code LLMs or general LLMs are better. Firstly, the current state-of-the-art general LLMs,
such as GPT-4 and GPT-3.5-turbo, are not open-sourced, which limits the ability to compare them
in many metrics. Additionally, there are restrictions when accessing the APIs of these models.
Secondly, due to the varying publication dates of each work, the choice of Code LLMs and general
LLMs differs among them [ 87], leading to variations in the evaluation results. For example, earlier
works often used GPT-2 and GPT-J as baselines for general LLMs [ 43], while more recent works
typically use Llama and GPT-3.5 as baselines [ 83]. Thirdly, the selection of benchmarks and software
engineering tasks differs among these studies [ 25,27]. Lastly, different works may select different
versions of the same model with varying parameter sizes, leading to significant performance
differences for that model across different studies [105].
Based on the results from the aforementioned literature, we can still draw the following conclu-
sions:
•Model parameters have a significant impact on performance, with larger-scale models often
exhibiting better performance within the same model architecture [78, 101, 110].
•For the same model, fine-tuning the model specifically for software engineering tasks gener-
ally leads to improved performance compared to the base model [50, 113].
•When parameter sizes are comparable, Code LLMs tend to outperform general LLMs [ 26,99].
•Currently, the state-of-the-art Code LLMs (such as CodeFuse-CodeLlama-34B and Open-
CoderPlus) outperform the current state-of-the-art general LLMs (GPT-4) in code generation
tasks [75, 96].
Furthermore, we can derive an interesting finding from the literature:
•The impact of parameter size on model performance may vary among different LLMs. In Zan
et al. [ 110], it is observed that different versions of CodeGen-Mono with parameter sizes of
2.7B, 6.1B, and 16.1B exhibit relatively small differences in performance on the MBPP and
HumanEval benchmarks. On the other hand, the variations between the two versions of
Codex, 2.5B and 12B, are relatively larger. Therefore, for Code LLMs, the marginal effect of
model parameter size on performance and the sensitivity of different models to parameter
size could be a topic worth discussing.
•All encoder-decoder models perform significantly worse in HumanEval compared to decoder-
only models. Li et al. [ 53] suggest that this discrepancy arises from the fact that encoder-
decoder models are not well-aligned with the HumanEval setup, whereas they align well
with competitive programming settings. In competitive programming, there is typically a
well-defined input (programming competition problem description) and output (solution
code). Encoder-decoder models may struggle to split the input and output into meaningful
segments for HumanEval, thus calculating the loss for all tokens in the decoder-only models22 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 3. The performance of Code LLMs and general LLMs in various literatures.
Work General LLMs Code LLMs Unknown Task
Jiang et al. [38] ✓ Code Generation
Maddigan et al. [60] ✓ Data visualization
Shirafuji et al. [82] ✓ Robustness of solving programming problems
Li et al. [48] ✓ Code Generation
Li et al. [47] ✓ Text-to-SQL
Yang et al. [105] ✓ Code Generation
Thakur et al. [91] ✓ Verilog Code Generation
Siddiq et al. [83] ✓ Code Generation
Sun et al. [86] ✓ Text-to-SQL
Sun et al. [87] ✓ Code Summarization
Wang et al. [98] ✓ Code Generation
Li et al. [50] ✓ Code Generation
Li et al. [49] ✓ Information extraction
Zhang et al. [113] ✓ API-related Tasks
Siddiq et al. [84] ✓ Unit test Generation
Zhuo et al. [124] ✓ Code Generation
Zheng et al. [116] ✓ Code Generation
Rozière et al. [78] ✓ Code Generation
Fu et al. [27] ✓ Code Generation& Understanding
Zan et al. [110] ✓ Code Generation
Lu et al. [56] ✓ Code Completion & Code Search
Clement et al. [20] ✓ Code Generation
Yue et al. [100] ✓ Code Generation
Xu et al. [103] ✓ Code Generation
Li et al. [53] ✓ Code Generation
Fried et al. [26] ✓ Code Generation
Wang et al. [99] ✓ Seven code-related tasks
Luo et al. [57] ✓ Code Generation
Li et al. [52] ✓ Code Generation
Guan et al. [96] ✓ Code Generation
MFTCoder [62] ✓ Code Generation
THUDM [93] ✓ Code Generation
Tang et al. [90] ✓ Understanding functions
Xia et al. [101] ✓ Vulnerability Remediation
Palacio et al. [68] ✓ Code Syntax Understanding
Pan et al. [69] ✓ Code Translation
Kou et al. [43] ✓ Attention to Programming Problems
Du et al. [25] ✓ Code Generation
Total Number 11 20 7
enables them to achieve stronger results in the HumanEval setting. Therefore, a more diverse
set of evaluation benchmarks may be needed to accurately quantify the true capabilities of
Code LLMs, as a single benchmark may not fully capture their performance.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 23
•The abilities of LLMs may not be solely determined by their model size [ 52], and data quality
can play a crucial role. This is exemplified by phi-1.5-1.3B, which has been able to outperform
LLMs with parameter sizes five times larger.
5WHICH SOFTWARE ENGINEERING TASKS ARE DIFFERENT LLMS PARTICULARLY
GOOD AT?
In this section, we primarily focus on addressing and summarizing RQ3 . First, we will gather the
relevant papers and extract the sections that evaluate LLMs in software engineering tasks. Next, we
will classify these works according to different software engineering tasks and further categorize
them based on the evaluation benchmarks used. Our goal is to compile a relatively comprehensive
list for each benchmark.
5.1 Code Generation
Code generation is currently one of the most prominent software engineering tasks of interest
for Code LLMs. Consequently, there is a rich variety of benchmarks available to evaluate LLMs in
code generation tasks. Some commonly used benchmarks include HumanEval [ 16], DS-1000 [ 44],
and MBPP [ 121]. Additionally, there have been efforts to propose new benchmarks from different
perspectives to assess the code generation capabilities of LLMs. For instance, Muennighoff et al.[ 64]
introduced HUMANEVALPACK to evaluate the multilingual code generation ability of LLMs, and
Du et al.[25] proposed ClassEval, a class-level code generation benchmark.
However, to the best of our knowledge, there is currently no organization or individual actively
maintaining these benchmarks. Therefore, it is challenging to find a comprehensive list that
showcases the performance of LLMs on a specific benchmark. In this section, we will organize
LLMs according to different benchmarks to facilitate a better comparison of their performance.
HumanEval: HumanEval consists of 164 handwritten Python programming problems. Each
problem provides a prompt that includes a description of the function to be generated, the function
signature, and example test cases in the form of assertions. The models are required to complete
the functionality based on the prompt, aiming to pass all the provided test cases, thereby measuring
their performance in terms of functional correctness. HumanEval is currently the most commonly
used benchmark to test the code generation capabilities of LLMs. We have organized the results
from the collected papers on HumanEval and obtained Table 4. In the case of *, this result is sourced
from Shen et al. [81].
We have marked the top five scores from each evaluation data using underlines and bold
formatting. From the table, we can see that LATS( GPT-4 based ) [ 121] performs the best in Pass@1.
In Pass@10, the best performer is Unnatural-Code-LLaMA-34B [ 78], and it remains the best in
Pass@100 as well. Considering the results from all three criteria, Unnatural-Code-LLaMA-34B is
currently the top-performing LLM (Language Model) according to HumanEval. To a certain extent,
GPT-4 and Unnatural-Code-LLaMA-34B is also the best-performing LLM for code generation tasks
at the moment. The next best performers are Code-LLaMA-Python-34B and Code-LLaMA-Python-
13B.
It is important to note that there may be variations in the data reported for the same model
across different papers. We have selected data from more recent publications. Additionally, for
cases where there are differences in the reported data, we have included them in Table 5 as well.
There are also some benchmarks such as APPS [ 32] and CodeXGLUE [ 56] where the available
data is limited, making it difficult to showcase performance differences among different LLMs.
Therefore, we won’t be presenting them here. Additionally, there are more complex benchmarks
that require different settings for different tasks, such as HumanEvalX. As a result, each paper24 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 4. Performance of LLMs in HumanEval Benchmark.
LLMsHumanEvalLLMsHumanEval
Pass@1 Pass@10 Pass@100 Pass@1 Pass@10 Pass@100
LATS(GPT-4 based) 94.4 - - LLaMA2-13B 20.1 34.8 61.2
Reflexion(GPT-4 based) 91 - - CodeGen-multi-16.1B 19.22 34.64 55.17
LATS(GPT-3.5 based) 86.9 - - CodeGen-16B-multi 19.2 34.6 55.2
Parsel 85.1 - - CodeGen2-7B 18.83 31.78 50.41
GPT-4(*) 82 - - CodeGen-multi-16B 18.3 - -
MetaGPT 81.7 - - CodeGen-multi-6.1B 18.16 27.81 44.85
CodeFuse-CodeLlama-34B 74.4 - - SantaCoder-1.1B 18 29 49
Phind-CodeLlama-34B-v2 73.8 - - AlphaCode-1.1B 17.1 28.2 45.3
WizardCoder-Python-34B 73.2 - - PanGu-Coder-317M 17.07 24.05 34.55
Phind-CodeLlama-Python-34B-v1 69.5 - - Codex-679M 16.22 25.70 40.95
GPT-3.5(*) 68.9 - - PaLM-62B 15.9 - 46.3
Phind-CodeLlama-34B-v1 67.6 - - LLaMA-13B 15.8 - 52.5
GPT-4(OpenAI) 67 - - BLOOM-176B 15.52 32.20 55.45
Unnatural-Code-LLaMA-34B 62.2 85.2 95.4 CodeT5+-770M 15.5 27.2 42.7
PanGu-Coder2-15B 61.64 79.55 91.75 GPT-NeoX-20B 15.4 25.6 41.2
WizardCoder-15B 57.3 73.2 90.46 InCoder-6.7B 15.2 27.8 47.0
Code-LLaMA-Python-34B 53.7 82.8 94.7 InCoder-multi-6.7B 15.2 27.8 47.0
Phi-1-1.3B 50.6 - - InCoder-6B 15.2 27.8 47.0
Code-LLaMA-34B 48.8 76.8 93.0 CodeGen-multi-2.7B 14.51 24.67 38.56
GPT-3.5(OpenAI) 48.1 - - Codegen-NL-16.1B 14.24 23.46 38.33
code-davinci-002 47.0 74.9 92.1 AlphaCode(dec)-685M 14.2 24.4 38.8
OctoCoder 46.2 - - LaMDA-137B 14.0 - 47.3
Code-LLaMA-Python-13B 43.3 77.4 94.1 Codex-300M 13.17 20.37 36.27
Code-LLaMA-Instruct-13B 42.7 71.6 91.6 CodeGen-Mono-350M 12.76 23.11 35.19
Code-LLaMA-Instruct-34B 41.5 77.2 93.5 CodeGen-mono-350M 12.76 23.11 35.19
StarCoder-Prompted-15B 40.8 - - LLaMA2-7B 12.2 25.2 44.4
code-davinci-001 39 60.6 84.1 CodeT5-770M 12.09 19.24 30.93
Code-LLaMA-Python-7B 38.4 70.3 90.6 CodeT5+-220M 12.0 20.7 31.6
PaLM-2-S 37.6 - 88.4 GPT-J-6B 11.62 15.74 27.74
PaLM-Coder-540B 36 - 88.4 AlphaCode-302M 11.6 18.8 31.8
Code-LLaMA-13B 36.0 69.4 89.8 LLaMA-7B 10.5 - 36.5
CodeGeeX2-6B 35.9 62.6 88.3 CODEGEN-NL-6.1B 10.43 18.36 29.85.
InstructCodeT5+-16B 35.0 54.5 77.9 InCoder-1.3B 8.9 16.7 25.6
Code-LLaMA-Instruct-7B 34.8 64.3 88.1 PyCodeGPT-110M 8.33 13.36 19.13
CodeGen-16.1B 34.6 - - Codex-85M 8.22 12.81 22.40
StarCoder-Python-15B 33.6 - - BLOOM-7.1B 7.73 17.38 29.47
StarCoder-15B 33.60 45.78 79.82 CODEGEN-NL-2.7B 6.7 14.15 22.84
code-cushman-001 33.5 54.3 77.4 CODEGEN-MULTI-350M 6.67 10.61 16.84
Code-LLaMA-7B 33.5 59.6 85.9 BLOOM-3B 6.48 11.35 20.43
CodeGen2.5-7B-mono 33.4 58.4 82.7 GPT-NEO-2.7B 6.41 11.27 21.37
CodeT5+-16B-mono 30.9 51.6 76.7 PolyCoder-2.7B 5.59 9.84 17.68
MIM-2.7B 30.7 48.22 69.6 JuPyT5-300M 5.4 15.46 25.60
Replit-Finetuned-2.7B 30.5 - - Codex-42M 5.06 8.8 15.55
LLaMA2-70B 30.5 59.4 87.0 GPT-Neo-1.3B(1.5B) 4.79 7.47 16.30
StarCoderBase-15B 30.4 - - GPT-Neo-1.3B 4.79 7.47 16.3
CodeGen-mono-16B(16.1B) 29.28 49.86 75.00 AlphaCode(dec)-89M 4.3 12.2 20.0
Codex-12B 28.81 46.81 72.31 AlphaCode(dec)-55M 4.2 8.2 16.9
CodeGen2.5-7B 28.36 47.46 75.15 BLOOM-1.7B 4.03 7.45 12.75
CodeT5+-6B 28.0 47.2 69.8 CodeParrot-multi-1.5B 4.0 8.7 17.9
PaLM-540B 26.2 - 76.2 CodeParrot-1.5B 3.99 8.69 17.88
CodeGen-Mono-6.1B 26.13 42.29 65.82 CodeParrot-110M 3.8 6.57 12.78
CodeGen-mono-6B 26.1 42.3 65.8 CodeParrot-1.5B 3.8 6.57 12.78
CodeT5+-2B 24.2 38.2 57.8 PaLM-8B 3.6 - 18.7
PanGu-Coder-2.6B 23.78 35.36 51.24 CodeParrot-small-110M 3.58 8.03 14.96
CodeGen-Mono-2.7B 23.7 36.64 57.01 AlphaCode(dec)-29M 3.4 5.8 11.2
LLaMA-65B 23.7 - 79.3 Codex-25M 3.21 7.1 12.89
CodeGen-mono-2B 23.7 36.6 57 PolyCoder-400M 2.96 5.29 11.59
CodeGeeX-13B 22.89 39.57 60.92 BLOOM-1.1B 2.48 5.93 9.62
LLaMA2-34B 22.6 47.0 79.5 PolyCoder-160M 2.13 3.35 4.88
MIM-1.3B 22.4 41.7 53.8 CODEGEN-NL-350M 2.12 4.10 7.38
Replit-3B 21.9 - - Codex-12M 2 3.62 8.58
Replit-2.7B 21.9 - - AlphaCode-13M 1.5 3.6 8.6
LLaMA-33B 21.7 - 70.7 GPT-NEO-350M 0.85 2.55 5.95
Codex-2.5B 21.36 35.42 59.50 BLOOM-560M 0.82 3.02 5.91
CodeGen2-16B 20.46 36.5 56.71 GPT-Neo-125M 0.75 1.88 2.97A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 25
Table 5. Performance inconsistency reported in different works on HumanEval.
LLMsHumanEval
Pass@1 Pass@10 Pass@100
InCoder-1.3B [98] 8.9 16.7 25.6
InCoder-1.3B [110] 11.09 16.14 24.20
CodeGen-multi-16.1B [116] 19.22 34.64 55.17
CodeGen-multi-16.1B [67] 18.32 32.07 50.80
Table 6. Percentage of compilable suggestions (code snippets) in [83].
Code Par-
rot smallCodeParrot
regularInCoder
1BCodeGen
350M
monoCodeGen
350M
multiCodeGen
2B monoCodeGen
2B multiPolyCoder
160MPolyCoder
0.4BPolyCoder
2.7BGPT 3.5
Java - 0.15% - 5.86% - 7.43% 0 .13% 0 .29% 0 .19% 0 .89%
Python 22.22% 25 .98% 34 .56% 37 .06% 34 .29% 40 .49% 19 .73% 21 .33% 22 .72% 57 .75%
Table 7. NDCG@10 scores for the original model ranking in [83].
Code Par-
rot smallCodeParrot
regularInCoder
1BCodeGen
350M
monoCodeGen
350M
multiCodeGen
2B monoCodeGen
2B multiPolyCoder
160MPolyCoder
0.4BPolyCoder
2.7BGPT 3.5
Java - - 0.0979 0.2330 - - 0.3019 0.1385 0.1525 0.2264 0.5775
Python 0.3297 0.4021 0.2012 0.3944 0.3640 0.4745 0.4738 0.3740 0.4200 0.4541 0.3742
provides different experimental setups and results. It is also challenging to observe the performance
of these LLMs in the context of HumanEvalX.
There have been various studies that have explored the performance of LLMs in code generation
from different perspectives. Zan et al. [ 108], focused on evaluating LLMs’ performance in generating
code tailored to private libraries. The study proposed four benchmark metrics for private libraries,
including TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNumEval. Based on the
data provided in the paper, we can observe that the proposed CodeGenAPI-6B performs the best in
this task. CodeGenAPI-6B is developed based on CodeGen. If we exclude CodeGenAPI, then the
best performer is code-davinci-002 (Codex), followed by CodeGen-6B.
Siddiq et al. [ 83] introduced a lightweight framework for recommending more secure source
code derived from LLMs. The article also provides performance results for several LLMs, as shown
in Tables 6and Tables 7. We can see that GPT-3.5 performs better than other LLMs. However, it’s
important to note that the evaluated LLMs in the article may not be state-of-the-art LLMs, so the
obtained results have limited value as a comparison to current top-performing models. However,
from the tables, we can also observe that the performance of the same LLM can vary significantly
across different programming languages.
Zhang et al. [ 113] also evaluated the performance of several LLMs on PublicLibrary Benchmark
and Private Library Benchmark, as shown in Tables 8 and Tables 9. Although the primary focus of
Zhang et al [ 113] was to introduce the proposed ToolCoder, we can also observe the differences
in LLMs’ ability to call APIs during code generation from Tables A and B. Based on the results
provided in the article, ToolCoder is undoubtedly the best performer, followed by GPT-3.5. However,
the performance of CodeGenAPI, which is CodeGen fine-tuned for this task, did not surpass that of
CodeGen.
Zhong et al. [ 119] also evaluated the issue of API misuse in code generation by LLMs. Table 10
in the article presents the test results for GPT-3.5/GPT-4/Llama/Vicunad on the ROBUSTAPI26 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 8. Pass rates of models on public library benchmarks in [113].
ModelNumpyEval PandasEval TorchDataEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
CodeT5-220M 0 0.1 0 0 0 0
PyCodeGPT-11M 18.04 38.61 12.75 37.62 3.80 14.00
CodeGen-350M 18.51 43.56 16.73 29.70 4.60 14.00
CodeGen2B-2B 29.10 53.46 30.69 42.57 7.00 18.00
GPT-3.5 58.41 66.21 30.09 33.16 6.00 24.00
CERT-numpy-220M 31.47 46.42 16.03 27.72 2.20 14.00
CERT-pandas-220M 18.81 33.66 28.42 48.04 2.80 6.00
CodeGenAPI-350M 16.55 29.48 13.58 34.95 7.19 16.93
CodeGenAPI-retrieval-475M 12.67 27.32 11.25 28.61 10.41 23.50
CodeGen-retrieval-475M 18.30 35.12 9.54 29.02 7.52 16.36
ToolCoder-OnlineTool-350M 35.64 50.50 22.77 37.62 7.40 20.00
ToolCoder-OnlineTool-2B 41.58 55.44 31.68 47.52 11.80 24.00
Table 9. Pass rates of models on private library benchmarks in [113].
ModelMonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10
CodeT5-220M 0 0 0 0
CodeGen-350M 0.95 4.90 5.15 11.96
CodeGen-2B 1.59 5.94 5.94 11.88
GPT-3.5 2.47 8.91 6.68 17.82
CodeGenAPI-350M 1.19 4.68 4.44 8.24
CodeGenAPI-retrieval-475M 3.41 8.33 5.90 11.79
CodeGen-retrieval-475M 2.46 6.35 6.65 13.68
ToolCoder-DocTool-350M 2.98 5.94 6.73 12.87
ToolCoder-DocTool-2B 3.02 7.92 6.93 13.86
Table 10. Performance of LLMs on ROBUSTAPI in [119].
LLMs Zero-shot One-shot-irrelevant One-shot-relevant
Misuse
RateExec.
SampleOverall
MisuseMisuse
RateExec.
SampleOverall
MisuseMisuse
RateExec.
SampleOverall
Misuse
GPT-3.5 62.97% 79 .14% 49 .83% 68 .09% 91 .06% 62 .00% 38 .56% 80 .71% 31 .13%
GPT-4 68.81% 90 .23% 62 .09% 70 .38% 91 .39% 64 .32% 54 .40% 90 .40% 49 .17%
Llama-2 7.34% 9 .02% 0 .66% 61 .36% 80 .13% 49 .17% 64 .47% 72 .93% 47 .02%
Vicuna-
1.545.66% 37 .17% 16 .97% 57 .85% 83 .86% 48 .51% 42 .53% 64 .24% 27 .32%
benchmark. The misuse rate refers to the proportion of misuse cases among executable cases, exec.
The sample represents the proportion of executable cases among all questions, and the overall
misuse percentage is the proportion of misuse cases among all questions. However, from Table 10,
it is difficult to determine which LLM performs better. We can only conclude that LLMs commonly
exhibit API misuse issues, even when generating code that is executable and aligns with the user’s
intent.
Tu et al. [ 24] introduce and define the problem of error code completion, where given a problem
statement and partially coded program with potential errors, the task is to complete the codingA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 27
Table 11. Pass@1 of completion methods on buggy-HumanEval and buggy-FixEval datasets in [24].
Prefix Methodbuggy-HumanEval buggy-FixEval
CodeGen InCoder CodeGen InCoder
350M 2 B 1 B 6 B 350M 2 B 1 B 6 B
Clean completion 43.0 54.9 41.1 50.6 27.6 37.8 24.1 32.3
Buggy completion 0.7 3.1 0.5 1.0 2.4 4.3 1.2 1.8
Table 12. Pass@k with Nucleus Sampling on ClassEval in [25].
ModelClass-level Method-level
Pass@1 Pass@3 Pass@5 Pass@1 Pass@3 Pass@5
GPT-4 37.6% 41.3% 42.0% 62.8% 67.4% 68.5%
GPT-3.5 29.6% 34 .9% 36 .0% 50 .4% 59 .0% 61 .1%
WizardCoder 12.2% 20 .0% 23 .0% 35 .2% 47 .1% 51 .1%
Instruct-StarCoder 10.2% 12 .7% 14 .0% 23 .1% 26 .5% 27 .7%
SantaCoder 8.6% 9 .9% 10 .0% 27 .7% 33 .0% 34 .9%
Instruct-CodeGen 8.2% 12 .3% 13 .0% 24 .9% 34 .3% 37 .1%
CodeGeeX 7.2% 9 .4% 10 .0% 21 .2% 27 .1% 29 .5%
InCoder 6.2% 7 .6% 8 .0% 21 .1% 26 .5% 29 .1%
Vicuna 3.0% 3 .6% 4 .0% 11 .0% 15 .8% 18 .4%
ChatGLM 1.4% 2 .6% 3 .0% 8 .2% 11 .2% 12 .4%
PolyCoder 1.4% 2 .2% 3 .0% 13 .2% 17 .5% 19 .6%
program. The article selects InCoder and CodeGen as the compared Code LLMs in the experiments.
According to the experimental data provided in Table 11, CodeGen-2B consistently achieves better
results across various metrics. However, it’s important to note that the article only compares
InCoder and CodeGen, which limits the scope of the results presented in the article.
Fu et al. [ 27] introduce CodeApex, a benchmark focused on evaluating LLMs’ programming
understanding and code generation capabilities. In terms of programming understanding, GPT-3.5-
turbo consistently ranks first in almost all tasks, followed by InternLM-Chat-7B. The experimental
conclusions for code generation are similar, with GPT-3.5-turbo performing the best. Additionally,
WizardCoder-15B also achieves very good results. It’s worth noting that the article’s evaluation of
LLMs is limited, and there is relatively little work that has tested LLMs using CodeApex.
Du et al. [ 25] attempted to evaluate LLMs’ class-level code generation capabilities and introduced a
class-level code generation benchmark called ClassEval. The article also presents the performance of
11 state-of-the-art LLMs on ClassEval, as shown in Table 12. We can observe that GPT-4 achieves the
best performance in almost all metrics, followed by GPT-3.5. Among the Code LLMs, WizardCoder
achieves the best scores.
Yu et al. [ 106] introduce a benchmark called CoderEval to evaluate the performance of models in
generating practical code. Compared to the HumanEval benchmark, CoderEval includes program-
ming tasks from various open-source projects and provides full coverage testing to assess models’
performance in practical code generation. The article does not conduct large-scale evaluation ex-
periments on existing LLMs but compares the performance of CodeGen, Codex, and PanGu-Coder
on CoderEval. The results indicate that Codex performs better in various testing scenarios.
Based on the information provided, we can see that the Code-LLaMA series of LLMs performs
well in several commonly used code generation benchmarks. Among them, Unnatural-Code-LLaMA-
34B stands out with outstanding performance. For API-related code generation tasks, ToolCoder
performs better. Additionally, GPT-4 and GPT-3.5 (GPT-3.5-turbo) also exhibit good performance.28 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 13. Average solved rates (%)for each type of problem formatting in [82].
Formatting CodeGen Codex InstructGPT ChatGPT
Raw HTML 10.9 30.9 75.9 90 .1
Parsed plain 9.2 34.7 73.5 89.0
AlphaCode-inspired 9.7 35.2 72.0 88.7
APPS-inspired 11.7 39.3 73.7 88.6
Fully Markdown-formatted 9.9 39.9 74.5 89.0
Average 10.3 36.0 73.9 89.1
Variance 1.01 13.61 2.04 0.36
Table 14. Performance comparison on Test Suite accuracy on Spider Dev Split in [82].
LLMs Execution Accuracy Test-suite Accuracy
GPT-3 ada (0-shot) 2.3 0.3
GPT-3 babbage (0-shot) 5.7 3.9
GPT-3 curie (0-shot) 12.6 8.3
GPT-3 davinci (0-shot) 26.3 21.7
CodeX cushman (0-shot) 63.7 53.0
CodeX davinci (0-shot) 67.0 55.1
CodeX davinci (few-shot) 71.0 61.5
ChatGPT (w/ OpenAI-default Prompt) 70.1 60.1
GPT-4 (Zero-shot) 72.9 64.9
GPT-4 (Few-shot) 76.8 67.4
5.2 Test Case Generation
In Schäfer et al. [ 80], GPT-3.5-turbo, StarCoder, and code-cushman-002 were tested for the number
of generated tests and the percentage of passed generated tests. The experimental results in the
article showed that the code-Cushman-002 model had a test coverage rate comparable to that
of GPT-3.5-turbo, with the latter having slightly higher median statement and branch coverage.
StarCoder exhibited relatively poorer performance in comparison.
Shirafuji et al. [ 82] aims to demonstrate the high capability of LLMs in solving a wide range of
programming problems, specifically their robustness in solving programming problems. Therefore,
the article assigns each LLM to generate 100 programs for each of the 40 questions and tests their
performance, as shown in Table 13. On average, we can observe that Codex performs three times
better than CodeGen. Codex’s successor, InstructGPT, improves the average success rate by twofold,
while ChatGPT exhibits even greater improvement. The article suggests that these significant
performance differences also reflect the impact of fine-tuning on programming problems, as Codex
is fine-tuned while CodeGen is not.
According to Sun et al. [ 86], a performance comparison of GPT-4, GPT-3.5, and Codex was
conducted on the Spider Dev Split test suite to evaluate their accuracy. The results are presented in
Table 14. It can be observed that on the Spider Dev Split, GPT-4 outperforms Codex, while Codex’s
performance is better than that of GPT-3.5.
In Siddiq et al. [ 84], ChatGPT, codegen-350M-multi, and Codex (2K and 4K) were evaluated for
their ability to generate unit tests. Although the article provides a detailed test analysis for these
three LLMs, it does not provide definitive results. Even when considering the compilation success
rate, it is not possible to draw deterministic conclusions, as shown in Table 15.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 29
Table 15. Compilation status of the generated unit tests in [9].
Benchmark LLM CompilableCompilable
(after fix)Test
MethodsTest
Files
HumanEval ChatGPT 43.1% 81 .3% 1,117 130
HumanEval CodeGen 23.8% 33 .1% 844 529
HumanEval Codex (2K) 37.5% 100% 697 160
HumanEval Codex (4K) 44.4% 99 .4% 774 159
SF110 ChatGPT 9.7% 85 .9% 194 87
SF110 CodeGen 21.0% 58 .5% 83 139
SF110 Codex (2K) 2.7% 74 .5% 1,406 222
SF110 Codex (4K) 3.4% 83 .5% 1,039 152
Table 16. Performance comparison on Test Suite accuracy on Spider Dev Split in [87].
MethodCSN-Python
BLEU METEOR ROUGE-L
NCS 15.8 10.6 31.3
CodeBERT 18.7 12.4 34.8
CodeT5 20.0 14.7 37.7
ChatGPT (one sentence) 10.28 14.40 20.81
Table 17. Performance (smoothed BLEU-4) on code summarization on CodeSearchNet in [98].
LLMs Ruby JS Go Python Java PHP Overall
RoBERTa-125M 11.17 11.90 17.72 18.14 16.47 24.02 16.57
CodeBERT-125M 12.16 14.90 18.07 19.06 17.65 25.16 17.83
UniXcoder-125M 14.87 15.85 19.07 19.13 20.31 26.54 19.30
CodeGen-multi-350M 13.48 16.54 18.09 18.31 19.41 24.41 18.37
PLBART-140M 14.11 15.56 18.91 19.30 18.45 23.58 18.32
CodeT5-220M 15.24 16.16 19.56 20.01 20.31 26.03 19.55
CodeT5+-220M 15.51 16.27 19.60 20.16 20.53 26.78 19.81
CodeT5+-770M 15.63 17.93 19.64 20.47 20.83 26.39 20.15
Based on the available information, it can be concluded that in the task of test case generation,
GPT-4 and GPT-3.5 (GPT-3.5-turbo) show better performance.
5.3 Code Summarization
Sun et al. [ 87] present the overall performance of ChatGPT (one sentence), NCS, CodeBERT, and
CodeT5 on the CSN-Python dataset. In Table 16, We can see that CodeT5 performs the best among
the four models in the code summarization task, outperforming ChatGPT. However, it is important
to note that the article’s evaluation includes a limited selection of LLMs, which still imposes
limitations on the presented results.
Wang et al. [ 98] also presented the performance of several LLMs on CSN and released CodeT5+.
The details of their performance can be found in Table 17. Furthermore, Wanget al. [ 99] demonstrates
the performance of CodeT5Mix on smoothed BLEU-4. CodeT5Mix is an improved version of CodeT5.
However, based on the results, CodeT5Mix performs almost on par with CodeT5 and does not show
significant performance improvement.30 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 18. Performance of code translation task in HumanEval-X in [116].
ModelTarget Language
Python C++ Java JavaScript Go
@1 @10 @100 @1 @10 @100 @1 @10 @100 @1 @10 @100 @1 @10 @100
PyInCoder-6.7B - - - 26.11 41.00 54.25 26.74 42.66 61.20 37.05 58.85 78.91 15.69 27.57 43.67
CodeGen-Multi-16B - - - 35.94 47.81 59.37 29.27 45.70 64.45 43.40 66.26 82.55 28.87 41.01 57.72
CodeGeeX-13B - - - 26.54 43.56 56.48 25.84 41.52 59.72 23.22 47.33 65.87 9.56 23.83 33.56
CodeGeeX-13B-FT - - - 34.16 46.86 61.22 41.98 58.17 72.78 34.81 53.05 66.08 16.41 30.76 46.37
C++InCoder-6.7B 34.37 58.41 78.57 - - - 34.04 57.02 68.70 37.05 65.05 79.61 25.54 39.11 58.02
CodeGen-Multi-16B 33.83 55.37 76.64 - - - 43.20 69.84 88.82 54.51 71.50 83.14 27.94 49.73 68.32
CodeGeeX-13B 27.18 49.02 67.69 - - - 22.56 40.91 64.08 30.23 55.68 75.58 8.64 18.79 31.76
CodeGeeX-13B-FT 62.79 80.39 87.10 - - - 71.68 81.62 85.84 50.83 64.55 74.57 16.71 34.18 52.98
JavaInCoder-6.7B 42.76 65.55 80.43 40.01 55.17 70.39 - - - 43.20 68.24 84.39 21.58 35.20 54.97
CodeGen-Multi-16B 52.73 69.30 82.74 41.42 54.68 65.50 - - - 57.65 67.90 79.22 34.00 48.49 67.94
CodeGeeX-13B 43.41 68.46 84.03 39.33 58.48 72.36 - - - 44.19 64.22 82.89 17.17 32.74 47.71
CodeGeeX-13B-FT 75.03 87.71 95.13 49.67 65.65 75.40 - - - 49.95 62.82 79.64 18.85 32.92 48.93
JSInCoder-6.7B 23.18 50.47 67.26 35.47 54.48 70.71 30.67 50.90 71.03 - - - 25.79 42.96 61.47
CodeGen-Multi-16B 35.52 52.23 69.78 35.41 53.12 64.47 33.79 56.06 74.00 - - - 33.38 49.08 64.14
CodeGeeX-13B 31.15 54.02 72.36 30.32 51.63 69.37 24.68 48.35 69.03 - - - 11.91 26.39 39.81
CodeGeeX-13B-FT 67.63 81.88 89.30 46.87 60.82 73.18 56.55 70.27 80.71 - - - 16.46 32.99 50.29
GoInCoder-6.7B 34.14 54.52 70.88 30.45 48.47 62.81 34.52 53.95 69.92 39.37 63.63 80.75 - - -
CodeGen-Multi-16B 38.32 50.57 68.65 32.95 45.88 59.56 36.55 59.12 78.70 38.93 56.68 70.68 - - -
CodeGeeX-13B 35.92 56.02 77.32 29.83 41.98 58.15 22.89 41.04 61.46 25.24 46.50 69.93 - - -
CodeGeeX-13B-FT 57.98 79.04 93.57 38.97 53.05 63.92 54.22 69.03 79.40 43.07 59.78 74.04 - - -
Based on the available information, it can be concluded that in the task of code summarization,
CodeT5+ demonstrates better performance compared to GPT-3.5 (GPT-3.5-turbo).
5.4 Code Translation
As shown in Tables 18, the performance of CodeGeeX on code translation tasks and compared with
two other models., it can be observed that CodeGeeX-13B-FT exhibits relatively better performance,
while CodeGen-Multi-16B also performs exceptionally well. However, their strengths lie in different
multilingual scenarios. Nonetheless, the non-fine-tuned CodeGeeX-13B does not perform as well
as CodeGen-Multi-16B in code translation. On the XLCoST benchmark, CodeGeeX outperforms
CodeT5, although the difference in scores between the two is not significant.
Pan et al. [ 69] also provide the performance of seven LLMs, including GPT-4 and StarCoder,
on code translation tasks across seven datasets. From the test results presented in the article (as
shown in Table 19, for detailed experimental settings please refer to Pan et al. [ 69]), it can be
observed that, except for GPT-4 and StarCoder, the other models perform poorly. The article also
points out a strong correlation between the average number of test attempts per translation sample
and unsuccessful translations. Additionally, unsuccessful translations do not exhibit consistent
patterns between the source and target languages, but due to stricter GO syntax constraints, code
translations related to GO perform poorly.
In the task of code translation, GPT-4 performs better. This is supported by Pan et al. [ 69], who
found that GPT-4 outperforms CodeGeeX significantly in terms of performance.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 31
Table 19. Performance of subject LLMs in translating code.
Dataset% Unsuccessful Translations
CodeGen CodeGeeX StarCoder GPT-4 Llama 2 TB-Airoboros TB-Vicuna
CodeNet76.6% 85 .1% 58 .0% 17 .0% 85 .1% 81 .2% 95 .6%
86.0% 96 .4% 60 .9% 20 .0% 90 .5% 91 .7% 96 .6%
85.7% 94 .1% 58 .0% 14 .5% 83 .1% 93 .4% 99 .1%
78.7% 89 .7% 69 .7% 18 .7% 86 .1% 93 .5% 99 .9%
82.5% 92 .7% 66 .7% 20 .1% 89 .0% 93 .5% 99 .0%
Total/Average (CodeNet) 81.9% 91 .6% 62 .7% 18 .0% 86 .8% 90 .7% 98 .0%
AVATAR91.9% 98 .2% 88 .1% 29 .2% 98 .2% 94 .9% 100%
96.2% 98 .4% 85 .8% 47 .8% 95 .3% 99 .1% 99 .1%
Total/Average (AvATAR) 94.1% 98 .3% 87 .0% 38 .5% 96 .8% 97 .0% 99 .6%
EvalPlus 83.5% 96 .3% 78 .0% 20 .7% 98 .8% 86 .0% 92 .1%
Commons CLI 100% 100% 100% 86 .4% 100% 100% 100%
Click 100% 100% 100% 100% 100% 100% 100%
Total/Average (All) 91.9% 97 .2% 85 .5% 52 .7% 96 .5% 94 .7% 97 .9%
5.5 Vulnerability Repair
MBPP: MBPP is also one of the important benchmarks for evaluating the code generation capabili-
ties of LLMs. MBPP, which stands for Massively Bugs and Performance Problems, is a benchmark
that consists of a large number of code snippets with defects and performance issues. The models
are required to generate the correct repair code that is relevant to the given problem. The benchmark
aims to assess the models’ ability to identify and resolve software errors and performance problems.
By using the MBPP benchmark, the practicality and robustness of the models in real-world software
engineering scenarios can be evaluated. We have organized the results from the collected papers
on MBPP and obtained Table 20.
Similarly, while organizing the data for MBPP, we have also noticed variations in the reported
data across different literature sources. We have compiled and included these variations in Table 21
as well. We have followed the principle of selecting the most recent literature to obtain the data.
We have also marked the top five performing data with underlines and bold for each metric (except
for pass@80). We can see that the Code-LLaMA series continues to exhibit strong performance,
carrying forward its excellent performance on HumanEval. Among them, Unnatural-Code-LLaMA
shows the best overall performance, followed by Code-LLaMA-Python-34B. However, it should
be noted that, unlike HumanEval, it is challenging to find scores for many LLMs on MBPP. We
attempted to search using a snowballing approach but did not make significant progress. Based on
the currently available data, Unnatural-Code-LLaMA-34B is the top-performing LLM on MBPP. To
some extent, Unnatural-Code-LLaMA-34B is also the best-performing LLM for code generation
tasks currently available.
Pearce et al. [ 71] explore the ability of LLMs to fix software vulnerabilities in a zero-shot
setting. The experimental section of the article mainly utilizes the following LLMs: code-cushman-
001, code-davinci-001, code-davinci-002, j1-large, j1-jumbo, and polycoder. Overall, based on the
combined evaluation datasets, code-davinci-002 performs relatively well. However, the article
does not explicitly provide performance differences between the LLMs. The study finds that LLMs
can generate fix programs for security vulnerabilities when provided with carefully constructed32 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 20. Performance of LLMs in MBPP benchmark.
LLMsMBPPLLMsMBPP
Pass@1Pass@10Pass@80Pass@100 Pass@1Pass@10Pass@80Pass@100
WizardCoder-16B 51.8 - - - CodeParrot-110M 0.48 3.89 - 15.93
Unnatural-Code-LLaMA-34B 61.2 76.6 - 86.7 CodeParro-1.5B 1.29 8.66 - 27.17
StarCoder-Python-15B 52.7 - - - Code-LLaMA-Python-7B 47.6 70.3 - 84.8
StarCoder-Prompted-15.5B 49.5 - - - Code-LLaMA-Python-34B 56.2 76.4 - 88.2
StarCoderBase-15B 49.0 - - - Code-LLaMA-Python-13B 49 74 - 87.6
StarCoder-5.5B 52.7 - - - Code-LLaMA-Instruct-7B 44.4 65.4 - 76.8
SantaCoder-1.1B 3.65 21.33 - 41.92 Code-LLaMA-Instruct-34B 57 74.6 - 85.4
SantaCoder-1.1B 35 - - - Code-LLaMA-Instruct-13B49.4 71.2 - 84.1
PyCodeGPT-110M 9.39 28.37 - 48.71 Code-LLaMA-7B 41.4 66.7 - 82.5
PolyCoder-400M 1.31 7.98 - 21.55 Code-LLaMA-34B 55 76.2 - 86.6
PolyCoder-2.7B 4.39 17.99 - 38.17 Code-LLaMA-13B 47 71.7 - 87.1
PolyCoder-160M 1.08 6.67 - 18.97 CodeGen-NL 6.1B 8.15 31.21 - 55.27
phi-1-1.3B 55.5 - - - CodeGen-NL 350M 0.96 6.37 - 19.91
PaLM-Coder-540B 47 - 80.8 - CodeGen-NL 2.7B 5.34 24.63 - 48.95
PaLMCoder-540B 47 - 80.8 - CodeGen-NL 16.1B 10.92 38.43 - 62.76
PaLM-540B 36.8 - 75 - CodeGen-Multi-16B 20.9 - - -
PaLM-2-S 50 - - - CodeGen-Multi-6.1B 18.35 47.27 - 67.92
LLaMA-7B 17.7 - - - CodeGen-Multi-350M 7.46 24.18 - 46.37
LLaMA-65B 37.7 - - - CodeGen-Multi-2.7B 18.06 45.80 - 65.34
LLaMA-33B 30.2 - - - CodeGen-Multi-16.1B 20.94 51.61 - 70.02
LLaMA2-7B 20.8 41.8 - 65.5 CodeGen-Mono-6.1B 33.70 62.70 - 70.25
LLaMA2-70B 45.4 66.2 - 83.1 CodeGen-Mono-350M 15.44 42.50 - 64.40
LLaMA2-34B 33.8 56.9 - 77.6 CodeGen-Mono-350M - - - -
LLaMA2-13B 27.6 48.1 - 69.5 CodeGen-Mono-2.7B 28.80 60.73 - 75.41
LLaMA-13B 22 - - - CodeGen-Mono-16B 35.3 - - -
LaMDA-137B 14.8 - 62.4 - CodeGen-Mono-16.1B 35.28 67.32 - 80.09
JuPuT5-300M - - 52.2 - CodeGen-Mono-16.1B 35.3 - - -
InstructCodeT5+-16B - - - - CODEGEN-Mono-6.1B 32.48 64.20 - 76.81
InCoder-6.7B 19.4 - - - CODEGEN-Mono-350M 14.59 41.49 - 63.00
InCoder-6.7B 21.3 46.5 - 66.2 CODEGEN-Mono-2.7B 27.31 59.19 - 74.24
InCoder-1.3B 10.00 34.02 - 55.50 CODEGEN-Mono-16.1B 35.28 67.32 - 80.09
InCoder-6B 21.30 46.50 - 66.20 CodeGen2-1B - - - -
GPT-Neo-2.7B 5.89 23.09 - 44.26 CodeGeeX-13B 24.4 48 68.5 -
GPT-Neo-125M 0.26 2.15 - 7.96 code-davinci-002 58.10 76.70 - 84.50
GPT-Neo-1.3B 3.77 16.26 - 29.51 code-davinci-001 51.80 72.80 - 84.10
GPT-J-6B 11.30 35.62 - 53.63 code-cushman-001 45.90 66.90 - 79.90
GPT-4 - - - - CodcGen2-7B - - - -
GPT-3.5 - - - - BLOOM-7.1B 1.01 7.91 - 24.12
GPT-3.5-turbo 52.2 - - - BLOOM-560M 0.26 2.04 - 8.90
CodeT5-770M 15.78 38.63 50.35 BLOOM-3B 2.25 13.58 - 32.08
CodeT5+-2B - - - - BLOOM-1.7B 3.16 14.23 - 31.38
CodeT5+-16B - - - - BLOOM-1.1B 1.90 9.20 - 23.42
Table 21. Data in the literature that differ for MBPP.
LLMsMBPP
Pass@1 Pass@10 Pass@80 Pass@100
InCoder-6.7B [105] 19.4 - - -
InCoder-6.7B [67] 21.3 46.5 - 66.2
CodeGen-Mono-16.1B [67] 35.28 67.32 - 80.09
CodeGen-Mono-16.1B [50] 35.3 - - -
prompts. However, the evaluation of LLM performance indicates that the current state of the
technology is not sufficient to deliver true value in the context of program repair frameworks.
Xia et al. [ 101] evaluates LLMs used for direct program repair. The article reveals the scaling
effects of increasing model size on various crucial factors in APR, such as the number of fixed
bugs, patch generation speed, and compilation rate. LLMs are also tested on widely used APRA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 33
Table 22. Number of samples generated per minute for different PLMs on Defects4J 1.2 and QuixBugs with
the 3 repair generation settings in [101].
Tools / Models Single func (255 bugs) Patch func Correct hunk Single line
AlphaRepair 67 - - -
RewardRepair 48 - - -
Recoder 61 - - -
TBar 54 - - -
CURE 52 - - -
GPT-Neo 125M 9 6 - 5
GPT-Neo 1.3B 18 7 - 12
GPT-Neo 2.7B 20 10 - 13
GPT-J 28 14 - 16
GPT-NeoX 34 18 - 21
CodeT5 6 - 6 -
INCODER 1.3B 32 - 32 -
INCODER 6.7B 37 - 37 -
Codex 99 63 62 32
Total 109 69 74 40
Table 23. Performance on Defects4J 2.0, QuixBugs-Java and -Python in [101]
Tools / Models Defects4J 2.0 (78 bugs) QuixBugs Java (40 bugs) QuixBugs Python (40 bugs)
AlphaRepair 35 28 27
RewardRepair 25 20 -
DeepDebug - - 21
Recoder 11 17 -
CURE - 21 -
TBar 8 - -
CoCoNuT - 13 19
GPT-Neo 125M 10 8 9
GPT-Neo 1.3B 11 20 17
GPT-Neo 2.7B 19 18 24
GPT-J 16 22 29
GPT-NeoX 24 21 31
CodeT5 9 10 7
INCODER 1.3B 15 21 25
INCODER 6.7B 21 26 27
Codex 45 38 40
Total 52 38 40
benchmarks, resulting in Table 22 and Table 23 (Columns CF, CI, SL refer to complete function,
correct infilling and single line generation, respectively). It can be observed that many models
achieve similar (or even better) performance through carefully designed APR tools. Additionally, in
the Defects4J 2.0, QuixBugs-Java, and QuixBugs-Python benchmarks, all nine LLMs outperform
TBar (state-of-the-art template-based APR tool). Among these nine LLMs, Codex demonstrates the
best performance, followed by InCoder-6.7B and GPT-NeoX.
In summary, based on the limited available results, Codex demonstrates better performance in
the task of vulnerability repair. However, the information available for this task is still limited, and
there is a lack of performance comparison between state-of-the-art (SOTA) Code LLMs and GPT-4.34 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 24. Average solved rate (%)for each type of problem in [107].
LLMDefect Detection (%)Clone Detection (%) Assert Generation (%) Code Summarization (%)
Zero-shot One-shot Zero-shot One-shot Zero-shot One-shot Zero-shot One-shot
CodeGen-6B 0.3 43.6 1.4 23.4 0.0 56.2 0.0 13.0
ChatGLM-6B 7.1 54.2 17.5 12.8 1.7 46.2 45.0 54.0
Vicuna-7B 54.0 54.1 13.2 - 10.1 31.2 48.0 37.0
Alpaca-7B 45.8 55.4 22.1 - 5.3 41.4 32.0 6.0
Dolly-7B 33.1 49.9 21.3 23.5 1.9 51.0 12.0 14.0
StableLM-7B 44.3 43.4 24.3 - 1.1 44.4 30.0 19.0
CodeAlpaca-7B 51.9 50.3 1.4 10.3 4.4 35.1 9.0 34.0
Dolly-12B 33.8 52.7 23.5 22.6 1.0 51.7 5.0 8.0
Vicuna-13B 49.8 53.0 14.1 6.5 12.0 44.0 63.0 24.0
WizardCoder-15B 54.4 53.8 23.8 7.3 19.4 63 .3 71 .0 50.0
Instruct-CodeGen-16B 47.8 54.6 14.2 20.7 8.4 55.0 9.0 41.0
5.6 Other Evaluation or New Benchmarks
Yuan et al. [ 107] provide a detailed evaluation of 10 open-source guided LLMs on four representative
code understanding and generation tasks: defect detection, clone detection, assertion generation,
and code summarization. While the main focus of the article is on the impact of instruction fine-
tuning on Code LLMs, it also provides valuable insights. Table 24 showcases the performance of
instruction-tuned LLMs on software engineering tasks under zero-shot and one-shot settings. We
can observe that WizardCoder-15B performs relatively well, particularly in the assertion generation
task. The paper also presents several interesting findings: (1) For zero-shot settings, guided LLMs
sometimes outperform small-scale SOTA models fine-tuned specifically for each downstream task
in code understanding and generation tasks. (2) For few-shot settings, the addition of demonstration
examples can significantly improve the performance of guided LLMs on most code understanding
and generation tasks. (3) For fine-tuning settings, further performance enhancement on downstream
code understanding and generation tasks can be achieved through fine-tuning.
Zan et al. [ 110] conducted a comprehensive investigation of Code LLMs on 27 existing LLMs and
reviewed widely used benchmarks and metrics, as shown in Table 25. In the table, P.NL represents the
Problem description’s Natural Language, S.PL denotes the code Solution’s Programming Language,
and T.N. denotes the average Number of Test cases. P.C. and P.L. (S.C. and S.L.) stand for the average
number of Characters and Lines in the Problem description (code Solution), respectively.
Athiwaratkun et al. [ 6] introduce new benchmarks for evaluating code generation models:
MBXP, Multilingual HumanEval, and MathQA-X. The article provides a detailed comparison of the
performance of the CodeGen, OPT, and BLOOM models in multilingual code generation and code
translation tasks. In each test, CodeGen-Mono-16B achieves higher scores. The article also highlights
some important findings: (1) Given the same model size, multilingual models generally outperform
the best monolingual models trained with equivalent training resources, especially when the
model is large enough. This observation suggests that training a single model on all programming
languages is beneficial, and as long as the model has sufficient capacity, its performance will
surpass the best monolingual models. (2) LLMs have the potential to learn from their uncurated
programming languages through unit tests. (3) Few-shot prompts can effectively help the model
acquire knowledge of new languages not seen during training, significantly improving out-of-
domain code generation capabilities. Through error analysis, it is observed that fewer prompts
help reduce compilation or parsing errors, which are the main source of errors when dealing with
programming languages the model is unfamiliar with. (4) Language models possess zero-shot codeA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 35
Table 25. Average solved rate (%)for each type of problem formatting in [ 110]. The asterisk (*) indicates the
number of instances per programming language.
Benchmark Number of instances P.NL S.PLData StatisticsScenarioT.N. P.C. P.L. S.C. S.L.
HumanEval 164 English Python 7.8 450.6 13.7 180.9 6.8 Code Exercise
MBPP 974 English Python 3.1 78.6 1.0 181.1 6.7 Code Exercise
APPS 5,000 English Python 21.0 1743.4 41.6 473.8 21.4 Competitions
CodeContests 165 English Multi. 203.7 1989.2 66.4 2239.3 92.1 Competitions
DS-1000 1,000 English Python 1.6 879.1 31.6 137.4 5.0 Data Science
DSP 1,119 English Python 2.1 756.9 17.8 226.3 7.6 Data Science
MBXP 974∗English Multi. 3.1 419.9 14.8 - - Multilingual
MBXP-HumanEval 164∗English Multi. 7.8 825.6 30.0 - - Multilingual
HumanEval-X 164∗English Multi. 7.8 468.4 15.5 264.6 12.1 Multilingual
MultiPL-HumanEval 164∗English Multi. 7.8 453.9 13.0 - - Multilingual
MultiPL-MBPP 974∗English Multi. 3.1 181.2 5.4 - - Multilingual
PandasEval 101 English Python 6.5 244.5 7.2 46.2 1.3 Public Library
NumpyEval 101 English Python 3.5 222.9 7.0 29.9 1.1 Public Library
TorchDataEval 50 English Python 1.1 329.0 8.6 50.7 1.3 Private Library
MTPB 115 English Python - 72.7 1.0 - - Multi-Turn
ODEX 2022 c 945 Multi. Python 1.8 26.6 2.0 50.4 1.9 Open-Domain
BIG-Bench 32 English Python 4.7 341.8 3.0 - - Code Exercise
translation capabilities, and this translation ability extends to monolingual models. (5) Multilingual
models are more robust to prompt perturbations and can better summarize code.
Tang et al. [ 90] introduce BIOCODER, a benchmark for evaluating LLMs’ ability to generate
bioinformatics code. The article presents the performance of InCoder, CodeGen, CodeGen2, Santa-
Coder, StarCoder, StarCoder+, InstructCodeT5+, and ChatGPT on BIOCODER, as shown in Table 26.
Notably, StarCoder+ is the result of fine-tuning StarCoder on Java for 2000 steps, while all others
are zero-shot results. It can be observed that ChatGPT performs the best on all tasks. The article
also highlights an interesting phenomenon: despite InstructCodeT5+, CodeGen, and CodeGen2
having larger parameter sizes than InCoder and SantaCoder, their performance is significantly
worse. The authors attribute this to InstructCodeT5+, CodeGen, and CodeGen2 being trained on
single-line completions rather than function completions. Additionally, InstructCodeT5+, CodeGen,
and CodeGen2 have relatively smaller context constraints. The authors further note that context
constraints have a significant impact on how different models perform under different prompts.
Kou et al. [ 43] evaluated the differences between LLMs and human model attention in pro-
gramming based on keyword coverage and Cohen’s Kappa consistency level. The article shows
that among the five models evaluated, CodeGen exhibited the highest consistency with human
programmers. However, the results presented in the article are limited, and there was no testing
conducted on state-of-the-art (SOTA) models.
In conclusion, we can summarize the following points:
•The current evaluation of LLMs focuses more on code generation tasks, with less emphasis on
evaluating or researching other tasks such as vulnerability repair. There is a lack of relevant
evaluation work, and when new models are released, there is limited attention given to tasks
like vulnerability repair.
•Code generation tasks have well-known benchmarks like HumanEval. However, other tasks
lack such widely recognized benchmarks.36 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 26. Performance with five prompt versions of BIOCODER [90].
Model PromptJava Python
Pass@1 Pass@5 5ass@10 Pass@20 Pass@1 Pass@5 Pass@10 Pass@20
InCoder-6BSummary at Top 0 0 0 0 0.828 2.016 3.006 4.459
Uncommented 0 0 0 0 0.032 0.159 0.318 0.637
Summary Only 0 0 0 0 1.688 5.320 8.332 12.006
Summary at Bottom - - - - 0.610 2.587 4.303 6.274
Necessary Only 0 0 0 0 0.032 0.159 0.318 0.637
SantaCoder-1.1BSummary at Top 0 0 0 0 0.637 1.338 1.844 2.548
Uncommented 0 0 0 0 0.287 0.764 0.955 1.274
Summary Only 0 0 0 0 2.965 9.848 14.227 18.181
Summary at Bottom - - - - 0.510 1.949 3.013 4.459
Necessary Only 0 0 0 0 0.032 0.159 0.318 0.637
StarCoder-15.5BSummary at Top 0 0 0 0 3.694 13.197 19.359 24.554
Uncommented 0 0 0 0 0.318 1.062 1.591 2.548
Summary Only 0 0 0 0 4.682 15.225 21.200 27.166
Summary at Bottom - - - - 6.465 13.824 16.746 19.076
Necessary Only 0 0 0 0 0.127 0.603 1.123 1.911
StarCoder-15.5B
(finetuned)Summary at top 0 0 0 0 - - - -
Uncommented 0 0 0 0 - - - -
Summary Only 0.200 1.000 2.000 4.000 - - - -
Summary at bottom - - - - - - - -
Necessary Only 3.300 12.097 19.545 30.000 - - - -
StarCoder+Summary at Top 0 0 0 0 2.675 9.133 14.019 19.650
Uncommented 0 0 0 0 0.510 0.955 1.274 1.911
Summary Only 1.300 5.031 8.042 12.000 2.548 8.279 12.864 18.057
Summary at Bottom - - - - 4.172 11.772 14.933 17.197
Necessary Only 0 0 0 0 0.127 0.457 0.609 0.637
InstructCodeT5+ All prompt types 0 0 0 0 0 0 0 0
CodeGen-6B-monoSummary at Top 0 0 0 0 0.637 0.637 0.637 0.637
Uncommented 0 0 0 0 0 0 0 0
Summary Only 0 0 0 0 0.637 0.637 0.637 0.637
Summary at Bottom - - - - 2.070 4.535 5.896 7.006
Necessary Only 0 0 0 0 0 0 0 0
CodeGen-16B-monoSummary at Top 0 0 0 0 0.637 0.637 0.637 0.637
Uncommented 0 0 0 0 0 0 0 0
Summary Only 0 0 0 0 0.637 0.637 0.637 0.637
Summary at Bottom - - - - 2.166 5.137 6.022 6.369
Necessary Only 0 0 0 0 0 0 0 0
CodeGen2-7BSummary at Top 0 0 0 0 0.637 0.637 0.637 0.637
Uncommented 0 0 0 0 0.510 0.637 0.637 0.637
Summary Only 0 0 0 0 0.860 2.494 3.962 6.242
Summary at Bottom - - - - 0.510 1.019 1.207 1.274
Necessary Only 0 0 0 0 0 0 0 0
GPT-3.5-TurboSummary at Top 4.100 7.235 8.989 11.600 22.771 33.461 36.551 39.490
Uncommented 6.300 11.563 14.436 18.000 11.019 19.075 21.680 24.204
Summary Only 17.400 33.199 37.878 42.000 24.682 33.997 37.132 40.127
Summary at Bottom - - - - 13.439 20.040 22.460 25.478
Necessary Only 43.500 52.582 53.995 55.400 28.758 39.529 44.029 47.771A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 37
•In code generation tasks, the Code-LLaMA series of LLMs perform the best, especially
with Unnatural-Code-LLaMA-34B showing outstanding performance. In API-related code
generation tasks, ToolCoder performs better. GPT-4 and GPT-3.5 (GPT-3.5-turbo) also exhibit
good performance in code generation.
•For test case generation tasks, GPT-4 and GPT-3.5 (GPT-3.5-turbo) demonstrate better perfor-
mance.
•In code summarization tasks, CodeT5+ outperforms GPT-3.5 (GPT-3.5-turbo).
•In code translation tasks, GPT-4 performs better.
•For vulnerability repair tasks, based on limited results, Codex shows better performance.
However it should be noted that, except for the relatively accurate results in code generation
tasks, the results in other tasks are not precise enough. For example, in tasks such as code translation
and code summarization, there is a lack of comparative evaluation work between SOTA Code LLMs
such as Unnatural-Code-LLaMA-34B and GPT-4. Furthermore, apart from code generation, we also
lack relevant benchmarks to measure the differences in capabilities among various models in other
software engineering tasks. An interesting thing to note is that different literature records vary in
terms of LLMs’ performance in HumanEval, especially for ChatGPT and GPT-3.5 models, and these
inconsistencies are not simply due to errors in quoting. The reasons for this occurrence are not yet
clear, and it is worth discussing.
6 RELATED WORK
Artetxe et al. [ 5] conducted a comprehensive analysis and summary of 27 code-based large models
released before December 2022. These 27 models were tested and evaluated on the HumanEval
benchmark using a zero-shot approach to provide intuitive comparative results. The article identifies
the key factors for the success of code-based large models as model parameters, data quality,
and expert tuning. However, there are still many open challenges in code testing benchmarks
for large language models (LLMs). For example, most of these benchmarks only have problem
descriptions in English and Python code solutions, which cannot cover multiple natural languages
and programming languages. The article suggests that current code-based large models still face
challenges in terms of comprehension ability, inference ability, explanatory ability, adaptive learning
ability, and multitasking ability.
Zheng et al. [ 118] discuss the applications of LLMs in the field of software engineering. The
article organizes and categorizes 123 selected works and literature on the intersection of software
engineering and LLMs. It classifies them according to software engineering tasks, revealing the
research focuses and potential directions for combining various software engineering tasks with
LLMs. Additionally, the article reveals the performance of LLMs in these tasks, along with their
limitations, and provides directions for future research and optimization.
Some efforts aim to enhance the capabilities of existing LLMs in software engineering tasks. Gong
et al. [ 29] introduces CODETF, an open-source library based on transformers for code LLMs and
code intelligence. CODETF is designed with a unified interface to enable fast access and development
across different types of models, datasets, and tasks. Strictly speaking, CODETF is not a conventional
LLM in the traditional sense but more of a methodology. Lu et al. [ 55] addresses the phenomenon of
training code LLMs on large, uncleaned source code corpora scraped from the internet. The paper
discusses the security, privacy, and licensing issues associated with generated LLMs and provides
four feasible recommendations to address these concerns. Le et al. [ 45] presents a Cross-language
Code representation with a large-scale pre-training (XCode) method, which utilizes several abstract
syntax trees and ELMo-enhanced variational autoencoders to train multiple pre-trained source
code language models on approximately 1.5 million code snippets. Maddigan et al. [ 60] enhances38 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
the robustness of existing pre-trained models by designing nine PL-NL enhancement operators to
group semantically equivalent variants.
Some works have utilized the capabilities of LLMs to design automated tools or frameworks.
Gao et al. [ 28] introduces CollabCoder, a system that supports users in completing qualitative
coding through multiple stages using LLMs. Tanaka et al. [ 89] proposes a new learning approach
called Inductive Bias Learning (IBL), which combines Inductive Concept Learning (ICL) and code
generation techniques. The article suggests that the prediction models generated by IBL have the
potential to replace existing machine learning models in terms of interpretability and inference
speed. Das et al. [ 22] addresses the cumbersome and time-consuming process of generating and
integrating code views for each programming language and presents a tool called COMEX. COMEX
allows researchers and developers to create and harvest multiple code views that can be used by
LLMs for various software engineering tasks.
Karmakar et al. [ 39] evaluated the code synthesis ability of the Codex model using a set of
115 Python problem statements from HackerRank and found clear evidence of Codex’s ability
to generate code from memory. Ding et al. [ 23] proposed a static evaluation framework for code
completion generated by large language models and performed error analysis on the CodeGen model
using a large-scale real-world Python evaluation set. Martínez et al. [61] explored the application
of law graduates in code detection and evaluated the code detection capability of GPT-3.5 using
matrix multiplication. N41 conducted an empirical study to assess ChatGPT’s unit test generation
capability and proposed a method called CHATTESTER, which utilizes ChatGPT itself to improve
the quality of generated tests. The results showed that GPT-3.5 achieved an accuracy rate close
to 100% in the evaluation. Furthermore, there are several related works that have developed new
benchmarks for testing the code generation capabilities of LLMs, such as CODETASKCL [ 104],
CodeBLEU [75], CodeSearchNet [36], and Galeras [77].
7 CONCLUSION
After a comprehensive review, this paper explores the performance and value of specialized LLMs
in the field of software engineering. Firstly, we collected and screened 134 works related to Code
LLMs. Next, we organized Code LLMs based on the types of institutions to which their main
developers belong, revealing the relationships between Code LLMs, general LLMs, and among
Code LLMs themselves. Furthermore, we conducted a comprehensive analysis and compilation
of the performance of general LLMs and Code LLMs in software engineering tasks. We provided
statistical results and analyzed interesting phenomena. Lastly, we maintained the scores of 126
Code LLMs on major benchmarks and conducted a detailed analysis of their performance across
different software engineering tasks. The contribution of this paper lies in the comprehensive
overview of Code LLMs and their performance. This work can assist Code LLM developers in
making informed decisions regarding base models and fine-tuning approaches, and it also provides
key improvement directions for Code LLMs.
REFERENCES
[1]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program
Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 . Association
for Computational Linguistics, 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[2]Toufique Ahmed and Premkumar T. Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization.
In37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October
10-14, 2022 . ACM.
[3]Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas
Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, YangtianA Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 39
Zi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert,
Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu,
Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li,
Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra.
2023. SantaCoder: don’t reach for the stars! CoRR abs/2301.03988 (2023). https://doi.org/10.48550/arXiv.2301.03988
arXiv:2301.03988
[4]Miltiadis Allamanis and Charles Sutton. 2013. Mining source code repositories at massive scale using language
modeling. In Proceedings of the 10th Working Conference on Mining Software Repositories, MSR ’13, San Francisco,
CA, USA, May 18-19, 2013 , Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim (Eds.). IEEE Computer
Society, 207–216. https://doi.org/10.1109/MSR.2013.6624029
[5]Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, et al .2022. Efficient Large Scale Language
Modeling with Mixtures of Experts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 . Association for Computational
Linguistics, 11699–11732. https://aclanthology.org/2022.emnlp-main.804
[6]Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, et al .2023. Multi-lingual Evaluation of Code
Generation Models. arXiv:2210.14868 [cs.LG]
[7]Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark
Chen. 2022. Efficient Training of Language Models to Fill in the Middle. CoRR abs/2207.14255 (2022). https:
//doi.org/10.48550/arXiv.2207.14255
[8]Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, et al .2022. GPT-NeoX-20B: An Open-Source
Autoregressive Language Model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large
Language Models . https://arxiv.org/abs/2204.06745
[9]Nghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven C. H. Hoi. 2023. CodeTF:
One-stop Transformer Library for State-of-the-art Code LLM. arXiv:2306.00029 [cs.SE]
[10] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,
Abhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming
Languages for Code LLMs. arXiv:2308.09895 [cs.PL]
[11] Aaron Chan, Anant Kharkar, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Alec Helyar, Eslam Kamal,
Mohamed Elkamhawy, and Neel Sundaresan. 2023. Transformer-based Vulnerability Detection in Code at EditTime:
Zero-shot, Few-shot, or Fine-tuning? arXiv:2306.01754 [cs.CR]
[12] Shubham Chandel, Colin B. Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and Evaluating a Jupyter
Notebook Data Science Assistant. CoRR abs/2201.12901 (2022). arXiv:2201.12901 https://arxiv.org/abs/2201.12901
[13] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.
com/sahil280114/codealpaca.
[14] Jiachi Chen, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021. Maintenance-related concerns for post-deployed
Ethereum smart contract development: issues, techniques, and future challenges. Empirical Software Engineering 26,
6 (2021), 1–44.
[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[17] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, et al .2022. PaLM: Scaling Language Modeling
with Pathways. CoRR abs/2204.02311 (2022). https://doi.org/10.48550/arXiv.2204.02311 arXiv:2204.02311
[19] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng
Xiao, Bo Shen, Lin Li, et al .2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv
preprint arXiv:2207.11280 (2022).
[20] Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:
multi-mode translation of natural language and Python code with transformers. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , Bonnie Webber,
Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 9052–9065. https://doi.org/
10.18653/v1/2020.emnlp-main.728
[21] CodedotAl. 2021. GPT-Code-Clippy . https://github.com/CodedotAl/gpt-code-clippy40 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[22] Debeshee Das, Noble Saji Mathews, Alex Mathai, Srikanth Tamilselvam, Kranthi Sedamaki, Sridhar Chimalakonda, and
Atul Kumar. 2023. COMEX: A Tool for Generating Customized Source Code Representations. arXiv:2307.04693 [cs.SE]
[23] Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali Krishna Ramanathan,
Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. 2023. A Static Evaluation of Code
Completion by Large Language Models. arXiv:2306.03203 [cs.CL]
[24] Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis. 2023.
Large Language Models of Code Fail at Completing Code with Potential Bugs. arXiv:2306.03438 [cs.LG]
[25] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin
Peng, and Yiling Lou. 2023. ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code
Generation. arXiv:2308.01861 [cs.CL]
[26] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke
Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
https://openreview.net/pdf?id=hQwb-lbM6EL
[27] Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui,
Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, and Yong Yu. 2023.
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940 [cs.CL]
[28] Jie Gao, Yuchen Guo, Gionnieve Lim, Tianqin Zhang, Zheng Zhang, Toby Jia-Jun Li, and Simon Tangi Perrault. 2023.
CollabCoder: A GPT-Powered Workflow for Collaborative Qualitative Analysis. arXiv:2304.07366 [cs.HC]
[29] Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang, and Zenglin Xu. 2022. MultiCoder: Multi-
Programming-Lingual Pre-Training for Low-Resource Code Completion. arXiv:2212.09666 [cs.CL]
[30] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. HiPPO: Recurrent Memory with Optimal
Polynomial Projections. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
[31] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, et al .2023.
Textbooks Are All You Need. CoRR abs/2306.11644 (2023). https://doi.org/10.48550/arXiv.2306.11644 arXiv:2306.11644
[32] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir
Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS.
NeurIPS (2021).
[33] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (1997), 1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735
[34] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu
Wang. 2023. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv preprint
arXiv:2308.10620 (2023).
[35] Huaggingface. 2021. Training CodeParrot from Scratch. https://huggingface.co/blog/codeparrot.
[36] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet
Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http:
//arxiv.org/abs/1909.09436
[37] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a
Neural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,
ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The Association for Computer Linguistics.
https://doi.org/10.18653/v1/p16-1195
[38] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2023. Self-planning
Code Generation with Large Language Models. arXiv:2303.06689 [cs.SE]
[39] Anjan Karmakar, Julian Aron Prenner, Marco D’Ambros, and Romain Robbes. 2022. Codex Hacks HackerRank:
Memorization Issues and a Framework for Code Synthesis Evaluation. arXiv:2212.02684 [cs.SE]
[40] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.
2023. xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation
and Retrieval. arXiv:2303.03004 [cs.CL]
[41] B. A. Kitchenham. 2007. Kitchenham, B.: Guidelines for performing Systematic Literature Reviews in software
engineering. EBSE Technical Report EBSE-2007-01. IEEE Computer Society (2007).
[42] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, et al .2022. The Stack: 3 TB of permissively
licensed source code. CoRR abs/2211.15533 (2022). https://doi.org/10.48550/arXiv.2211.15533 arXiv:2211.15533
[43] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is Model Attention Aligned with Human
Attention? An Empirical Study on Large Language Models for Code Generation. arXiv:2306.01220 [cs.SE]A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 41
[44] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel
Fried, Sida Wang, and Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
arXiv:2211.11501 [cs.SE]
[45] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. 2022. CodeRL: Mastering
Code Generation through Pretrained Models and Deep Reinforcement Learning. In NeurIPS . http://papers.nips.cc
[46] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, et al .2021.
Datasets: A Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, EMNLP 2021, Online and Punta Cana, Dominican
Republic, 7-11 November, 2021 , Heike Adel and Shuming Shi (Eds.). Association for Computational Linguistics, 175–184.
https://doi.org/10.18653/v1/2021.emnlp-demo.21
[47] Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng,
Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.
2023. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs.
arXiv:2305.03111 [cs.CL]
[48] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured Chain-of-Thought Prompting for Code Generation.
arXiv:2305.06599 [cs.SE]
[49] Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large
Code Generation Models are Better Few-Shot Information Extractors. arXiv:2305.05711 [cs.CL]
[50] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,
Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier
Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel
Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,
Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,
Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,
Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan
Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish
Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you!
arXiv:2305.06161 [cs.CL]
[51] Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung, and Jeff Kramer. 2023. Finding
Failure-Inducing Test Cases with ChatGPT. arXiv:2304.11686 [cs.SE]
[52] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks
Are All You Need II: phi-1.5 technical report. arXiv:2309.05463 [cs.CL]
[53] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, et al .2022. Competition-level
code generation with AlphaCode. Science 378, 6624 (2022), 1092–1097. https://doi.org/10.1126/science.abq1158
arXiv:https://www.science.org/doi/pdf/10.1126/science.abq1158
[54] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692
(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[55] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and Chun Zuo. 2023. LLaMA-Reviewer: Advancing Code Review Automation
with Large Language Models through Parameter-Efficient Fine-Tuning. arXiv:2308.11148
[56] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, et al .2021. CodeXGLUE: A Machine Learning Benchmark Dataset for
Code Understanding and Generation. In Proceedings of the Neural Information Processing Systems Track on Datasets
and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual , Joaquin Vanschoren and Sai-Kit
Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc
[57] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and
Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. CoRR abs/2306.08568
(2023). https://doi.org/10.48550/arXiv.2306.08568 arXiv:2306.08568
[58] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural
Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing .
Association for Computational Linguistics, Lisbon, Portugal, 1412–1421. https://doi.org/10.18653/v1/D15-1166
[59] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023. The Scope of
ChatGPT in Software Engineering: A Thorough Investigation. arXiv:2305.12138 [cs.SE]
[60] Paula Maddigan and Teo Susnjak. 2023. Chat2VIS: Generating Data Visualisations via Natural Language using
ChatGPT, Codex and GPT-3 Large Language Models. arXiv:2302.02094 [cs.HC]
[61] Pablo Antonio Martínez, Gregorio Bernabé, and José Manuel García. 2023. Code Detection for Hardware Acceleration
Using Large Language Models. arXiv:2307.10348 [cs.SE]42 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[62] MFTCoder. 2023. CodeFuse-MFTCoder: Multitask Fine-Tuned Code LLMs . https://github.com/codefuse-ai/MFTCoder
[63] Anthony MOI, Nicolas Patry, Pierric Cistac, Pete, et al .2022. huggingface/tokenizers: Rust 0.13.2 . https://doi.org/10.
5281/zenodo.7298413
[64] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru
Tang, Leandro von Werra, and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models.
arXiv:2308.07124 [cs.CL]
[65] Artashes Arutiunian Nathan Coooper et al .2022. Code Clippy Data: A large dataset of code data from Github for
research into code language models. https://github.com/ncoop57/gpt-code-clippy
[66] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. CodeGen2: Lessons for
Training LLMs on Programming and Natural Languages. arXiv:2305.02309 [cs.LG]
[67] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
https://openreview.net/pdf?id=iaYcJKpY2B_
[68] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Evalu-
ating and Explaining Large Language Models for Code Using Syntactic Structures. arXiv:2308.03873 [cs.SE]
[69] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris
Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2023. Understanding the Effectiveness of Large
Language Models in Code Translation. arXiv:2308.03109 [cs.SE]
[70] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2022. Examining
Zero-Shot Vulnerability Repair with Large Language Models. arXiv:2112.02125 [cs.CR]
[71] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining
Zero-Shot Vulnerability Repair with Large Language Models. In 44th IEEE Symposium on Security and Privacy, SP
2023, San Francisco, CA, USA, May 21-25, 2023 . IEEE, 2339–2356. https://doi.org/10.1109/SP46215.2023.10179420
[72] Phind. 2023. Phind-CodeLlama . https://huggingface.co/Phind/Phind-CodeLlama-34B-v1
[73] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.
Res.21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[74] Veselin Raychev, Pavol Bielik, and Martin T. Vechev. 2016. Probabilistic model for code with decision trees. In
Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages,
and Applications, OOPSLA 2016, part of SPLASH 2016, Amsterdam, The Netherlands, October 30 - November 4, 2016 ,
Eelco Visser and Yannis Smaragdakis (Eds.). ACM, 731–747. https://doi.org/10.1145/2983990.2984041
[75] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,
and Shuai Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. CoRR abs/2009.10297 (2020).
arXiv:2009.10297 https://arxiv.org/abs/2009.10297
[76] replit. 2023. replit-code-v1-3b . https://huggingface.co/replit/replit-code-v1-3b
[77] Daniel Rodriguez-Cardenas, David N. Palacio, Dipin Khati, Henry Burke, and Denys Poshyvanyk. 2023. Benchmarking
Causal Study to Interpret Large Language Models for Source Code. arXiv:2308.12415 [cs.SE]
[78] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,
Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer,
Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin,
Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code.
arXiv:2308.12950 [cs.CL]
[79] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexan-
dra Sasha Luccioni, François Yvon, Matthias Gallé, et al .2022. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 (2022).
[80] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An Empirical Evaluation of Using Large Language
Models for Automated Unit Test Generation. arXiv:2302.06527 [cs.SE]
[81] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang
Zhao, Yuenan Guo, and Qianxiang Wang. 2023. PanGu-Coder2: Boosting Large Language Models for Code with
Ranking Feedback. arXiv:2307.14936 [cs.CL]
[82] Atsushi Shirafuji, Yutaka Watanobe, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, and Jun Suzuki. 2023.
Exploring the Robustness of Large Language Models for Solving Programming Problems. arXiv:2306.14583 [cs.CL]
[83] Mohammed Latif Siddiq, Beatrice Casey, and Joanna C. S. Santos. 2023. A Lightweight Framework for High-Quality
Code Generation. arXiv:2307.08220 [cs.SE]
[84] Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and Vini-
cius Carvalho Lopes. 2023. Exploring the Effectiveness of Large Language Models in Generating Unit Tests.A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends 43
arXiv:2305.00418 [cs.SE]
[85] Giriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar. 2023. ChatGPT: A Study on its Utility for Ubiquitous
Software Engineering Tasks. arXiv:2305.16837 [cs.SE]
[86] Ruoxi Sun, Sercan O. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas Pfister. 2023.
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. arXiv:2306.00739 [cs.CL]
[87] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen,
Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023. Automatic Code Summarization via ChatGPT:
How Far Are We? arXiv:2305.12865 [cs.SE]
[88] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation
using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering . 1433–1443.
[89] Toma Tanaka, Naofumi Emoto, and Tsukasa Yumibayashi. 2023. Inductive-bias Learning: Generating Code Models
with Large Language Model. arXiv:2308.09890 [cs.LG]
[90] Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark Gerstein. 2023. BioCoder: A Benchmark
for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. arXiv:2308.16458 [cs.LG]
[91] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth
Garg. 2023. VeriGen: A Large Language Model for Verilog Code Generation. arXiv:2308.00708 [cs.PL]
[92] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, et al .2022. LaMDA: Language Models for Dialog
Applications. CoRR abs/2201.08239 (2022). arXiv:2201.08239 https://arxiv.org/abs/2201.08239
[93] THUDM. 2023. CodeGeeX2: A More Powerful Multilingual Code Generation Model . https://github.com/THUDM/
CodeGeeX2
[94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al .2023. LLaMA: Open
and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). https://doi.org/10.48550/arXiv.2302.13971
arXiv:2302.13971
[95] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA . 5998–6008.
[96] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. opencoderplus . https://huggingface.co/openchat/
opencoderplus
[97] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. CoRR abs/2212.10560 (2022). https:
//doi.org/10.48550/arXiv.2212.10560 arXiv:2212.10560
[98] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+:
Open Code Large Language Models for Code Understanding and Generation. arXiv:2305.07922 [cs.CL]
[99] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Junnan Li, and Steven Hoi. 2023. CodeT5Mix: A Pretrained Mixture
of Encoder-decoder Transformers for Code Understanding and Generation. https://openreview.net/forum?id=
VPCi3STZcaO
[100] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-
decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[101] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2022. Practical Program Repair in the Era of Large Pre-trained
Language Models. arXiv:2210.14179 [cs.SE]
[102] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.
WizardLM: Empowering Large Language Models to Follow Complex Instructions. CoRR abs/2304.12244 (2023).
https://doi.org/10.48550/arXiv.2304.12244 arXiv:2304.12244
[103] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A Systematic Evaluation of Large
Language Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming
(San Diego, CA, USA) (MAPS 2022) . Association for Computing Machinery, New York, NY, USA, 1–10. https:
//doi.org/10.1145/3520312.3534862
[104] Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh
Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. 2023. Exploring Continual Learning for Code
Generation Models. arXiv:2307.02435 [cs.LG]
[105] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun Han, and David Lo. 2023. What Do
Code Models Memorize? An Empirical Study on Large Language Models of Code. arXiv:2308.09932 [cs.SE]
[106] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxi-
ang Wang. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models.
arXiv:2302.00288 [cs.SE]44 Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[107] Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. Evaluating Instruction-Tuned
Large Language Models on Code Comprehension and Generation. arXiv:2308.01240 [cs.CL]
[108] Daoguang Zan, Bei Chen, Yongshun Gong, Junzhi Cao, Fengji Zhang, Bingchao Wu, Bei Guan, Yilong Yin, and Yongji
Wang. 2023. Private-Library-Oriented Code Generation with Large Language Models. arXiv:2307.15370 [cs.SE]
[109] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, et al .2022. CERT: Continual Pre-training on Sketches for Library-
oriented Code Generation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI
2022, Vienna, Austria, 23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org, 2369–2375. https://doi.org/10.24963/ijcai.2022/329
[110] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2023.
Large Language Models Meet NL2Code: A Survey. arXiv:2212.09420 [cs.SE]
[111] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, et al .2021. PanGu- 𝛼: Large-scale Autoregressive Pretrained Chinese
Language Models with Auto-parallel Computation. CoRR abs/2104.12369 (2021). arXiv:2104.12369 https://arxiv.org/
abs/2104.12369
[112] Jiyang Zhang, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. 2023. Multilingual Code Co-Evolution Using Large
Language Models. arXiv:2307.14991 [cs.SE]
[113] Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. 2023. ToolCoder: Teach Code Generation Models
to use API search tools. arXiv:2305.04032 [cs.SE]
[114] Yuntong Zhang, Xiang Gao, Gregory J. Duck, and Abhik Roychoudhury. 2022. Program vulnerability repair via
inductive inference. In ISSTA ’22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis,
Virtual Event, South Korea, July 18 - 22, 2022 , Sukyoung Ryu and Yannis Smaragdakis (Eds.). ACM, 691–702. https:
//doi.org/10.1145/3533767.3534387
[115] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).
[116] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li,
Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
Evaluations on HumanEval-X. arXiv:2303.17568 [cs.LG]
[117] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023.
Towards an Understanding of Large Language Models in Software Engineering Tasks. arXiv preprint arXiv:2308.11396
(2023).
[118] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023.
Towards an Understanding of Large Language Models in Software Engineering Tasks. arXiv:2308.11396 [cs.SE]
[119] Li Zhong and Zilong Wang. 2023. A Study on Robustness and Reliability of Large Language Model Code Generation.
arXiv:2308.10335 [cs.CL]
[120] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and Mingwen Wang. 2022. CodeGen-Test:
An Automatic Code Generation Model Integrating Program Test Information. arXiv:2202.07612 [cs.SE]
[121] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language Agent Tree
Search Unifies Reasoning Acting and Planning in Language Models. arXiv:2310.04406 [cs.AI]
[122] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. 2023. CodeBERTScore: Evaluating Code Generation
with Pretrained Models of Code. arXiv:2302.05527 [cs.SE]
[123] Terry Yue Zhuo. 2023. Large Language Models Are State-of-the-Art Evaluators of Code Generation.
arXiv:2304.14317 [cs.AI]
[124] Terry Yue Zhuo. 2023. Large Language Models Are State-of-the-Art Evaluators of Code Generation.
arXiv:2304.14317 [cs.AI]
[125] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023.
On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on
Codex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics .
Association for Computational Linguistics, Dubrovnik, Croatia, 1090–1102.

Title: A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
Abstract: A graph is a fundamental data model to represent various entities and their
complex relationships in society and nature, such as social networks,
transportation networks, financial networks, and biomedical systems. Recently,
large language models (LLMs) have showcased a strong generalization ability to
handle various NLP and multi-mode tasks to answer users' arbitrary questions
and specific-domain content generation. Compared with graph learning models,
LLMs enjoy superior advantages in addressing the challenges of generalizing
graph tasks by eliminating the need for training graph learning models and
reducing the cost of manual annotation. In this survey, we conduct a
comprehensive investigation of existing LLM studies on graph data, which
summarizes the relevant graph analytics tasks solved by advanced LLM models and
points out the existing remaining challenges and future directions.
Specifically, we study the key problems of LLM-based generative graph analytics
(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),
LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based
applications. LLM-GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge graph (KG) based
augmented retrieval, while LLM-GIL focuses on learning and reasoning over
graphs, including graph learning, graph-formed reasoning and graph
representation. We summarize the useful prompts incorporated into LLM to handle
different graph downstream tasks. Moreover, we give a summary of LLM model
evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM
models. We also explore open problems and future directions in this exciting
interdisciplinary research area of LLMs and graph analytics.
Full Text: A Survey of Large Language Models on Generative
Graph Analytics: Query, Learning, and Applications
Wenbo Shang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
cswbshang@comp.hkbu.edu.hkXin Huang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
xinhuang@comp.hkbu.edu.hk
Abstract —A graph is a fundamental data model to represent
various entities and their complex relationships in society and
nature, such as social networks, transportation networks, finan-
cial networks, and biomedical systems. Recently, large language
models (LLMs) have showcased a strong generalization ability
to handle various NLP and multi-mode tasks to answer users’
arbitrary questions and specific-domain content generation.
Compared with graph learning models, LLMs enjoy superior
advantages in addressing the challenges of generalizing graph
tasks by eliminating the need for training graph learning models
and reducing the cost of manual annotation. In this survey, we
conduct a comprehensive investigation of existing LLM studies
on graph data, which summarizes the relevant graph analytics
tasks solved by advanced LLM models and points out the
existing remaining challenges and future directions. Specifically,
we study the key problems of LLM-based generative graph
analytics (LLM-GGA) with three categories: LLM-based graph
query processing (LLM-GQP), LLM-based graph inference and
learning (LLM-GIL), and graph-LLM-based applications. LLM-
GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge
graph (KG) based augmented retrieval , while LLM-GIL focuses
on learning and reasoning over graphs, including graph learning ,
graph-formed reasoning and graph representation . We summarize
the useful prompts incorporated into LLM to handle different
graph downstream tasks. Moreover, we give a summary of LLM
model evaluation, benchmark datasets/tasks, and a deep pro and
cons analysis of LLM models. We also explore open problems
and future directions in this exciting interdisciplinary research
area of LLMs and graph analytics.
Index Terms —Graph, LLMs, GNNs, Prompt, Survey
I. I NTRODUCTION
Large language models (LLMs) possess billions of parame-
ters and have been trained on extensive corpora using training
strategies like instruction tuning [1] [2] and Direct Preference
Optimization(DPO) [3], enabling them to exhibit powerful
reasoning and semantic representation capabilities, thereby
advancing AI intelligence closer to human levels. Undoubt-
edly, LLMs currently serve as the foundation model for NLP
tasks [4] [5] [6], showcasing strong generalization abilities to
handle various NLP tasks such as question answering [7] [8],
machine translation [9], code generation [10] [11], etc. LLMs
have demonstrated extensive common knowledge and robust
semantic comprehension abilities, fundamentally transforming
existing text-processing workflows. While initially designed
for text data, LLMs are increasingly being utilized for tasks
LLM-GGALLM-GQPLLM-GILGraph-LLM-based applications
GraphsLLMs+
Graph Queries
LLMsAnswersLLMs
Graphs
GraphsGraph representationGraph learning tasksGraph reasoning
KGsFig. 1: Illustration of the LLM-GGA domain. LLM-GGA do-
main includes three principal components: LLM-based graph
query processing (LLM-GQP), which necessitates the melding
of graph analytics techniques and LLM prompts for query pro-
cessing; LLM-based graph inference and learning (LLM-GIL),
focusing on learning and reasoning over graphs; Graph-LLM-
based applications that employ the graph-LLM framework to
address non-graph tasks, such as recommendation systems.
beyond language processing, aiming to leverage the robust ca-
pabilities of LLMs across different tasks, showcasing superior
performance.
Graphs, as structured data, play a crucial role in various real-
world application scenarios, including the citation networks
[12], social networks [13], molecular graphs [14], web links
[15], and to name a few. Various graph analytics tasks have
been studied to show their usefulness, e.g., node classification,
link prediction, subgraph mining, influence maximization, and
so on. Their versatility and ability to capture complex rela-
tionships have made graphs indispensable tools in academic
research and industry platforms. Recently, one kind of graph-
based learning model, graph neural network (GNN) [16] [17],
has been widely studied and applied to solve challenging graph
tasks. The GNN models utilize recursive message passing
[18] and aggregation mechanisms [19] among nodes to derive
representations of nodes, edges, or entire graphs, which have
been used for various downstream tasks. This is thanks to
the strong ability of GNN models to capture both graph
structure and node features. However, GNNs exhibit weak
generalization capabilities [20] [21] [22], requiring retraining
for different graph tasks and showing limited transfer ability.arXiv:2404.14809v1  [cs.CL]  23 Apr 2024In other words, no universal graph foundation model could be
easily generalized to handle various types of graph tasks.
Therefore, whether LLMs’ powerful reasoning, semantic
representation, and generalization capabilities can be applied
to address graph tasks, leading to the inspiration of a graph
foundation model, is the core of current efforts in leveraging
existing large language models for graph-related tasks. In one
word, can LLMs solve graph data tasks? More specifically,
we study three detailed questions: (a) what specific graph
tasks can LLMs answer? (b) How do LLMs tackle these
tasks? (c) What is the effectiveness of LLM-based methods
in solving these tasks compared with the existing graph-based
approaches?
To address the above question, this survey conducts a
comprehensive study of existing relevant work on graph an-
alytics and LLMs, focusing on exploring the key issue of
the LLM-based generative graph analytics (LLM-GGA) field.
Drawing from a thorough investigation of the LLM-GGA
domain, we offer a structured and methodical analysis that
delineates the field into three principal components: LLM-
based graph query processing (LLM-GQP), which necessitates
the melding of graph analytics techniques and LLM prompts
for query processing; LLM-based graph inference and learning
(LLM-GIL), focusing on learning and reasoning over graphs;
and lastly, graph-LLM-based applications that employ the
graph-LLM framework to address non-graph tasks, such as
recommendation systems. The framework is shown in Figure
1.
We categorize these three main components into a total
of six directions to provide a guideline for researchers to
conduct more in-depth studies. LLM-GQP includes graph
understanding and KG-based augmented retrieval directions.
LLM-GIL covers graph learning, graph-formed reasoning, and
graph representation directions. The sixth direction is graph-
LLM-based applications. The following section details these
six directions:
•Graph understanding tasks. This research direction is
studying whether LLMs can solve graph algorithm prob-
lems, exploring whether LLMs can comprehend graph
structures to conduct graph mining and graph search. Cur-
rent methods have primarily explored LLMs’ understand-
ing of graph structures, such as shortest path, clustering
coefficient computation [23] [24], and more complex
problems like maximum flow and Hamilton path [25] [26]
[27]. Two main methods are introduced: prompting and
supervised fine-tuning (SFT). The prompting methods
explore the LLM’s current structural understanding abil-
ity through query processing. Meanwhile, SFT methods
enhance LLMs’ structure understanding capability by
tuning it on specific graph datasets. However, many more
tasks are yet to be explored, such as the community
search, keyword search, subgraph pattern mining, and
other NP-hard complex graph problems [28] [29].
•Graph learning tasks. This direction explores whether
LLMs can combine graph structure and attributes for
learning, extracting features of nodes, edges, and graphs,and understanding the semantic information of graphs,
for example, tasks like node classification, graph classi-
fication, and GQL generation [30] [31] [32] [33]. There
are two main pipelines: LLM-GNN pipelines and LLM
pipelines. LLMs can leverage their powerful reasoning
ability and vast knowledge repository to enhance GNNs
and also can predict results directly.
•Graph-formed reasoning. This direction explores how
LLMs use graph structures to simulate human thinking
during reasoning [34] [35] [36], enabling them to solve
more complex reasoning problems such as algorithmic,
logical, and mathematical tasks. Graph-formed reasoning
involves two types of reasoning: think on the graph and
verify on the graph. Think on the graph refers to LLMs
deriving the final conclusion through the graph structure.
Verify on the graph refers to verifying the correctness of
the LLMs’ intermediate or final outputs through the graph
structure.
•Graph representation. This direction explores enhanc-
ing graph representation with LLMs, particularly for Text
Attribute Graphs (TAGs). LLMs’ strong text representa-
tion capabilities allow text embeddings to capture deeper
semantic nuances. However, the key challenge in this
area remains how to capture and integrate graph structure
into graph representation effectively [37] [38] [39]. There
are three forms of graph representation: graph embed-
ding, graph-enhanced text embedding, and graph-encoded
prompts. Graph embedding methods transform a graph
into a sequential format for LLM processing. Graph-
enhanced text embedding methods integrate structure into
text embedding, where the integration method can be
concatenation. Graph-encoded prompts focus on the way
a graph is described within prompts.
•Knowledge Graph (KG) based augmented retrieval.
This direction investigates the relationship between LLMs
and Knowledge Graphs (KGs). With the emergence of
LLMs, discussions have arisen regarding the potential
replacement of KGs [40] [41] [42] [43]. Consequently,
this paper discusses the limitations of LLMs in processing
factual knowledge, evaluates strategies for improving
LLM efficacy via KG-based augmented retrieval, and
investigates potential avenues for future advancements in
this field.
•Graph-LLM-based applications. This part explores the
tasks where graph-LLM-based methods can be applied
for useful downstream application [44] [45] [46], such as
recommendation systems, conversational understanding,
and so on.
We comprehensively analyze these six research directions
of LLM-GGA to provide valuable definitions and highlighted
methodologies. We also highlight the pros and cons of these
methods and showcase future directions. To further explore the
capabilities of LLMs reliably, this paper uses the prompting
method to test the effectiveness of LLMs in tasks such as
graph structure understanding, graph learning, and graph-formed reasoning. Details of the prompts and results obtained
during testing are also provided. Additionally, we refine and
compile commonly used and effective prompts for graph-
related tasks, assisting researchers in conducting experiments.
Furthermore, this paper also organizes and introduces the
code for existing popular methods, benchmarks for LLM-GGA
tasks, and evaluations measuring LLM performance in graph
tasks to facilitate future research.
Our contributions and the identified challenges for future
research. In this paper, we provide a comprehensive survey of
the state-of-the-art work on LLMs applied to graph data. We
begin by delineating six critical directions in the field of LLM-
GGA: graph structure understanding, graph learning, graph-
formed reasoning, graph representation, KG-based augmented
retrieval, and graph-LLM-based applications. This categoriza-
tion clarifies the current work and offers a guideline for future
research endeavors. In each direction, we propose a structured
introduction and summarization using vivid examples and
offer suitable specific pipelines. We analyze the advantages
and limitations of current methodologies and suggest avenues
for future research. Furthermore, we organize resources related
to benchmarks, evaluations, and code links within the LLM-
GGA domain to facilitate further investigation by researchers.
Lastly, we identify the fundamental challenges in the LLM-
GGA field, which are the primary obstacles to advancing LLM
in solving graph tasks, including the fundamental issue of how
sequential LLM handles structural graph data, the efficiency
issue of large-scale graph data, and the NP-hard problems of
complex graph analytics. This clarification guides the research
direction for future work on LLM-GGA.
Roadmaps . The organization of this paper is as follows. We
first present the fundamental preliminaries and summarize
the graph description language, which converts graphs into
sequences before inputting them into LLMs in Section II.
Then, we introduce six tasks of LLM-based graph analytics
one by one. We present the graph structure understanding
direction in Section III, graph learning direction in Section IV,
graph-formed reasoning in Section V, graph representation in
Section VI, KG-based augmented retrieval in Section VII and
graph-LLM-based applications in Section VIII. In the above
six directions, we clarify the tasks that LLMs can perform,
discuss the methodologies, conduct a comparative analysis,
and propose guidelines and principles in this direction. Fol-
lowing this, Section IX introduces the popular datasets and
new datasets for solving the above tasks and also provides
metrics for evaluating LLMs or tasks in different directions. In
Section X, we identify and discuss the current and upcoming
challenges that LLM-GGA faces and future directions. Finally,
our conclusions are presented in Section XI.
II. P RELIMINARY
In the subsequent section, we will initially introduce graph
data, proceed to discuss GNNs as a paradigm of graph-
based learning models, then introduce LLMs and distinguish
LLMs and PLMs, and ultimately introduce graph descriptionlanguage, which can transform the graph into sequential data
as the input of LLMs.
A. Graph
Graph data represents complex relationships through nodes
and edges, where nodes represent entities and edges represent
their interconnections. This structure excels at modeling intri-
cate networks such as social, biological, and transportation
systems. It enables analyses like community detection and
shortest path calculations, offering critical insights into the
dynamics of various systems. Formally, a general graph can
be represented as G= (V,E), where VandEdenote the set
of nodes and edges. V={v1, v2, ..., v n}where the number
of nodes is |V|and|V|=n.E={eij}where the number of
edges is |E|andeijis an edge from vitovj.
B. Graph Neural Network
Graph Neural Networks (GNNs) [16] [17] are a type of deep
learning model that can handle graph-structured data. The goal
of these GNNs is to learn representations for each node, which
are computed based on the node’s own features, the features of
the edges connected to it, the representations of its neighbors,
and the features of its neighboring nodes,
hl
v=AGGR (hl−1
v,{hl
u−1 :u∈Nv};θl) (1)
where hl
vrepresents the representation of node vin the l-th
layer. AGGR denotes the aggregation function that aggregates
the representations of neighboring nodes from the previous
layer. For the tasks that focus on individual nodes, e.g.,
node classification, the learned representations can be used
directly to accomplish specific objectives. However, for the
tasks that consider the entire graph, e.g., graph classification,
a global representation can be obtained by pooling or applying
other methods to the representations of all nodes. This global
representation can then be used to perform the corresponding
tasks.
C. Large Language Models
Currently, there is no precise definition for Large Language
Models (LLMs). However, according to the pioneering surveys
[47] [48] on LLMs, a distinction can be made between LLMs
and Pre-trained Language Models (PLMs). LLMs are large
language models with billion-level parameters that are pre-
trained on massive amounts of data, such as Llama [5] and
ChatGPT. Conversely, PLMs are pre-trained language models
with million-level parameters that can be more easily fine-
tuned on task-specific data. While LLMs and PLMs share
similarities in their pre-training process, the former is char-
acterized by its larger size and ability to generate human-like
text. Thus, it is essential to consider the potential implications
of using LLMs in various applications.
D. Graph Description Language
Graphs are represented in the structured data in arbitrary
shapes, while LLMs typically process sequential data, such
as the text as a sequence of words. To bridge this gap,Graph Structure Understanding Tasks
14320Graph  Size CalculationGiven <graph>, what is the number of nodes and edges in this graph? Please answer with the number of nodes: X, number of edges: X. 14320Degree CalculationGiven <graph>, what is the degree of node 4?  Or, like, find the node degree of node [given node] in the given graph.14320Connected Nodes SearchGiven <graph>. Is node 3 the 1-hop neighbor of node 4? List the answers after “Ans:” in the format of [Yes, No,]. 14320Edge ValidationGiven <graph>. Is there an edge between node 1 and node 2?14320Path SearchGiven <graph>. Simple path: Find a single path from node 0 to node 4 connected by edges in the given graph. Shortest path: Give the shortest path from node 0 to node 4.14320Attribute RetrievalGiven <graph>, what is the title of node 0?Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.14320Graph DensityGiven <graph>, what is the density of the given graph?14320EccentricityGiven <graph>, what is the eccentricity of the node 0?14320Pattern matchingGiven <graph>, in the given graph, the triangle must be connected by three edges, list the triangle after ”Ans:” in the format of [0-1-2]14320Topological SortingIn a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 1, ... Q: Can all the nodes be visited? Give the solution.12100Bipartite Graph MatchingThere are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some of the jobs. Each job can only accept one applicant and a job applicant can be appointed for only one job. Applicant 0 is interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find the job they are interested in.14320Hamilton PathGiven <graph>, is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path, adjacent nodes must be connected with edges. 14320Maximum FlowIn a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity 10... Q: What is the maximum flow from node 0 to node 3?101559(a)(b)(c) (d) (e)
(f)(g)(h)(j)
(k)(l) (m)(n) 14320Graph DiameterGiven <graph>, what is the diameter of the given graph?(i)Fig. 2: Graph Structure Understanding tasks.
the graph description language (GDL) transforms the graph
into sequential data, which can be inputted into an LLM.
Specifically, GDL aims to convert graphs into sequential data
while retaining the structure and unique attributes of the graph.
This conversion allows the graph’s information to be fed into
an LLM for processing. There are several graph description
languages:
•Text description. Graph structure can be described using
words such as ‘Node 1 is connected to Node 2’ and
‘There are three nodes connected to Node 1’.
•Adjacency list. An adjacency list represents each vertex
in the graph with the collection of its neighbouring
vertices or edges. Node A is connected with node B and
node C can be denoted as N(v) ={B, C}.
•Edge list. An edge list represents the edge connections
between two nodes in the graph. (A, B) indicates a
connection between nodes A and B.
•GML. Graph Modelling Language [49] consists of an
unordered sequence of node and edge elements enclosed
within ‘[·]’.
•GraphML. Graph Markup Language [50] consists of
XML containing a graph element and an unordered
sequence of node and edge elements.
•SQL. Several specialized SQL languages are designed
specifically for working with graph data. These languages
are also capable of serving as graph description lan-
guages. Some notable examples include Cypher [51], a
query language developed by Neo4j, and Gremlin [52],
SPARQL [53], and GSQL [54]. They combine SQL-
like syntax with graph-specific constructs and algorithms,
making them suitable for complex graph analytics tasks.
•Multi-modality encoding. Except for text description,graph structure can also be represented using image
description and motif description. The graph can be visu-
alized as an image and inputted into an LLM to process
images. Alternatively, motifs such as stars, triangles, or
clique patterns can represent the graph structure as input
into an LLM.
•Encode as a story. The graph can be encoded within
a specific context, such as a friendship, co-authorship,
social network, politician, or expert. For example, the
connections between nodes can represent friendship re-
lationships. We can assign names to the nodes, such as
‘David’ and ‘Alice’.
Notably, (1) different graph description languages can yield
different results of LLMs. Therefore, it is suggested to test
with multiple GDLs and select the one with the best experi-
mental results. (2) If needed, the LLM’s output form can be
specified along with GDLs in the prompt. LLMs often generate
excessive reasoning processes that may be unnecessary, so
standardizing the LLM’s output can be beneficial.
III. G RAPH STRUCTURE UNDERSTANDING TASKS
Graph structure understanding tasks evaluate whether LLMs
can comprehend graph structures. Simple tasks include the
queries of neighbors, shortest paths, connectivity, the calcu-
lation of graph radius, and the clustering coefficient. More
complex tasks include solving maximum flow problems and
performing topological sorting. These tasks need LLMs to
comprehend graph structures locally and globally, as shown in
Figure 2. In this section, we present 21 graph understanding
tasks along with their definitions. Subsequently, we elaborate
on the two main methods currently used to address graph
structure understanding tasks: prompting and supervised fine-
tuning LLMs.Task Prompts
Graph Data Loading The structure of the [file path] molecular graph of the benzene ring contains a hexagon.
Graph Size Detection Given [graph], what is the number of nodes and edges in this graph? Please answer with the number of nodes:
X, number of edges: X.
Degree Detection Given [graph], what is the degree of node 4? Or, find the node degree of node [given node] in the given graph.
Connected Nodes Given [graph]. Is node 5 the 1-hop neighbor of node 4? List the answers after “Ans:” in the format of [Yes, No,].
Edge Detection Given [graph]. Is there an edge between node 1 and node 2?
Path Simple path: Given the undirected graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0,
1), (1, 4), (1, 3), (4, 3), (3, 2)], find a single path from node 1 to node 2 connected by edges in the given graph.
Shortest path: Given the directed graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0, 1),
(1, 4), (1, 3), (4, 3), (3, 2)], give the shortest path from node 0 to node 4.
Attribute Retrieval Given [graph]. What is the title of node 0?
Graph Density Given [graph]. What is the density of the given graph?
Eccentricity Given [graph]. What is the eccentricity of the given graph?
Graph Radius Given [graph]. What is the radius of the given graph?
Graph Diameter Given [graph]. What is the diameter of this graph?
Graph Periphery Given [graph]. What is the periphery of this graph? Or What are the nodes included by the periphery of the given
graph?
Clustering Coefficient Computing Given [graph]. What is the clustering coefficient of [given node]?
TABLE I: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data.
A. Task Introduction
1) Graph size calculation: Graph size refers to the number
of nodes and edges in a graph. Given a general graph G=
(V,E), the graph size detection task is to detect the |V|and|E|
inG. Through this task, LLMs are expected to understand the
fundamental structure of a graph accurately. Given a prompt
describing the graph and asking related queries, LLMs are
supposed to determine |V|and|E|, as shown in Figure 2 (a).
2) Degree calculation: The degree detection task involves
determining the degree of a specific node in a graph. The
neighbors of node vcan be denoted as N(v) ={u|(u, v)∈
E(v)}, where E(v)is the edge set including edges connected to
v. The degree of viis the number of its neighbors in G, which
can be denotes as degG(vi) =|N(vi)|. Through this task,
LLMs are expected to comprehend the context surrounding vi
and identify N(vi)accurately. By inputting a prompt about
viandG, LLMs are expected to calculate the degree of the
node. This task is shown in Figure 2 (b).
3) Connected nodes search: The connected nodes detection
task involves finding all the nodes in NG(vi)ofviinG. Given
the prompt about G, LLMs are expected to analyze the local
structure of the given node viand determine NG(vi), as shown
in Figure 2 (c).
4) Edge validation: The edge detection task refers to
whether there exists an edge eijoreijbetween viandvi.
Through this task, LLMs are expected to accurately identify
the connectivity between nodes and understand the localstructure of nodes. Given the prompt about the neighbors of
vito the LLMs, LLMs will likely indicate whether eijoreij
exists, as shown in Figure 2 (d).
5) Path search: We consider two types of paths, including
the simple path and the shortest path, as shown in Figure 2
(e). Given a graph G={V,E}, the simple path task involves
detecting whether there exists a path (vi, ..., v j)between a
source node viand a target node vjinG. In other words, it
is about finding a simple path (vi, ..., v j)between viandvj
without specific requirements. This task evaluates the ability of
LLMs to traverse a graph and understand its structure. Given
the prompt about Gto LLMs, the goal is to return a simple
path from vitovj.
Given a weighted directed acyclic graph G={V,E}with
each edge e∈ E has a non-negative weight w(e), the shortest
paths task involve finding a path p= (e1, e2, . . . , e n)from a
source node to a target node in Gsuch that the sum of the
weights of edges w(p) =Pn
i=1w(ei)is minimized. LLMs
can evaluate the length of the shortest path and identify the
qualified paths. This task can be further divided into three
objectives: 1. Finding the shortest path between two nodes. 2.
Finding all the shortest paths for all paired nodes. 3. Finding
the average length of all the shortest paths. This task assesses
whether the LLM can effectively determine the shortest route
between two specified nodes within the graph.
6) Attribute retrieval: The attribute retrieval task involves
retrieving detailed information related to nodes, such as theTask Prompts
Graph Partition In the academic collaboration network dblp, scholar #355233 is involved in [TBR] local community formed by his/her
collaborators.
Graph Searching According to the Freebase knowledge graph, the relation between entity /m/027rn and entity /m/06cx9 is [TBR].
Pattern matching Triangle: find a single triangle containing node X. Or in the given graph, the triangle must be connected by three edges,
list the triangle after ”Ans:” in the format of [0-1-2]. Cliques: find all the cliques with Nnodes in the given graph, list all
the cliques after ”Ans:” in the format of [0-1-2] and separate the answers by a comma. Wedge Centering find a single
wedge containing node X in the given graph, node X must be the center of this wedge, list the wedge after ”Ans:” in the
format of [0-1-2].
Cycle Check In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge. The nodes are numbered
from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0) Q: Is there a cycle in this graph?
Topological Sort In a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 4, ... Q: Can all the nodes
be visited? Give the solution.
Maximum Flow In a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity
10... Q: What is the maximum flow from node 0 to node 3?
Bipartite Graph Matching There are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some
of the jobs. Each job can only accept one applicant and a job applicant can be appointed for only one job. Applicant 0 is
interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find
the job they are interested in.
Hamilton Path Given [graph], is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path,
adjacent nodes must be connected with edges.
Graph Neural Networks Given [graph]. Embeddings: node 0: [1,1], ... In a simple graph convolution layer, each node’s embedding is updated by
the sum of its neighbors’ embeddings. Q: What’s the embedding of each node after one layer of simple graph convolution
layer?
Dynamic Graph In an undirected dynamic graph, (u, v, t) means that node u and node v are linked with an undirected edge at time t. Your
task is to answer when two nodes are first connected in the dynamic graph. Two nodes are connected if there exists a
path between them. Given an undirected dynamic graph with the edges [(0, 1, 0), (1, 2, 1), (0, 2, 2)]. When are node 0
and node 2 first connected?
TABLE II: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data. [TBR] means to be
reasoned by LLMs.
attributes of a node. For example, in a citation network, LLMs
are tasked with retrieving specific attributes of a node, such
as the title, abstract, or author of a paper. Given the prompt
aboutGand detailed attribute information, LLMs are expected
to retrieve the required information, as shown in Figure 2 (f).
7) Graph density: Graph density represents the ratio be-
tween the number of edges present in a graph and the
maximum number of edges that the graph can have. For an
undirected simple graph G={V,E}, the graph density is
defined as:
D=2|E|
|V|(|V| − 1)(2)
For a directed simple graph, the graph density is defined as:
D=|E|
|V|(|V| − 1)(3)
This task requires LLM to calculate the density of a given
graph and assess its understanding of the entire graph, as
shown in Figure 2 (g).
8) Eccentricity: The eccentricity of a node in a graph is
defined as the length of the longest shortest path starting at that
node. The eccentricity of one node: this task requires LLMsto answer the eccentricity of a given node. The eccentricity of
many nodes: this task requires LLMs to answer the eccentricity
of a subset of nodes or all the nodes in the graph, as shown
in Figure 2 (h).
9) Graph radius: Based on the eccentricity of nodes, the
radius of a graph is the minimum eccentricity of any vertex in
the graph. LLMs can calculate the radius of the given graph
with the description of the graph.
10) Graph center: The center of a graph is the set of
vertices of graph eccentricity equal to the graph radius. Based
on the eccentricity task and graph radius task, LLMs should be
given the graph information and asked to calculate the graph
center.
11) Graph diameter: Based on the shortest path, the diam-
eter of a graph is the length of the shortest path between the
most distant nodes. LLMs can calculate the graph’s diameter
with the given graph information, as shown in Figure 2 (i).
12) Graph periphery: Based on the graph eccentricities and
graph diameter, the graph periphery is a set of vertices that
have graph eccentricities equal to the graph diameter. LLMs
can answer questions related to the graph periphery using the
given graph information.Fig. 3: Examples for Path Task with GPT3.5 - Graph Structure
Understanding Tasks.
Fig. 4: Examples for Maximum Flow Task with GPT3.5 -
Graph Structure Understanding Tasks.
Fig. 5: Examples for Bipartite Graph Matching Task with
GPT3.5 - Graph Structure Understanding Tasks.13) Clustering coefficient computing: The clustering coef-
ficient is a measure of how connected a vertex’s neighbors are
to one another. We define the edges among neighbors of vi
as{ejk:vj, vk∈ NG(vi), ejk∈ E} . For directed graphs, the
clustering coefficient is defined as:
Ci=|{ejk:vj, vk∈ NG(vi), ejk∈ E}|
|NG(vi)||NG(vi)−1|(4)
For undirected graphs, the clustering coefficient is defined as:
Ci=2|{ejk:vj, vk∈ NG(vi), ejk∈ E}|
|NG(vi)||NG(vi)−1|(5)
LLMs can calculate the clustering coefficient as a measure of
the degree to which nodes in a graph tend to cluster together.
14) Graph partition: This task is an online social network
reasoning task, which is to infer the community structure of
an online social network by partitioning users into different
clusters based on their interaction information. Each cluster
represents a social community formed by users who interact
with each other frequently. LLMs partition the users of the
social network based on user social interaction patterns and
generate the resulting cluster assignments.
15) Graph searching: This task is a knowledge graph
reasoning task, which involves inferring relationships between
entities based on their information or inferring connected
entities based on the information of entities and relationships.
Specifically, LLM takes entities or relationships as input
and searches for relevant entities or relationships to generate
output.
16) Pattern matching: This task is to identify star, wedge,
triangle, or clique patterns that contain a target node. The
target node can be defined as the center of the pattern.
Alternatively, the task can involve identifying whether these
patterns exist in a given graph and determining the number
of occurrences. Given a description of the LLM graph, the
goal is for LLM to identify different patterns and provide the
corresponding answers, as shown in Figure 2 (j).
17) Cycle validation: This task is to determine whether a
graph contains a cycle. Given G={V,E}, a cycle is a non-
empty trail with a vertex sequence (v1, v2, ..., v n, v1). Given
the graph information, LLM is asked to determine whether
this graph has a cycle.
18) Topological sorting: Topological sorting of a directed
graph G={V,E}refers to a linear ordering of its nodes,
where each node comes before all the nodes it points to,
for example, there exists a directed edge eijfrom vitovj,
vicomes before vjin the ordering. The resulting array of
node ordering is called topological ordering. LLM is required
to generate a valid topological sorting for the given directed
graph, and there may be multiple valid solutions, as shown in
Figure 2 (k).
19) Maximum flow: Given a capacity constraint, the max-
imum flow problem involves finding the maximum flow that
can be sent through pipes, channels, or other pathways in a
network. Define a flow as fijfrom vitovjand the capacity
on edge eijascij. Given the capability constraints, fij≤cijManual promptSelf-promptingAPI call prompts………………Frozen LLMManual prompt: Given <graph>, what is the number of nodes and edges in this graph? Please answer with the number of nodes: X, number of edges: X. ………………Frozen LLMInstructor: You are a brilliant graph master that can handle anything related to graphs like retrieval, detection and classification.Graph description language: GML, GraphML, etc. Query: What is the clustering coefficient of node X?New contexts: Text description of input graph generated by LLM itself.Final output: The clustering coefficient of node X is …………………Trainable LLMRegular prompt: What is the diameter of the binomial tree?API call prompt: The diameter of the binomial tree is Cerulean [GR(GL(“gpr”, “binomial_tree”), “toolx:diameter”) →r]Fig. 6: Promoting methods in graph structure understanding tasks. There are three categories: manual prompts, self-prompting,
and API call prompts.
for all eij. Meanwhile,P
fij>0fij=P
fji>0fjifor∀viexcept for
the source and the target {s, t}Given a network graph, LLM
generates a path that maximizes the flow from the source to
the sink, as shown in Figure 2 (l).
20) Bipartite graph matching: A bipartite graph is a type
of graph where the nodes can be divided into two disjoint sets,
UandV, such that there are no adjacent nodes within each set.
A matching in a bipartite graph is a set of edges where no two
edges share an endpoint. In a maximum matching, if any edge
is added, it is no longer a matching. For a given bipartite graph,
there can be multiple maximum matchings. LLM can generate
a solution that finds the maximum matching, as shown in
Figure 2 (m).
21) Hamilton Path: In an undirected graph, a Hamiltonian
path is a path in the graph that visits each vertex exactly once.
Given an undirected graph, the task is for LLM to find a valid
Hamiltonian path, as shown in Figure 2 (n).
B. Graph Structure Understanding Methods
The rise of LLMs has sparked researchers’ interest in
exploring their powerful text processing and generalization
capabilities for graph reasoning. Therefore, existing efforts
have introduced various benchmarks to test LLMs’ graph
reasoning potential, aiming to explore their capacity to address
graph-related problems. Prompting methods have emerged as
the primary approach to assess LLMs’ understanding of graph
structures, with some studies also focusing on fine-tuning
LLMs to enhance their graph reasoning abilities. Thus, the
following two main methods are introduced: prompting method
andfine-tuning LLMs .
1)Prompting method :The prompting method [55] can
be categorized into three main types: manual prompt, self-
prompting, and API call prompt, as shown in Figure 6.
Most studies utilize manual prompts, where carefully crafted
prompts guide LLMs to comprehend graph structures better
and understand the objectives of graph tasks, thereby leading
to improved performance on graph-related tasks.
Manual prompts. NLGraph [27] introduces a benchmark
aiming to assess the understanding capabilities of LLMs in
processing textual descriptions of graphs and translating theminto conceptual spaces. This benchmark covers various graph
reasoning tasks like connectivity, shortest path, maximum flow,
and graph neural network construction, with three difficulty
levels (easy, medium, hard) based on graph size and density.
Meanwhile, the number of nodes n=|V|and the probability
pcontrol edge generation, allowing manipulation of graph size
and density for a more reliable evaluation of LLM potential
in graph comprehension.
Next, to guide LLMs in solving these graph tasks, two
prompt methods are proposed by NLGraph [27]: build-a-graph
prompting and algorithmic prompting.
Prompt III-1: Build-a-Graph Prompting. Build-a-Graph
prompting method is to guide LLMs to conceptual grounding
by adding one sentence shown as red words below:
Prompt III-1: Build-a-Graph Prompting
Given <graph description >.Let’s construct a graph
with the nodes and edges first. Q: What is the degree
of node 4?
Prompt III-2: Algorithmic Prompting. The algorithmic
prompting method is designed to guide LLMs to engage in
algorithmic reflection and thinking by adding the details of
the algorithm shown as red words below:
Prompt III-2: Algorithmic Prompting
We can use a Depth-First Search (DFS) algorithm to
find the shortest path between two given nodes in an
undirected graph.
The basic idea is to start at one of the nodes and use
DFS to explore all of its adjacent nodes. At each
node, you can keep track of the distance it takes to
reach that node from the starting node.
Once you have explored all the adjacent nodes, you
can backtrack and pick the node which has the
shortest distance to reach the destination node.
Given <graph description >. Q: Give the shortest pathfrom node 0 to node 4.
Compared with other advanced prompts and in-context
learning techniques, the two proposed prompts perform better
on graph tasks. Based on the experiments, LLMs indeed pos-
sess preliminary graph reasoning abilities. Also, the benefits
of advanced prompting and in-context learning diminish in
complex graph problems and may even have a negative impact.
LLMs are also susceptible to false correlations, performing
poorly on graph structures such as chains and cliques.
To explore whether LLMs can truly comprehend graph
structures and reason on graphs, meanwhile, enhance the
performance of LLM-GQP tasks, [26] and [24] test LLMs
also using manual prompts, where [26] explores the conditions
under which LLMs can benefit from the inherent structural
information in the data and examines two potential factors in-
fluencing LLM’s performance: data leakage and homogeneity.
In summary, the conclusions are as follows:
•No evidence suggests that LLM’s performance is signif-
icantly attributed to data leakage.
•The performance of LLMs on target nodes is positively
correlated with the local homogeneity of the nodes.
[24] investigates the graph reasoning capabilities of LLMs
and introduces new evaluation metrics—comprehension, cor-
rectness, fidelity, and rectification—to assess LLMs’ pro-
ficiency in understanding graph structures and performing
reasoning tasks. The findings reveal that LLMs can effectively
understand graph structures and perform reasoning tasks.
However, LLMs still face challenges in structural reasoning,
particularly in multi-answer tasks where GPT models demon-
strate errors and overconfidence. In contrast, GPT-4 displays
improved self-correction abilities.
Beyond static graphs, LLMs’ ability to understand dynamic
graph structures is also assessed. Dynamic graphs change
over time, capturing temporal network evolution patterns.
LLM4DyG [25] introduces the LLM4DyG benchmark, which
uses prompting methods to evaluate LLMs’ spatio-temporal
understanding capabilities on dynamic graphs.
Prompt III-3: DST2. The newly proposed Disentangled
Spatial-Temporal Thoughts (DST2) prompting technique en-
hances LLMs’ spatial and temporal understanding of dynamic
graphs. DST2 is shown below:
Prompt III-3: DST2
DyG Instruction: In an undirected dynamic graph, (u,
v, t) means that node u and node v are linked with an
undirected edge at time t.
Task Instruction: Your task is to answer when two
nodes are first connected in the dynamic graph. Two
nodes are connected if there exists a path between them.
Answer Instruction: Give the answer as an integer
number at the last of your response after ’Answer:’Exemplar: Here is an example: Question: Given an
undirected dynamic graph with the edges [(0, 1, 0), (1,
2, 1), (0, 2, 2)]. When are node 0 and node 2 first
connected? Answer:1
Question: Question: Given an undirected dynamic
graph with the edges [(0, 9, 0), (1, 9, 0), (2, 5, 0), (1, 2,
1), (2, 6, 1), (3, 7, 1), (4, 5, 2), (4, 7, 2), (7, 8, 2), (0, 1,
3), (1, 6, 3), (5, 6, 3), (0, 4, 4), (3, 4, 4), (3, 6, 4), (4, 6,
4), (4, 9, 4), (6, 7, 4)]. When are node 2 and node 1
first connected?
Results show that LLMs have preliminary spatio-temporal
understanding capabilities on dynamic graphs. Dynamic graph
tasks become increasingly challenging with larger graph sizes
and densities while insensitive to periods and data generation
mechanisms.
We provide manual prompt examples for various graph
structure understanding tasks in Table I and Table II. Addi-
tionally, we test LLMs with GPT 3.5 for path, max flow, and
bipartite graph matching using manual prompts, as shown in
Figure 3, Figure 4 and Figure 5 respectively.
For self-prompting. Self-prompting refers to the process
where an LLM continuously updates the initial prompt to make
it easier for LLMs to understand and more beneficial for solv-
ing tasks. In other words, the LLM designs prompts based on
the original prompt. GPT4Graph [23] utilizes self-prompting
by continuously updating the prompt with descriptions related
to the graph. Specifically, first, the graph data is converted into
graph description languages, as shown in Section II-D. Then,
together with queries, it is inputted into the prompt handler to
create a prompt, which is then inputted into the LLM. Based
on the output of the LLM, the prompt is updated and re-
input into the LLM, repeating multiple rounds of updates to
obtain an optimized graph description context, such as context
summarization and format explanation. This process can be
seen as the LLM’s self-updating prompt procedure. Finally,
the optimized graph description context is input along with
the original input into the LLM to obtain the final result.
Prompt III-4: Self-prompting. The input original prompt is
shown below:
Prompt III-4: Self-prompting
Instructor: You are a brilliant graph master that can
handle anything related to graphs like retrieval, detection
and classification.
Graph description language: GML, GraphML as
shown in Section II-D.
Context: Node P357 has 4 neighbors, where each of
which are about anomaly detection with statistical
models...
Query: What is the clustering coefficient of node P357?
This paper conducts experiments on the obgn-arxiv [56] andAminer [57] datasets and finds that:
•The design of prompts significantly impacts the results.
The choice of graph description language, the orga-
nization of input data, and the position of in-context
knowledge, such as questions, statements, and examples,
all affect the model’s ability to understand the graph
structure.
•Role prompting techniques can improve the effectiveness
of LLMs by guiding the model to view the graph as
roles and relationships between roles in a specific context.
Providing LLMs with more semantic information leads to
more accurate results.
•Examples in prompts have mixed impacts on graph
structure understanding. Adding examples in prompts to
guide LLMs in understanding graph structures may not
necessarily improve the results; in some graph structure
learning tasks, examples may introduce noise.
API call prompts LLMs exhibit limited ability to perform
precise mathematical calculations, multi-step logical reason-
ing, spatial topological structuring, and temporal information
processing. To bridge these gaps, taking inspiration from
recent models such as ChatGPT and Toolformer [58], Graph-
ToolFormer [59] is proposed to equip LLMs with graph
reasoning capabilities by training them over a prompt dataset
that contains graph reasoning API annotated by ChatGPT.
These graph reasoning APIs are used to call external reasoning
tools. Then, the trained LLMs can solve graph tasks, from
loading graph data and inferring graph attributes to graph
partition tasks.
The framework consists of three parts. First, it generates a
prompt dataset by providing ChatGPT with a regular prompt,
guiding ChatGPT to add an API call to the original prompt,
and then creating a prompt with an API call.
Prompt III-5: API call prompts
Prompt III-5: API call prompts
Example 1
Input:(Regular prompt)
The structure of the benzene ring molecular graph of
benzene ring contains a hexagon.
Output:(API call prompt)
The structure of the [GL(”benzenering”)] molecular
graph of benzene ring contains a hexagon.
Example 2
Input:(Regular prompt)
What is the diameter of the binomial tree?
Output:(API call prompt)
The diameter of the binomial tree is [GR(GL(”gpr”,
”binomial tree”), ”toolx:diameter”) →r].
Second, fine-tune existing LLMs such as GPT-J [60] [61],
LLaMA [5] [62], etc., using technologies like LoRA [63]
on the generated prompt dataset. Thirdly, utilize the fine-
tuned LLM for inference to add graph reasoning API calls
………………Instructions: How many C-C-O triangles are in the molecule?Graph-enhanced prefix:Structural and textual featuresLLMResponse: There is 1 C-C-O triangle in the molecule.Fig. 7: Supervised fine-tuning (SFT) method in graph structure
understanding tasks. Prefix tuning is shown above: combine
graph structural and textual information as prefixes in prefix
tuning and input it into LLM with instructions, like GraphLLM
[64]. Instruction tuning can also be used.
into statements. After generating API call statements, how
can external graph tools be invoked? Graph reasoning query
processing comes in. Graph reasoning query processing entails
utilizing external graph reasoning tools based on API call
statements to obtain the final answer.
2)Supervised fine-tuning (SFT) method :Beyond lever-
aging prompts for graph-structured tasks with LLMs, cer-
tain studies have also implemented supervised fine-tuning of
LLMs, illustrated in Figure 7. GraphLLM [64] is committed
to addressing the obstacles in graph reasoning by LLMs and
introduces a hybrid model that inherits the capabilities of both
graph learning models and LLMs, enabling LLMs to interpret
and reason about graph data proficiently, utilizing the superior
expressive power of graph learning models.
C. Comparisons and Discussions
In the following part, we compare the prompting and SFT
methods mentioned above.
The prompting method can be divided into three cate-
gories: manual prompts, self-prompting, and API call prompts.
Most current methods primarily rely on manual prompts,
incorporating techniques like Chain of Thought (CoT) [65],
self-consistency [66], and in-context learning [67]. To obtain
better prompt representations, self-prompting methods are also
widely used. However, the exclusive use of manual prompts
and self-prompting offers limited enhancement to model per-
formance, as they merely tap into the pre-existing capabilities
of LLMs. Additionally, due to the limited input window of
LLM, the graph size that can be input to LLM at once is also
restricted, while graph sizes in the real world are typically
large.
For the prompting method, we also propose two feasible
directions to better leverage existing LLMs for handling struc-
ture understanding tasks. The first direction is breaking down
complex tasks into several sub-problems. While LLMs can
tackle simple graph tasks, they struggle with more challenging
ones. Breaking down complex graph understanding tasks into
simpler components enables LLMs to engage in multi-step
reasoning processes, leading to the resolution of complexGraph Learning Tasks
USKGQA
Given <knowledge graph>, the director who directs Inception also direct what?InceptionNolanOppenheimerLeonardois starred byGQL Generation
Given <graph>, the director who directs Inception also direct what? Use Cypher to answer.14320Node ClassificationGiven <graph>, which arxiv CS subcategory does paper ”paper title” with abstract ”paper abstract” belongs to? use the abbreviation to answer.Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.CCCCCOHCGiven <graph>, is this molecule active with H3C4?Graph Classification
14320Node Feature ExplanationGiven <graph>, which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS subcategories as a comma-separated list ordered from most to least likely, in the form ”cs.XX”, and provide your reasoning. Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.14320Edge ClassificationLearnable prompt
USInceptionNolanOppenheimerLeonardois starred by(a) (b) (c) 
(d) (e) (f) Fig. 8: Graph Learning tasks.
issues, such as GoT [59], which can help address more
intricate graph tasks like generating GNN frameworks, k-truss
tasks, kd-core tasks, etc. The second direction is API call
prompts. Inspired by ToolFormer [58], LLMs can be trained
as agents to utilize tools for graph tasks that are hard to
solve. However, current API call prompt methods [59] utilize
LLMs not as agents but solely to convert user queries into
API command strings for processing by subsequent programs,
exemplified in Prompt III-5 .
However, compared to prompting methods, fine-tuning
LLMs with graph data seems a better way to enhance their
understanding of graph structures. There are two mainstream
methods for fine-tuning LLMs: Supervised Fine-Tuning (SFT)
and Reinforcement Learning with Human Feedback (RLHF)
[6]. SFT helps LLMs understand prompts and generate mean-
ingful responses. However, SFT only offers a single human-
written response for each prompt, whereas RLHF provides
detailed human feedback through pairwise comparison label-
ing. Furthermore, to address the instability issue in PPO [68]
training, the Reward Ranked Fine-Tuning (RAFT) [69] can
also be attempted which requires online interaction. For offline
algorithms, methods like DPO [3] and Preference Ranking
Optimization (PRO) [70] can also be utilized for training
LLMs.
IV. G RAPH LEARNING TASKS
A. Tasks Introduction
Recently, LLMs have been shown to possess extensive
common sense and powerful semantic understanding capa-
bilities, fundamentally transforming the existing workflow
for processing text. However, whether LLMs can effectively
handle graph learning tasks, transferring their generalization
ability from text tasks to graph learning tasks, such as node
and graph classification, is still a research subject that needsexploring. These tasks require the model to learn and solve
graph learning tasks, as shown in Figure 8. In this section, we
present seven graph learning tasks along with their definitions.
Next, we introduce graph learning methods, categorized into
three types based on the role of LLMs: LLMs act as enhancers,
LLMs act as predictors, and graph prompts.
1) Node classification: The node classification task requires
LLM to learn based on the neighbors of a node or the attributes
of a node. It involves classifying unseen nodes in a given
graph, such as categorizing papers in an academic network
into different research directions, as shown in Figure 8 (a).
2) Graph classification: The graph classification task re-
quires LLM to classify the entire graph. LLM is given several
labeled graphs and is expected to classify unseen graphs. For
example, a molecule can be viewed as a graph, and LLM
can predict the properties or functions of the molecule by
classifying the graph, as shown in Figure 8 (b).
3) Edge classification: The edge classification task involves
classifying the edges in a graph. Existing methods improve
edge classification by training a learnable graph prompt and
combining it with a GNN or LLM, as shown in Figure 8 (c).
4) Node generation: The node generation task refers to pro-
viding requirements for an LLM to generate nodes, allowing it
to generate node attributes, which are then added to the TAG
to enhance it.
5) Knowledge graph question qnswering (KGQA): Knowl-
edge graph organizes data into a structured format, represent-
ing entities, properties, and relationships. Knowledge graph
question answering (KGQA) aims to capture the most appro-
priate answers by querying the knowledge graph (KG) using
natural language questions. This task evaluates the ability of
LLM to reason and understand the underlying graph structure
to provide accurate answers, as shown in Figure 8 (d).
6) Graph query language (GQL) generation: The graph
query language generation task involves generating graphTask Prompts
KGQA Given [knowledge graph], the director who directs Inception also direct what?
GQL Generation Given [graph], the director who directs Inception also direct what? Use Cypher to answer.
Node Classification Which arxiv CS subcategory does paper ”paper title” with abstract ”paper abstract” belongs to? use the abbreviation
to answer.
Graph Classification Given [graph]. Is this molecule active with H3C4?
Node Feature Explanation Abstract: Text in curve orientation, despite being one of the common text orientations in real world environment... Title:
Total Text A Comprehensive Dataset For Scene Text Detection And Recognition. Question: Which arXiv CS sub-category
does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least
likely, in the form ”cs.XX”, and provide your reasoning.
Edge classification learnable prompt
TABLE III: Prompts for Graph Learning Tasks, where [·] is the input of the data.
query languages, including GQL and Cypher, to perform op-
erations on graph databases. Evaluating LLM’s ability to gen-
erate GQL helps users extract information from the database,
as shown in Figure 8 (e).
7) Node feature explanation: Node feature explanation task
involves extracting the attributes of nodes in a text attribute
graph. For example, in an academic paper network, the node
attributes may include abstracts, titles, etc. LLM is expected to
provide reasoning for the classification process of nodes based
on their text attributes and explain the features of the nodes,
as shown in Figure 8 (f).
B. Graph Learning Methods
LLM-GIL studies focusing on graph learning tasks can be
categorized into three main groups: LLMs act as enhancers,
LLMs act as predictors, and graph prompts. When LLMs act
as enhancers, they leverage their advanced semantic under-
standing of the text, strong reasoning capabilities, and vast
knowledge repository to enhance the text attributes associated
with nodes in the graph to enhance GNNs. When LLMs act
as predictors, LLMs are queried or fine-tuned to predict task
results. Inspired by NLP ideas, the Graph prompt aims to
create a unified framework capable of solving multiple graph
learning tasks. Although LLMs are not used, the concept aligns
with LLM-based pipelines.
In summary, integrating LLMs in graph learning tasks
presents a promising avenue for advancing the field. By
leveraging the strengths of LLMs as enhancers and predictors,
along with the strategic use of graph prompts, researchers can
explore new directions for enhanced performance and more
profound insights in LLM-GIL tasks.
1)LLMs act as enhancers : LLMs act as enhancers per-
tains to the LLMs-GNNs pipelines, where LLMs assume an
enhancer role. Within this framework, LLMs are tasked with
processing text attributes, while GNNs are responsible for
handling graph structures, capitalizing on the complementary
strengths of both components to address graph learning tasks
effectively. LLMs bolster GNNs through three distinct mecha-
nisms: encoding the graph into embeddings (as shown in Fig-
Encoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.…Trainable LLM………Frozen LLM………Trainable LM……
TextEmbeddingsNode Text Attribute
GraphStructureGNN+Fig. 9: Encoding graph into embeddings, when LLMs act as
enhancers. Input the node text attribute into LM/LLM to obtain
text embeddings, then combine the text embeddings with the
graph structure for training and learning in GNNs.
ure 9), generating graph pseudo labels (as shown in Figure 10),
and providing external knowledge or explanations (as shown
in Figure 11). Subsequently, we will provide a comprehensive
elaboration on these three enhancement strategies.
Encoding graph into embeddings. LLMs possess signif-
icant semantic comprehension capabilities to encode better
node embeddings, as shown in Figure 9. TAPE [30] integrates
LM with LLM to generate node embeddings. The process
involves fine-tuning two LM models using original node text
attributes and LLM explanations for node prediction. TheEncoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.Unlabelednodes
NodeswithpseudolabelsTrainingGNNAnnotationwithLLMFig. 10: Generating graph pseudo labels, when LLMs act as
enhancers. Input unlabeled nodes into LLM for labeling, then
use the labeled nodes with pseudo-labels as input for training
the GNNs for graph learning.
Encoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.Node Text Attribute
LLMsEnhancedtextattributesNode Text Attribute
Designedqueries
LLMsExplanationforreasoningprocess1.2.
Fig. 11: Providing external knowledge/explanations, when
LLMs act as enhancers. Two pipelines are shown above. In
the first pipeline, input node text attributes into LLM for
elaboration, enhancing the detail of the text attributes. In the
second pipeline, input node text attributes and designed queries
into LLM. LLM leverages the text attributes to answer queries
and explains the reasoning process.
resulting embeddings are then used as input to train a GNN
model for node classification tasks. To unify graph data and
graph learning tasks, OFA [32] introduces a comprehensive
framework that unifies diverse graph data by describing nodes
and edges using natural language and encoding varied and
potentially cross-domain text attributes into feature vectors
within the same embedding space. The obtained feature vec-
tors are then fed into a GNN to tackle various downstream
tasks effectively. Moreover, SIMTEG [71] and GLEM [31]
involve training an LM with Lora and subsequently generating
embeddings as text representations, then a GNN is trained on
top of these text embeddings. On this basis, G-prompt [33]
introduces a graph adapter to extract node features, thereby
obtaining improved node representations.
Generating graph pseudo labels. Many existing pipelines
utilize LLMs to process text attributes as node features, then
feed the embeddings produced by LLM into a GNN model forlearning, as shown in Figure 10. However, the simultaneous
training of LLM and GNN poses a significant computational
challenge. To bridge this gap, GLEM [31] suggests training
the GNN and LM separately in a variational Expectation-
Maximization (EM) framework. In the E-step, the LM predicts
both gold labels and pseudo-labels from the GNN, while in the
M-step, the GNN predicts gold labels and LM-inferred pseudo
labels using the embeddings and pseudo-labels provided by the
LM.
Moreover, due to the high cost of annotation and the
necessity for GNN to learn from a substantial amount of
high-quality labeled data to ensure its performance on graph
tasks, leveraging the zero-shot learning capability of LLM
becomes advantageous. Therefore, employing LLM for graph
annotation can enhance GNN training even with limited la-
beled data. LLM-GNN [72] proposes to select a candidate
node set to be annotated. Subsequently, LLMs annotate the
candidate node set, and post-filtering is conducted to eliminate
low-quality annotations. Finally, the GNN is trained using
the high-quality annotation set and utilized for prediction.
LLM-GNN [72] proposes to select a candidate node set for
annotation by LLMs, followed by post-filtering to remove low-
quality annotations. Then, GNN is trained using high-quality
annotations for prediction.
Providing external knowledge/explanations. LLMs pos-
sess a vast knowledge base, enabling them to provide external
knowledge or explanations related to node features when en-
coding them, as shown in Figure 11. The additional knowledge
assists the model in better extracting and capturing node
features. Graph-LLM [73] utilizes LLMs, such as ChatGPT, to
explain text attributes, enhancing them and generating pseudo
labels. These enhanced attributes are then fed into a trainable
LLM, like Llama, to produce node feature embeddings. The
combined pseudo labels and embeddings are input into a GNN,
which delivers the final prediction outcomes.
Similarly, TAPE [30] leverages LLMs to provide external
explanations. In a citation network where each node contains
text attributes like title and abstract, the text attribute of each
node serves as input to an LLM. The LLM categorizes the
nodes and generates multiple predictions ranked in a list with
accompanying reasoning explanations. This approach aims
to extract the LLM’s reasoning capabilities while integrating
external knowledge to aid in understanding node text attributes
and extracting node features.
2)LLMs act as predictors. :When LLMs are predictors,
they are usually directly employed as standalone predictors.
The critical aspect of integrating LLMs as predictors lies
in crafting a well-designed prompt that encompasses text
attributes and graph structures, enabling LLMs to compre-
hend the graph structure effectively and enhance prediction
accuracy. Additionally, there are other methodologies to fine-
tune LLMs, such as utilizing techniques like LoRA [63] and
instruction tuning, aiming to deepen the LLM’s understanding
of the graph structure. Based on whether LLMs undergo
parameter training, they are categorized into prompting LLMs
and SFT LLMs, as shown in Figure 12.Prompting LLMs
Supervised fine-tuning (SFT) LLMs.………………Trainable LLMInstructions: How many C-C-O triangles are in the molecule?Response: There is 1 C-C-O triangle in the molecule.Response: There is no C-C-O triangle in the molecule.Response: There is 4C-C-O triangle in the molecule.Instructiontuning………………Frozen LLMManualprompt:The title of one paper is <Title>and its abstract is <Abstract>. This paper is cited by the following papers: <Titlelist1>. Each of these papers belongs to one category in: <Categories>.You need to analyze the paper’s topic based on the given title and abstract.Fig. 12: LLMs act as predictors. For prompting LLMs, input
designed manual prompts into LLM, enabling it to predict
nodes/links/graphs. For SFT LLMs, input instructions into the
LLM to generate multiple answers. Tuning the LLM is then
based on these multiple responses.
Prompting LLMs. The prompting method can be divided
into two categories. One type is the manual prompts, which
are manually written prompts.
Prompt IV-1: Manual Prompt Template with Slots. For
instance, Beyond Text [74], ENG [75], and Graph Agent
[76] provide a manual prompt template with slots. By filling
these slots with different examples, various prompts can be
constructed. For example:
Prompt IV-1: Manual Prompt Template with Slots
The title of one paper is <Title>and its abstract is
<Abstract >. This paper is cited by the following
papers: <Titlelist1 >. Each of these papers belongs to
one category in: <Categories >. You need to 1.Analyse
the papers’ topic based on the given title and abstract;
2.Analyse the pattern of citation information based on
their titles, and retrieve the citation information you
think is important to help you determine the category of
the first given paper. Now you need to combine the
information from 1 and 2 to predict the category of the
first given paper. You should only output one category.
Compared to manual prompts, LPNL [77] generates
Fig. 13: Examples for Node Classification Task with GPT4 -
Graph Learning Tasks.
prompts through sampling. Specifically, it conducts a two-
stage sampling process on the source node and each candidate
neighbor from the original candidate set to acquire anchor
nodes. Prompt generation is then based on these anchor nodes.
We provide manual prompt examples for various graph
learning tasks in Table III. Additionally, we test LLMs with
GPT 3.5 for node classification and KGQA using manual
prompts, as shown in Figure 13 and Figure 14.
Supervised fine-tuning (SFT) LLMs. IntructGLM [78] and
GraphGPT [79] both employ SFT to train LLM for the node
classification task. IntructGLM [78] utilizes a single LLM by
prompting methods. The prompt includes the description of
node attributes and structure through text descriptions and
corresponding queries. LLMs are then tasked with answer-
ing questions and determining node categories, leading to
fine-tuning through supervised learning. On the other hand,
GraphGPT [79] feeds graph structural information and textFig. 14: Examples for KGQA with GPT3.5 - Graph Learning
Tasks.
PrefixtasksDownstreamtasksUnifiedtasksGNNTrainingonunifiedtasks
TunablepromptPre-trainedGNNTrainingonPrefixtasksDownstreamtasksUnifying+DownstreamtasksTuningpromptfor
Fig. 15: Graph prompt for graph learning.Graph prompt meth-
ods first unify prefix and downstream tasks, then pre-train
GNN on the unified tasks. The pre-trained GNN, when faced
with different downstream tasks, combines with a tunable
prompt through tuning prompts to handle the downstream
tasks better.
into LLM via embedding. Subsequently, two rounds of in-
struction tuning are conducted to refine LLM and effectively
address the node classification task. IntructGLM [78] em-
ploys prompts to input subgraph structures into LLM, while
GraphGPT [79] inputs them into LLM through embedding.
3)Graph prompt :In graph learning tasks, a wide array of
tasks at the node, edge, and graph levels creates a challenge in
achieving compatibility between pre-training and downstream
tasks, potentially leading to negative transfer effects that can
harm the performance of downstream tasks and compromise
the reliability of transfer learning in graph data. Current
methods aim to harmonize pre-training and downstream tasks
to facilitate more effective transfer learning of graph infor-
mation. Despite these efforts, it remains essential to identify
task-specific differences for optimal performance. Inspired by
NLP, researchers have started incorporating prompts in graph
contexts to enable the reuse of pre-trained models across
various downstream tasks without the need for repeated fine-tuning, as shown in Figure 15. The integration of prompts
is crucial in assisting downstream tasks in achieving task-
specific optimal outcomes, bridging the gap between pre-
trained models and the diverse array of graph tasks to enhance
performance and transferability.
GPPT [80] and GraphPrompt [81] aim to unify pre-training
and downstream tasks in graph learning. GPPT transforms
node classification tasks into edge prediction tasks and em-
ploys masked edge prediction for GNN pre-training. Mean-
while, GraphPrompt combines node and graph classification
tasks into a subgraph similarity prediction task and utilizes
graph prompt functions, introducing unified instances and task
templates to enhance performance. Subsequent research, like
All in One [82], further consolidates edge, node, and graph
classification tasks into a single framework using multi-task
prompting approaches, standardizing graph prompts similar to
language prompts and enhancing initialization through meta-
learning techniques for improved reliability and generality
across different tasks in graph data analysis.
C. Comparisons and Discussions
For addressing graph learning tasks, existing methods [30]
[79] [82] categorize based on the role of LLM into three types:
LLMs act as enhancers (LLM-GNN pipelines), LLMs act as
predictors (LLM pipelines), and graph prompts. In the part
of graph prompts, we introduce the prompting engineering
in GNNs without utilizing LLMs. Graph prompts aim to
unify downstream tasks and construct a universal framework.
Therefore, it is compared with LLM-GNN pipelines and LLM
pipelines to provide a comprehensive overview.
When LLMs act as enhancers, the most popular pipeline is
the LLM-GNN pipeline. There are three categories of LLM-
GNN pipelines, depending on how LLM enhances GNN:
encoding the graph into embeddings, generating graph pseudo
labels, and providing external knowledge/explanations. How-
ever, the LLM-GNN pipelines that are currently available are
not end-to-end pipelines, meaning that LLM and GNN cannot
be trained together. LLM and GNN can be trained separately
using frameworks like EM framework [31] or by freezing
LLM and using it as an external knowledge base. Co-training
LLM and GNN can lead to issues like gradient vanishing,
which is a significant obstacle in current LLM-GNN pipelines
due to the large number of parameters in LLM compared
to GNN. To solve this problem, methods like knowledge
distillation can reduce the number of LLM parameters while
retaining the beneficial capabilities for downstream tasks.
When LLMs act as predictors, two main methods are
used: prompting LLMs and SFT LLMs. All approaches for
fine-tuning LLMs can be reviewed in the ”comparisons and
discussions” section of Section III. Currently, SFT and DPO
are popular methods for fine-tuning LLMs.
For graph prompt, the workflow involves unifying pre-
training and downstream tasks, followed by prompt tuning
for different downstream tasks through prompt engineering,
as shown in Figure 15. Graph prompts require fewer tunableGraph ReasoningSortingSort the following list of numbers in ascending order. Output only the sorted list of numbers, no additional text. Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7] Output: [0, 0, 1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9]Find the intersection of two sets of numbers. Output only the set of numbers that are present in both sets, no additional text. Input Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24, 10, 9, 3, 20, 8] Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23]Set Operations
Count the frequency of how many times each country is explicitly named in the input text. You can generate any intermediate lists and states, but the final output should only contain the frequency of each country that appears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the output as in the input text): {{ ”country1”: frequency1, ”country2”: frequency2, ... }}Keyword CountingKeyword: frequencyMerge the following 4 NDA documents -into a single NDA, maximizing retained information and minimizing redundancy. Output only the created NDA between the tags and , without any additional text. Here are NDAs: [four documents] Document MergingMath word problemsJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?Math problems
Multi-hop Question AnsweringQuestion triplets: (’Hypocrite’, directed by, $1), ($1, death date, $2) Question: When did the director of film Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite (Film)? (2) When did Miguel Morayta die? Logic reasoning• Premises: 1.It is not true that some giant language models do not have good performance. 2.All language models with good performance are used by some researchers. 3.If a language model is used by some researchers, it is popular. 4.If BERT is a giant language model, then GPT-3 is also a giant language model. 5.BERT is a giant language model. • Hypothesis: GPT-3 is popular. Give hypothesis label, true or false.51012048195133970011112334557899(a) (b) (c) (d) 
(e) (a) 
(f) (g) Fig. 16: Graph-formed Reasoning Tasks.
Fig. 17: Illustration of human logical derivation. [35]
parameters compared to LLM-GNN and LLM pipelines; how-
ever, they have a shallower semantic understanding of graph
attributes. In LLM pipelines, LLMs need to undergo alignment
tuning before they can be used for various downstream tasks.
In LLM-GNN pipelines, there is a general trend of training
GNNs. Combining LLM-GNN and graph prompts is possible
because graph prompts are designed for GNNs through prompt
engineering and can be applied to LLM-GNN pipelines. By
leveraging LLM’s robust semantic representation capabilities
and the lightweight fine-tuning of graph prompts, similar
results can be achieved.
Classical graph tasks, such as node classification on at-
tributed static networks, have recently obtained the most
attention. However, there is potential for more complex tasks
in the future, such as predicting graph evolution on dynamic
graphs. Leveraging LLM models that are suitable for handling
sequential data and can process time series data, along with
GNNs that are adept at capturing changes in graph structures,
can help address a broader range of problems effectively. By
combining the strengths of LLM and GNN, we can tackle
more challenging tasks in the field of graph analysis.V. G RAPH -FORMED REASONING
A. Tasks Introduction
Graph-formed reasoning refers to combining the graph form
with LLMs to obtain more accurate and reliable answers.
LLMs have strong reasoning capabilities, and many prompting
methods are proposed to enhance LLMs’ reasoning abilities,
addressing algorithmic problems, mathematical issues, etc.,
such as chain of thought, self-consistency, in-context learning,
and more. However, these methods diverge from the patterns
of human thought. The human thought process is typically
non-linear rather than a simple chain of continuous thoughts,
like in Figure 17. Graphs can represent the thinking patterns
of individuals during the thought process. Suppose LLMs
can also use graph-formed reasoning for inference. In that
case, they may be able to solve more complex problems,
such as algorithmic problems, logical reasoning problems,
and mathematical word problems, as shown in Figure 16. In
this section, we present seven graph-formed reasoning tasks
along with their definitions. Next, we introduce graph-formed
reasoning methods involving two types of reasoning: think on
the graph and verify on the graph.
1) Sorting: The problem of sorting involves arranging
certain elements in a specific order. For example, sorting a
list of duplicate numbers from 0 to 9 can be done using a
merge-based sorting algorithm. First, the input sequence of
numbers is divided into subarrays. Then, these subarrays are
sorted individually and merged to form the final solution, as
shown in Figure 16 (a).
2) Set operations: Set operation task mainly focuses on
set intersection. Specifically, the second input set is split into
subsets and the intersection of those subsets with the first input
set is determined with the help of the LLM, as shown in Figure
16 (b).
3) Keyword counting: The keyword counting task aims to
determine the frequency of specific keywords within a given
category in the input text. The input text is divided intoThinkongraphVerifyongraphLLM's intermediate answerLLM’sfinalanswerInput
Deletedintermediate answer
ThinkingprocessInputLLM's intermediate conclusionVerificationVerify whether two conclusions from two paths are the same.VerifyingprocessFig. 18: Graph-formed reasoning. Two directions: think on graphs and verify on graphs. Think on the graph refers to using
the graph structure to derive the final conclusion during the LLMs’ reasoning process. Verify on the graph refers to using the
graph to verify the correctness of the LLMs’ intermediate and final output.
multiple paragraphs, and the keywords are counted in each
paragraph, with the sub-results aggregated, as shown in Figure
16 (e).
4) Document merging: Document merging is the process
of generating a new document based on multiple input docu-
ments that have overlapping content sections. The goal is to
minimize duplication as much as possible while preserving the
maximum amount of information, as shown in Figure 16 (c).
5) Math word problems: Math word problems include
single- and multi-step word problems with addition, multipli-
cation, subtraction, division and other math topics. LLM re-
quires an understanding of text and mathematical relationships
and involves a multi-step reasoning process where calculations
are performed step by step to arrive at an answer ultimately,
as shown in Figure 16 (d).
6) Multi-hop question qnswering: Multi-hop question an-
swering requires LLM to retrieve and integrate information
from multiple text passages or multi-hop graphs to answer
questions. For a complex reasoning question, LLM uses a
sophisticated thinking process to perform reasoning and ul-
timately arrive at the correct answer, as shown in Figure 16
(f).
7) Logic reasoning: Logical reasoning is a process aimed at
concluding rigorously. It occurs in inference or argumentation,
starting from a set of premises and reasoning towards a
conclusion supported by those premises. Propositional logic
is the most fundamental logical system, consisting of p, q, r,
and various operations, as shown in Figure 16 (g).
B. Graph-formed Reasoning Methods
The graph form, with its inherent structural features, not
only mimics human reasoning patterns but also validates
answers from LLM through the relationships between nodes
and local structure. Existing work can roughly be divided
into two categories: think on the graph and verify on the
graph , as shown in Figure 18. Think on the graph refers
to LLM thinking in the form of a graph, where each node
on the graph represents a step in the thinking process oran intermediate conclusion during thinking, and the edges
on the graph indicate the direction of LLM inference or the
relationships between intermediate thinking steps. In this way,
the LLM thinking process can be visually represented in graph
form. Verify on the graph means verifying the consistency and
correctness of answers by utilizing the graph’s structure. For
example, if the end node of different paths is the same, the
results derived from different paths should be the same. If
contradictory conclusions arise, then the obtained conclusion
is incorrect.
1)Think on the graph :The GoT* reasoning method [36]
is proposed with a two-stage framework to enable LLM to
reason on a graph for answering multiple-choice questions.
Initially, the input query is converted into a graph form, and
with the incorporation of graph and multimodal features, LLM
generates rationale. This rationale updates the graph to a graph
with rationales, which is then combined with the original input
and fed into the decoder to obtain the final answer.
However, GoT* allows LLM to enhance the graph us-
ing multimodal information but does not reason step-by-
step deduction in graph form. The Graph of Thought (GoT)
[34] represents LLM’s intermediate thinking as an arbitrary
graph, facilitating powerful prompting for solving algorithmic
problems like sorting and keyword counts. LLM thoughts are
depicted as vertices in this approach, with edges representing
dependencies between them. By continuously adding LLM
responses to the graph, arbitrary thoughts can be aggregated,
forming a directed acyclic graph.
Multiple LLMs can also be collaboratively harnessed to
tackle complex mathematical challenges, extending beyond
the capabilities of a single LLM. Cumulative Reasoning (CR)
[35] is proposed as a more human-like reasoning process. CR
utilizes three LLMs in different roles: the proposer, verifier,
and reporter. The proposer suggests the next step, the verifier
checks the accuracy of the steps, and the reporter decides
when the reasoning process should end. Three roles of LLMs
collaborate to achieve more accurate reasoning processes.Task Prompts
Sorting <Instruction >Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no
additional text. </Instruction ><Examples >like Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7] Output: [0, 0,
1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9] </Examples >Input: [input list]
Set Operations <Instruction >Find the intersection of two sets of numbers. Output only the set of numbers that are present in
both sets, no additional text. </Instruction ><Examples >like Input Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24,
10, 9, 3, 20, 8] Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23] Output: [24, 10, 20, 8]
</Examples >Input Set 1: set1 Input Set 2: set2
Keyword Counting <Instruction >Count the frequency of how many times each country is explicitly named in the input text. You can
generate any intermediate lists and states, but the final output should only contain the frequency of each country that
appears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling
for each country in the output as in the input text): {{”country1”: frequency1, ”country2”: frequency2, ... }}
</Instruction ><Approach >To count the frequency for each country follow these steps: 1. Split the input passage
into four paragraphs of similar length. 2. Count the frequency of each country in each paragraph. 3. Combine the
frequencies of each country from each paragraph by adding them together. </Approach ><Examples >(Omitted)
</Examples >Input: input text
Document Merging Merge the following 4 NDA documents <Doc1>-<Doc4>into a single NDA, maximizing retained information
and minimizing redundancy. Output only the created NDA between the tags <Merged >and</Merged >, without
any additional text. Here are NDAs: [four documents]
Math word problems Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends
every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
in dollars does she make every day at the farmers’ market?
Multi-hop Question Answering Question triplets: (’Hypocrite’, directed by, $1), ($1, death date, $2) Question: When did the director of film
Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite
(Film)? The film Hypocrite was directed by Miguel Morayta. (2) When did Miguel Morayta die? Miguel Morayta
died on 19 June 2013. So the answer is 19 June 2013.
Logic reasoning • Premises: 1. It is not true that some giant language models do not have good performance. 2. All language
models with good performance are used by some researchers. 3. If a language model is used by some researchers,
it is popular. 4. If BERT is a giant language model, then GPT-3 is also a giant language model. 5. BERT is a
giant language model. • Hypothesis: GPT-3 is popular. • Label: [True]
TABLE IV: Prompts for Graph-formed Reasoning.
2)Verify on the graph : Verify on the graph is to validate
the intermediate reasoning results of LLM to enhance its
performance. The Reasoning Graph Verifier (RGV) [83] in this
study assumes a logical connection between the intermediate
steps of different inference paths created by LLM. This allows
the multiple solutions generated by LLM for a reasoning task
to be structured into a reasoning graph, aiming to improve
the accuracy and reliability of the outcomes. By constructing
reasoning graphs from the various solutions provided by LLM,
a verifier is trained to determine the correctness of the resulting
reasoning graph. During the prediction phase, RGV assesses
the solutions and selects the highest-scoring one as the final
answer.
However, this work trains an extra model to determine
whether the graph formed by the solutions generated by LLM
is correct rather than utilizing the knowledge within the graph
and the relationships between the knowledge for validation.
The Graph-guided CoT [84] approach aims to improve the
relevance of rationales generated by CoT during multi-step
reasoning. It starts by extracting triplets from questions using
LLM to build a question graph and generates intermediate sub-
questions from this graph. To ensure the rationale from LLM is
logical, Retrieval Augmented Generation (RAG) is used. In an
open-book scenario, knowledge retrieval is based on the sub-questions, providing retrieved documents and sub-questions
as input to LLMs. LLMs generate rationales for the sub-
questions, creating a rationale graph. Based on the rationale
graph, the study assesses whether the generated rationales
aid in solving the original question. By iteratively generating
intermediate rationales, the solution to the original question
can be determined.
Finally, we provide manual prompt examples for various
graph learning tasks in Table IV. Additionally, we test LLMs
with GPT-4 for sorting and logic reasoning using manual
prompts, as shown in Figure 19.
C. Comparisons and Discussions
Graph-formed reasoning is categorized into think on the
graph andverify on the graph .Think on the graph refers to
using the graph structure to derive the final conclusion during
the reasoning process with LLM. On the other hand, verify
on the graph involves treating the intermediate or final results
generated by LLM as nodes on the graph and using the graph
to determine if there are contradictions between the nodes,
thus verifying the correctness of the LLM output.
For “think on the graph”, a common issue with existing
approaches is their lack of convenience. Compared to CoT
and SC, the reasoning processes in current works are complex,Fig. 19: Examples for Logic Reasoning Task with GPT4 -
Graph Reasoning Tasks.
requiring multiple stages of reasoning and validation. Graph
of thought methods are not plug and play, which contradicts
the original intent of prompts. Even though using more LLMs
can simplify the reasoning and validation process, it raises the
cost and barrier to entry for reasoning. Therefore, the current
challenge is to find a plug-and-play, low-barrier LLM graph
reasoning method that improves LLM reasoning capabilities.
For “verify on the graph”, the current approaches have yet to
utilize the nature of the graph structure for validation. Existing
methods either retrain a model to determine correctness or use
a KG for assessment without using the relationships between
nodes to infer whether the conclusions within each node in
the graph are correct.
Therefore, for the “think on the graph,” the future direction
could focus on developing a plug-and-play, low-barrier LLM
graph reasoning method that enhances LLM reasoning abili-
ties, a pressing issue that needs to be addressed. On the other
hand, concerning the “verify on the graph” method, future
research could explore how to utilize the relationships between
nodes in the graph structure to verify the outputs of LLM or
the reasoning process itself.VI. G RAPH REPRESENTATION
A. Tasks Introduction
LLMs’ powerful text representation abilities empower text
embeddings to capture deeper semantic nuances, which also
can enhance graph representations, particularly for Text At-
tributed Graphs (TAGs). When dealing with structured text
data, the key challenge is integrating graph structures into text
embeddings produced by LLMs to enhance their informative-
ness or enable LLMs to process text embeddings with graph
structures within the text space. Moreover, effectively incor-
porating the graph description within the prompt is essential
for LLMs, especially in closed-source models like ChatGPT,
where the embedding is invisible. How the graph is encoded
within the prompt influences the model’s comprehension of
the graph. Thus, we summarize three types of graph repre-
sentation: graph embedding ,graph-enhanced text embedding ,
andgraph-encoded prompts , as shown in Figure 20. Next, we
introduce graph-formed reasoning methods corresponding to
the above three types.
1) Graph embedding: Graph embedding focuses on trans-
forming a graph into a specific ordered sequence, which is then
fed into an LLM to learn the sequence’s embedding using their
excellent semantic capturing ability and then derive the graph
embedding.
2) Graph-enhanced text embedding: Graph-enhanced text
embedding emphasizes incorporating structural embedding
into text embedding. There are two types of embeddings:
structural embedding, which captures the local structure, and
text embedding, which captures the semantic meaning. How to
combine these two types of embeddings is the core of graph-
enhanced text embedding.
3) Graph-encoded prompts: Graph-encoded prompts con-
centrate on how to describe a graph so that LLMs can
understand it more efficiently and then input it into LLMs.
For instance, in a regular graph, the graph can be placed in a
story context by assuming that the relationships between the
nodes are friends or colleagues.
With the emergence of LLM, much work has been done on
graph representation. Three goals of the graph representation
direction can be identified from the above three categories:
to obtain better graph embeddings as an input into GNNs, to
obtain better text embeddings as an input into LLMs/LMs,
and to get better prompts for graph description as an input
into LLMs.
B. Graph Representation Methods
For the three categories of tasks mentioned above, each
type of task has specific focuses, technical characteristics, and
objectives.
1)Graph embedding :Text data is sequential, while graph
data is structural, posing a challenge for LLMs, which excel at
handling text but struggle with graphs. How do we transform
graphs into sequences? Graph embedding methods use specific
order sequences to represent the graph, where specific order
represents graph structure. WalkLM [38] aims to enhanceGraphrepresentationGraph embeddingGraph-enhanced text embeddingGraph-encoded promptsGraph1432014314013204……SpecificordersequencesLLM/PLMGraphEmbeddingsTextembeddingsTexttokensLLM/PLMTextembeddingswithgraphstructureGraphstructuralembeddings+……
Prompt:(placinggraphGinmultiplecontexts)GdescribesafriendshipgraphamongJames,David,John…Gdescribesaco-authorshipgraphamongJames,David,John…GdescribesasocialnetworkgraphamongJames,David,John………LLMResponse: ……Fig. 20: Graph representation. Three types of graph representation are shown: graph embedding, graph-enhanced text embedding,
and graph-encoded prompts. Graph embedding methods use specific order sequences to represent the graph. Graph-enhanced
text embedding emphasizes incorporating structural embedding into text embedding. Graph-encoded prompts concentrate on
how to describe a graph in prompts.
graph representations in TAGs by utilizing a language model.
Initially, text sequences are generated on the TAG through
the random walk algorithm, capturing structural features and
node proximity. By incorporating text information from nodes
and edges into these sequences based on the graph structure,
the texturing process preserves component attributes. Subse-
quently, these sequences are input into a masked language
model for training, where each token represents a node or
edge, leading to improved graph representations and enhanced
downstream task efficiency. Notably, various masked language
model options, including LLMs, are available.
While WalkLM [38] focuses on superior graph embeddings
for tasks like node classification, GraphText [37] transforms
graphs into the natural language to enable LLMs to process
graph data in the text domain, leveraging LLMs’ generalization
capabilities for graph tasks. GraphText [37] reformulates graph
reasoning as text-to-text problems, establishing text input and
output spaces. GraphText first constructs grammar trees for
graphs, then traverses them to generate graph text sequences,
and finally maps the graph to the text space. The text input is
then fed into an LLM, with the LLM results mapped to the
label space, effectively enabling LLMs to handle graph tasks.
2)Graph-enhanced text embedding :Current work focuses
on simply passing graph structure information to the LLM
through prompts without deeply learning the graph structure,
which can lead to an LLM’s insufficient understanding of
complex structural relationships.
DGTL [39] integrates graph information into text with
LLMs for node classification tasks. It begins by inputting text
into a frozen LLM to create text embeddings from the last
layer. Then, a disentangled graph learning method is employed
to extract various structural details and generate structure
embeddings. These structure embeddings are combined with
the text embeddings and fed back into the frozen LLM for
node classification. The entire process is fine-tuned to optimize
the disentangled graph learning for better results.
While DGTL [39] concentrates on utilizing LLMs to in-
tegrate text and graph structure for graph tasks, G2P2 [85]
emphasizes merging graph structure with text to address textclassification tasks. Textual data commonly exhibit network
structures, such as hyperlinks in citation networks or purchase
networks, which encapsulate meaningful semantic relation-
ships that can enhance text classification performance.
G2P2 [85] is proposed to tackle low-resource text clas-
sification through a dual approach. Three graph interaction-
based contrastive strategies are introduced during pre-training
to jointly pre-train the graph-text model. In the downstream
classification process, efforts are made to facilitate the joint
pre-trained model in achieving low-resource classification.
3)Graph-encoded prompts :The prompting method is
crucial for LLMs to solve tasks. For closed-source LLMs,
the prompt serves as instructions to guide the LLM in under-
standing and solving problems. Therefore, effectively encoding
graphs in the prompt is vital for LLMs to comprehend graph
structure and solve graph tasks. Graph encoding refers to how
graphs are represented in the prompt.
Talk Like A Graph [86] introduces diverse graph encoding
techniques by placing the same graph in multiple contexts.
This strategy highlights how a node, which may lack intrinsic
meaning, can be interpreted differently based on the context;
for instance, a node could represent a person named David,
with edges indicating various relationships like co-authorships
or friendships. When asking LLM the degree of one node, in
the given contexts, that equals how many friendships David
has.
In contrast, Talk Like A Graph [86] primarily emphasizes
text modality graph encoding, while Which Modality Should I
Use [87] employs three encoding modalities - text, image, and
motif - to encode graphs. The latter method utilizes different
prompt techniques to evaluate the overall connectivity of a
graph, enabling LLMs to handle intricate graph structures
more effectively. Specifically, the text modality encoding pro-
vides insights into subgraphs and their connections at a local
level, while the motif modality encoding captures essential
graph patterns like stars, triangles, and cliques, offering a bal-
anced perspective on local and global information. Moreover,
the image modality encoding delivers a broader view of nodes
with limited labels, effectively utilizing the input context.Query: What other works does the director who directed Inception have?
KGsLLMs1. "The Dark Knight Trilogy" 2. "Interstellar"3. "Dunkirk"4. "Memento"5. "The Prestige"6. "Insomnia"Incomplete answerQuery: What other works does the director who directed Inception have?LLMs1. "The Dark Knight Trilogy" 2. "Interstellar"3. "Dunkirk"4. "Memento"5. "The Prestige"6. "Insomnia"7. “Oppenheimer"Complete answer+KGs enhanced LLMsFig. 21: KG-based augmented retrieval. Knowledge graphs can
enhance LLMs to provide more comprehensive answers.
In comparing these two methods, Talk Like A Graph [86]
focuses on diverse graph encoding within text modality by
constructing contexts, whereas Which Modality Should I Use
[87] utilizes multiple modalities to encode graphs compre-
hensively, enhancing the LLMs’ ability to understand graph
structures.
C. Comparisons and Discussions
Graph embedding focuses on transforming a graph into a
specific ordered sequence, which is then fed into an LLM
to learn the sequence’s embedding and derive the graph em-
bedding. On the other hand, graph-enhanced text embedding
emphasizes incorporating structural embedding into text em-
bedding. Lastly, graph-encoded prompts concentrate on how
to describe a graph and input it into an LLM.
However, due to LLMs’ powerful text representation capa-
bilities, the first two methods exhibit a deep semantic under-
standing of graph attributes. However, they still need suitable
structural information capturing, which remains rudimentary
and inadequate. Additionally, aligning the graph structure
features with text features to better represent the graph’s
features is a current issue that needs to be addressed.
For graph-encoded prompts, most methods build a narrative
context for the graph or describe it multimodally before feed-
ing it into an LLM. Both methods enable the LLM to interpret
the graph from various perspectives to improve performance.
The critical challenge currently lies in designing diverse and
easily understandable graph descriptions for LLMs, convey-
ing essential graph descriptions while enhancing the LLM’s
comprehension of the input description.
VII. K NOWLEDGE GRAPH BASED AUGMENTED
RETRIEVAL
LLMs have shown remarkable reasoning capabilities in
challenging tasks, sparking debates on the potential replace-
ment of Knowledge Graphs (KGs) in triplet form (subject,
predicate, object) by LLMs. Recent LLMs are seen as viable
alternatives to structured knowledge repositories such as KGs,indicating a shift towards utilizing LLMs for processing real-
world factual knowledge [88] [89].
A. LLMs limitations and comparison with KGs
LLMs, while powerful, face several significant challenges:
•Hallucination is a common issue for LLMs due to a
lack of domain-specific knowledge and knowledge ob-
solescence, leading to incorrect reasoning and reduced
credibility in critical scenarios like medical diagnosis and
legal judgments [88] [90] [43]. Although some LLMs can
explain predictions through causal chains, they struggle
to address hallucination effectively. Integrating external
KGs can help mitigate these problems [41].
•Insufficient domain knowledge hampers LLM perfor-
mance in specific areas, including private datasets, ne-
cessitating the integration of domain-specific knowledge
graphs to enhance their ability to answer domain-specific
questions [40].
•LLMs struggle with recalling facts when generating
knowledge-based content, despite excelling in learning
language patterns and conversing with humans [89].
•LLMs have limitations in accurately capturing and re-
trieving foundational knowledge, hindering their ability
to access factual information effectively [42].
In contrast, KGs like Wikipedia and DBpedia are structured
repositories of rich factual knowledge, providing a more
explicit and reliable source of information compared to the
black-box nature of LLMs, as shown in Figure 21. How do
we measure the shortcomings of LLM relative to KG? KGLens
is proposed as an effective method to evaluate the factual
accuracy and identify knowledge gaps in LLMs by assessing
the alignment between a KG and LLM [91].
B. Solutions to LLMs limitations
To address the limitations of LLMs, such as hallucination,
insufficient domain knowledge, etc., integrating LLMs with
KGs is a potential way to allow LLMs to learn knowledge
from KGs and enhance their capabilities. The REASONING
ON GRAPHS (RoG) framework [43] synergizes LLMs with
KGs for faithful and interpretable reasoning. Specifically,
RoG utilizes a planning retrieval-reasoning framework where
relation paths grounded by KGs are generated as faithful plans.
These plans are then used to retrieve valid reasoning paths
from KGs to facilitate LLMs’ faithful reasoning. Existing work
has taken on the challenges posed by the four main limitations
of LLMs through distinct perspectives, each offering unique
solutions.
Addressing the first limitation concerning hallucination is-
sues in LLMs, the Head to Tail benchmark [88] is introduced
to assess LLMs’ reliability in answering factual questions
and to evaluate the probability of hallucination in generating
KG triples. Additionally, it explores whether factors like
model size or instruction tuning can enhance LLM knowledge.
Think-on-Graph (ToG) [41] partially addresses hallucination
by involving the LLM agent in iteratively searching KGs,
identifying promising reasoning paths, and providing likelyreasoning outcomes. The second limitation is LLM needs
domain-specific knowledge. To tackle this, GLaM [40] is
developed to convert knowledge graphs into text paired with
labeled questions and answers, allowing LLMs to acquire and
respond to domain-specific knowledge. Regarding the third
limitation related to LLMs forgetting facts, integrating KGs
with PLMs (KGPLMs) [89] is introduced to enhance the
model’s ability to recall facts compared to standalone LLMs.
This approach emphasizes the competitive and complementary
relationship between LLMs and KGs, where LLMs improve
knowledge extraction accuracy, and KGs guide LLM training
to enhance memory and knowledge application capabilities.
Finally, the fourth limitation pertains to LLMs’ challenge
in accurately retrieving and returning knowledge from KGs.
KGs can enhance LLM performance by incorporating them
during pre-training and inference stages or to deepen LLM’s
understanding of acquired knowledge. Graph Neural Prompt-
ing (GNP) [42] is proposed to augment pre-trained LLMs
using foundational knowledge, such as retrieval-augmented
generation, to facilitate effective learning from KGs. GNP [42]
retrieves and encodes relevant, grounded knowledge to gener-
ate Graph Neural Prompts, embedding vectors that provide
guidance and instructions for LLMs.
C. Other KG + LLMs works
1)KG tasks with LLMs :Moreover, LLMs can enhance
KGs to tackle a broader array of challenges. By leverag-
ing LLMs, KGs can be fortified to perform various KG-
related tasks such as embedding, completion, construction,
text generation from graphs, and question answering [90].
An illustrative example is how LLMs can support KG tasks
such as knowledge graph alignment. In entity alignment tasks
between different knowledge graphs, the objective is to iden-
tify pairs of entities representing the same entity. To address
this, AutoAlign [92] facilitates alignment without the need for
expensive manual seed creation. Specifically, AutoAlign [92]
automatically identifies similarities between predicates across
different KGs with the assistance of LLMs.
2)Applications of KGs + LLMs :The combination of KGs
and LLMs has other applications as well. For instance, it can
address tasks like multi-document question answering. Knowl-
edge Graph Prompting (KGP) [93] is introduced to design
appropriate context by building and exploring a knowledge
graph. Subsequently, this context guides LLMs for answering
multi-document questions.
D. Summary
In conjunction with LLMs, the future directions for KGs fo-
cus on overcoming challenges and seizing opportunities in this
evolving field. Firstly, leveraging KGs for Hallucination Detec-
tion in LLMs aims to address the issue of generating inaccurate
content. Secondly, utilizing KGs for Editing Knowledge in
LLMs will enable the swift adaptation of internal knowledge
to real-world changes. Moreover, the challenge of injecting
knowledge into Black-box LLMs due to restricted access to
internal structures necessitates innovative approaches. Lastly,
OccupationsLLM tuningBehavior GraphFig. 22: Graph-LLM-based applications - Recommendation
systems. This shows LLM for graph data understanding in
online job recommendations [46].
integrating Multi-Modal LLMs with KGs can enrich handling
diverse data types within knowledge graphs [90].
VIII. G RAPH -LLM- BASED APPLICATIONS
Graph-LLM-based applications refer to frameworks that
integrate graphs with LLMs. Apart from their applications
in graph-related tasks, they are also utilized in various other
domains (as shown in Figure ??), such as conversational
understanding and recommendation systems, as shown in
Figure 22. Common frameworks involve combining GNNs
with LLMs, merging graph data with LLMs, and exploring
additional innovative approaches that leverage the advantages
between graph structures and language models for diverse
applications.
1) Conversational understanding: By combining LLM
with graph traversal, collaborative query rewriting [94] is
proposed to improve the coverage of unseen interactions,
addressing the flawed queries users pose in dialogue systems.
Flawed queries often arise due to ambiguities or inaccura-
cies in automatic speech recognition and natural language
understanding. When integrated with graph traversal, LLM
can effectively navigate through the graph structure to retrieve
relevant information and provide more accurate responses.
2) Response forecasting: LLM can effectively handle social
networks and extract latent personas from users’ profiles and
historical posts. SOCIALSENSE [95] is proposed to utilize
LLMs to extract information to predict the reactions of news
media. By analyzing individuals’ characteristics and behavior
patterns within social networks, LLM can effectively predict
the impact of news releases and prevent unintended adverse
outcomes.
3) Multi-domain dialogue state tracking: LLM can learn
from multi-domain dialogue history, query, and graph prompts,
enabling it to track dialogue states and generate dialogue
content, like SHEGO [96]. By incorporating information from
various sources, such as previous dialogue exchanges, user
queries, and relevant graph prompts, LLM can understand the
conversation’s context and dynamics, allowing LLM to track
the current dialogue state effectively and generate appropriate
responses or dialogue content based on the inputs.
4) Recommendation systems: LLMs can also help address
issues in recommendation systems [46], as many tasks in
recommendation systems require learning graph structures,
such as user-item interaction networks. LLMRec [44] aimsto enhance recommendation systems by tackling data sparsity
by adopting three simple yet effective LLM-based graph-
enhancement strategies.
5) Graph neural architecture search: LLMs can help ad-
dress Graph Neural Architecture Search (GNAS). GNAS re-
quires intensive human effort and rich domain knowledge
to design search spaces and strategies. Leveraging powerful
knowledge and reasoning capabilities, LLMs can identify
suitable GNN frameworks within the search space of graph
neural network frameworks. GPT4GNAS [45] integrates GPT-
4 into GNAS, introducing a new set of prompts for GPT-4 to
guide it towards generating graph neural structures.
IX. B ENCHMARK DATASETS AND EVALUATIONS
In this section, we summarize benchmark datasets and
evaluation metrics for LLMs.
A. Datasets
This paper summarizes the popular and new datasets, the
LLM employed, the performed tasks, and the links to the open-
source code in the LLM-GGA area, as illustrated in Table V.
Below, we introduce commonly used benchmarks and the new
benchmarks proposed for the LLM-GGA field.
1) Popular datasets: Popular benchmark refers to a graph
benchmark that is widely and frequently used. We have sys-
tematically categorized these popular benchmarks according to
six directions, detailing which benchmarks are used for each
direction. Below are listed popular benchmarks commonly
used in the six directions.
•Graph structure understanding : ogbn-arxiv [56],
ogbn-products [56], Cora [100], CiteSeer [101],
Aminer(DBLP) [57], MetaQA [102], Wikidata5M [103],
PROTEINS [104], MUTAG [105], NCI1 [106], PTC
[107], Foursqure [108].
•Graph learning : ogbn-arxiv [56], ogbn-products [56],
ogb-papers110M [56], ogb-citation2 [56], Cora [100],
CiteSeer [101], Amazon-items [109], PubMed [110],
Reddit [111], CoraFull [112], Amazon [113], PROTEINS
[104], COX2 [114], BZR [114], OAG [115]
•Graph-formed reasoning : GSM8K [116], SV AMP
[117], FOLIO [118]
•Graph representation : Cora [100], CiteSeer [101],
Goodreads-books [119], PubMed [110], Amazon [113],
MIMIIC-III [120], Freebase [121], FB15K-237 [122]
•KG-based augmented retrieval : CWQ [123], WebQSP
[124], Wikidata [103]
•Graph-LLM-based applications : depending on specific
applications.
2) New datasets: More than existing datasets are needed to
explore LLMs’ ability to understand graph structures and their
potential to solve graph problems better. As a result, many
works have proposed new benchmarks to advance research in
this field, as shown in Table VI.
•GPR [59] contains 37 particular connected graph in-
stances generated by the Networkx toolkit, which include
the “bull graph,” “wheel graph,” “lollipop graph,” etc.These generated graph instances are relatively small, with
about 15 nodes and 28 links on average.
•GraphTMI [87] is a graph benchmark featuring a hi-
erarchy of graphs, associated prompts, and encoding
modalities. Different graph task difficulty depends on the
dual criteria of 1) count of motifs and 2) homophily in
the graph, which yields a dataset of EASY , MEDIUM,
and HARD graph problems.
•LLM4DyG [25] benchmark is to evaluate whether LLMs
are capable of understanding spatial-temporal information
on the dynamic graph. Nine dynamic graph tasks are
designed to assess LLMs’ abilities considering spatial and
temporal dimensions.
•GraphQA [86] comprises a set of diverse fundamental
graph problems with more varied and realistic graph
structures compared to previous studies in LLM research.
GraphQA is designed to measure the performance of
LLMs in graph data reasoning.
•NLGraph [27] benchmark is to examine whether lan-
guage models can reason with graphs and structures.
NLGraph contains eight graph structure understanding
tasks with varying algorithmic difficulties. Depending
on different network sizes, graph sparsity, and more,
NLGraph results in easy, medium, and hard subsets in
each graph reasoning task to enable difficulty scaling and
fine-grained analysis.
•GraphextQA [98] benchmark is a dataset for open do-
main question answering. It includes paired subgraphs
used to develop and evaluate graph language models.
The subgraphs are retrieved from Wikidata and contain
reasoning paths from entities mentioned in the questions
to the entities that the questions are asking about.
•CS-TAG [99] benchmark is a comprehensive and wide-
ranging compilation of benchmark datasets for TAGs.
This dataset encompasses a variety of challenging scenar-
ios, ranging from citation networks to purchase graphs.
The collection consists of eight distinct TAGs sourced
from diverse domains.
We also list which directions these new benchmarks are typ-
ically used for. For graph structure understanding, GPR [59],
GraphTMI [87], LLM4DyG [25], NLGraph [27], and CS-TAG
[99]can be used. For graph learning, CS-TAG [99] can be used.
For graph-formed reasoning, GraphextQA [98] can be used.
For graph representation, GraphTMI [87], GraphQA [86], and
CS-TAG [99] can be used. For KG-based augmented retrieval,
GraphextQA [98] can be used.
B. Evaluations
Evaluating the results of different tasks related to LLM-
GGA is also a critical issue. Thus, selecting evaluation metrics
to assess the results is essential to determining how well LLMs
perform their understanding of graphs and how effectively
models combining graphs and LLMs perform on various tasks
is vital. This section summarizes the metrics of different tasks,
shown as Table VII. Note that all test results related to LLMsTABLE V: A summary of LLM-GGA methods with datasets and source links.
Method Dataset LLM Task Link
InstrucGLM [78] ogbn-arxiv, Cora, PubMed Flan-T5 (instruction-finetune), Llama-
v1-7b (LoRA)Link, Node code link
GPT4Graph [23] ogbn-arxiv,Aminer,Wiki,MetaQA InstructGPT-3(frozen) Reasoning, Node, Graph code link
LLMtoGraph [24] generated by GPTs GPT-3.5-turbo, GPT-4, Wizard-Vicuna-
13B, 30B-Lazarus-Uncensored-HFMulti-hop Reasoning code link
Graph-LLM [73] ogbn-arxiv, Cora, PubMed, ogbn-products LLaMA, text-ada-embedding-002,
Palm-Cortex-001Node code link
TAPE [30] ogbn-arxiv, Cora, PubMed, ogbn-products GPT-3.5 Node code link
LLM4DyG [25] LLM4DyG GPT-3.5-turbo, Vicuna-7B, Vicuna-13B,
Llama-2-13B, CodeLlama-2-13BGraph -
GraphGPT [79] ogbn-arxiv, Cora, PubMed vicuna-7B-v1.1, vicuna-7B-v1.5 Node code link
GPPT [80] Cora, Reddit, CoraFull, Amazon-CoBuy,
ogbn-arxiv etc.- Link, Node code link
GraphPrompt [81] Flickr, PROTEINS, COX2, ENZYMES, BZR - Link, Node, Graph code link
All in one [82] Cora, CiteSeer, Reddit, Amazon, Pubmed - Link, Edge, Node, Graph code link
Graph-ToolFormer [59] GPR, Cora, Pubmed, Citeseer, PROTEINS,
MUTAG, NCI1, PTC, Twitter, FoursquareGPT-J-6B Q&A, Reasoning code link
RGV [83] GSM8K, SV AMP, ASDiv-a GPT-3.5-turbo math problems -
LLM-GNN [72] CORA, CITESEER, PUBMED, WIKICS,
OGBN-ARXIV , OGBN-PRODUCTSGPT-3.5-turbo Node code link
Which Modality should I use [87] Cora, Citeseer, Pubmed,GraphTMI GPT-4, GPT-4V Representation, Node -
WalkLM [38] PubMed, MIMIC-III, Freebase, FB15K-237 PLMs Representation, Node, Link code link
GraphText [37] Cora, Citeseer, Texas, Wisconsin, Cornell Llama-2-7B Node code link
TALK LIKE A GRAPH [86] GraphQA PaLM 2-XXS, PaLM 62B Node, Link -
Graph-guided CoT [84] 2WikiMultihopQA, MusiQue, Bamboogle Llama-2-13B,Llama-2-70B multi-hop question answering -
NLGraph [27] NLGraph TEXT-DA VINCI-003, GPT-3.5-
TURBO, CODE-DA VINCI-002, GPT-4Link,Node,Graph,Path,Pattern code link
Collaborative Query Rewriting [94] opportunity test sets, guardrail test set Dolly V2 Conversational Understanding -
WHEN AND WHY [26] OGBN-ARXIV , CORA, PUBMED, OGBN-
PRODUCT, ARXIV-2023ChatGPT Node code link
CR [35] FOLIO, LogiQA, ProofWriter, LogicalDe-
ductionGPT-3.5-turbo, GPT-4, LLaMA-13B,
LLaMA-65BLogic reasoning code link
SOCIALSENSE [95] RFPN, Twitter PLMs Response Forecasting code link
DGTL [39] Cora, PubMed, Books-History Llama-2-13B Node -
SHEGO [96] SGD, MultiWOZ 2.1 T5-small multi-domain DST -
Graph of Thought(GoT) [34] individual data GPT3.5(frozen) Graph-formed reasoning code link
GLEM [31] ogbnarxiv, ogbn-products, ogbn-papers100M PLMs Node code link
LPNL [77] OAG T5-base Link -
SIMTEG [71] OGBN-Arxiv, OGBN-Products, OGBL-
Citation2PLMs Node, link code link
Llmrec [44] Netflix, MovieLens gpt-3.5-turbo-16k Recommendation code link
ENG [75] OGB gpt-3.5-turbo Node generation -
OFA [32] OGBN-ARXIV , CORA PLMs Node, link, graph code link
G-prompt [33] OGBN-ARXIV , Instagram, Reddit PLMs Representation -
Beyond Text [74] OGBN-ARXIV , CORA, PubMed GPT-3.5, GPT-4 Node, link -
GPT4GNAS [45] OGBN-ARXIV , CORA, PubMed, Citeseer GPT-4 Graph neural architecture search -
Graphllm [64] NLGraph Llama2-7B, Llama2-13B Link, node, graph, path, pattern code link
G2P2 [85] Cora, Amazon PLMs Representation code link
ChatGraph [97] Gradio GPT-4V , Next-GPT Link, node, graph, application -
Graph Agent [76] Cora, PubMed GPT-4 Link, node, graph -
GoT* [36] AQUA-RAT, ScienceQA T5-base Graph-formed reasoning code link
KGP [93] HotpotQA, IIRC, 2WikiMQA, MuSiQue,
PDFTriage, RankLlama KG+LLM code link
Head-to-Tail [88] DBpredia, Movie, Book, Academics GPT-4 KG+LLM -
GLaM [40] DBLP, UMLS Llama-7B KG+LLM -
ToG [41] CWQ, WebQSP, GrailQA, QALD10-en, etc. GPT-3.5, GPT-4, Llama-2 KG+LLM code link
Autoalign [92] DBpedia, Wikidata ChatGPT, Claude KG+LLM code link
GNP [42] ConceptNet, UMLS, OpenBookQA, etc. FLAN-T5 xlarge (3B), xxlarge (11B) KG+LLM code link
RoG [43] WebQSP, CWQ, Freebase LLaMA2-7B KG+LLM code link
KGLens [91] Wikidata GPT-3.5-turbo, GPT-4, Babbage-002,
Davinci-002, Vicuna-33b-v1.3, Xwin-
LM-13B-V0.2, Yi-34B-ChatKG+LLM -TABLE VI: A summary of new datasets.
New Benchmark Link
GPR [59] https://github.com/jwzhanggy/Graph Toolformer/tree/main/data
GraphTMI [87] To be released
LLM4DyG [25] To be released
GraphQA [86] To be released
NLGraph [27] https://github.com/Arthur-Heng/NLGraph/tree/main/NLGraph
GraphextQA [98] https://huggingface.co/datasets/drt/graphext-qa
CS-TAG [99] https://github.com/sktsherlock/TAG-Benchmark
TABLE VII: Evaluations.
Tasks Metrics
Graph structure understanding task Accuracy, ROUGE, BLEU, Time cost, Comprehension, Correctness, Fidelity, Rectification
Comprehension
Graph learning task Accuracy, Macro-F1, Training Time, Tuned Parameters, GPU Occupy, Mismatch Rate,
Denial Rate, Token Limit Fraction
Graph resoning task Accuracy, F1-score, Precision, Recall, The Latency-V olume Trade-off, Number of errors
and cost
Graph representation depending on downstream tasks
KG-based augmented retrieval Accuracy, F1-score, Precision, Recall,
Graph-LLM-based applications depending on different tasks
in this paper are conducted using GPT-3.5 turbo or GPT-4
turbo.
1) Graph structure understanding task.: Several metrics are
usually used in graph structure understanding tasks: accuracy,
ROUGE [125], BLEU [126], Time cost, comprehension, cor-
rectness, fidelity, and rectification comprehension. Accuracy,
ROUGE, BLEU, and time cost are viral metrics. Meanwhile,
comprehension, correctness, fidelity, and rectification compre-
hension are new metrics [24] used to evaluate the ability
of LLMs to understand graphs through natural language,
the accuracy of solving graph problems, and the level of
confidence in the answers provided.
2) Graph learning task.: For graph learning tasks, when
evaluating a model, various metrics are considered to deter-
mine its effectiveness, efficiency, and computational demands.
When assessing the effectiveness of a model, metrics such
as accuracy, macro-F1, mismatch rate, and denial rate [87]
are considered. In terms of efficiency, metrics like training
time and tuned parameters are assessed. For computational
costs, metrics such as GPU occupancy and token limit fraction
are examined. Notably, the token limit fraction indicates the
proportion of tokens used compared to the maximum allowed
by the model’s constraints and can be formed as follows:
T =Number of usage tokens
Token limit constraint for the model(6)
3) Graph reasoning task.: When it comes to graph reason-
ing tasks, two main factors that are taken into consideration
are effectiveness and efficiency. Several metrics are used to
assess effectiveness, including accuracy, number of errors and
cost, F1-score, precision, and recall [127]. On the other hand,
efficiency is evaluated through metrics such as the Latency-
V olume Trade-off.
4) Graph representation.: The effectiveness of graph rep-
resentation is typically judged based on the performance of
downstream tasks that use this graph representation.5) Knowledge graph-based augmented retrieval.: Tasks in
the KG-based augmented retrieval direction typically involve
question-answering tasks. Evaluation metrics commonly used
include accuracy, precision, recall, F1-score, Hits@k [128],
EM [129], MSE, and for some generative tasks, human eval-
uation may also be utilized.
X. F UTURE DIRECTIONS
The above survey of the state-of-the-art LLM-GGA research
reveals a promising and young research field. The following
section discusses exciting directions for future work.
A. More Complex Graph Problems
More complex graph tasks. Can LLMs solve graph al-
gorithm problems? Existing works on traditional graph tasks
are based on fundamental graph problems such as shortest
path, clustering coefficient computing, maximum flow, etc.
However, can LLMs address NP problems such as community
search, interactive graph problems, or even NP-hard problems,
and if so, how can they tackle them? For graph learning tasks,
current research primarily focuses on simple node, edge, and
graph classification. Future work can focus on more complex
graph learning problems, such as the diverse classification
outcomes arising from isomorphic and heterogeneous graphs.
More complex graph patterns. Graphs contain various
graph patterns, each with its explicit definition and unique
characteristics, such as stars, triangles, cliques, butterflies, and
more. Therefore, recognizing graph patterns and utilizing their
characteristics to solve downstream tasks can be highly advan-
tageous. Currently, only limited works leverage the properties
of stars, triangles, and cliques to solve problems.
Furthermore, understanding graph data still remains a sig-
nificant challenge for existing LLMs, limiting their ability to
tackle more complex graph problems. Therefore, incorporating
LLMs into the process is a promising direction for solving
more complex graph problems.B. LLM Exploration on Diverse Graphs
Most existing work mainly focuses on static graphs, while
there exists a wide range of different graphs, including
undirected, directed, cyclic, acyclic, isomorphic, heteroge-
neous, dynamic, etc. Different types of graphs have significant
structural differences, such as static graphs, dynamic graphs,
temporal graphs, uncertain graphs, heterogeneous graphs, etc.
Specifically, unlike static graphs, dynamic graphs can be
represented as ordered lists or asynchronous streams of timed
events, capturing patterns of temporal network evolution, such
as the addition or removal of nodes and edges. Evaluating the
ability of LLMs to understand the spatio-temporal information
of dynamic graphs is crucial for web applications. Evaluating
whether LLMs can determine when nodes are connected,
identify which nodes are connected to a given node at a
specific time, and find a chronological path by combining
temporal and spatial information is essential to assessing
LLMs’ understanding of dynamic graphs. Future work can
further explore other types of graphs, such as dynamic graphs
and temporal graphs, address problems like maximum flow,
and predict the evolution of graphs.
Moreover, existing studies have conflicting views on the
LLM graph reasoning ability, with some presenting contradic-
tory findings. This ambiguity could be due to various factors,
including dataset selection, diverse prompt engineering tech-
niques, the range of graph reasoning tasks, and the utilization
of different LLM models.
C. Better LLM-GNN Pipelines
GNNs are designed to handle structural information by
continuously learning information from surrounding subgraphs
through aggregation functions. On the other hand, LLMs excel
in processing textual information, text reasoning, semantic
understanding, and more. The challenge lies in leveraging both
advantages to enable a pipeline that can effectively handle both
attributed and pure graphs. If GNNs and LLMs are simply
stacked, the parameter size of GNNs is notably smaller than
that of LLMs. This discrepancy may result in the issue of
vanishing gradients during training, as mentioned in [130],
which can impede the iterative updating process of GNNs.
Additionally, GNNs need to utilize the extensive knowledge
contained within LLMs fully, and they cannot effectively
extract specific knowledge tailored for specific downstream
tasks in different graphs.
D. Graph Foundation Model
LLM is undoubtedly the foundational model in NLP. Can
we draw inspiration from LLMs to train a graph foundation
model? For example, can training strategies like instruction
tuning and DPO be applied to tasks involving graphs? The
current research has primarily introduced graph foundation
models in the form of LLM-GNN pipelines and graph-aware
tuning LLMs. Future endeavors can focus on exploring graph
foundation models better suited for tasks involving graphs.E. Better Graph Prompts
Most graph prompts are currently designed based on GNNs,
with only a few works focusing on LLMs. Graph prompts for
LLMs have yet to be sufficiently explored.
Graph Prompt for GNNs. The typical approach uses
simple concatenation, addition, or dot product operations with
trainable parameters. Some existing works have considered
more complex fusion methods, such as [82], which assumes
the structural features of graph prompts. However, compared
to the combination of prompts and pretexts, the variety of
graph prompts and pre-graphs is still in the exploratory stage.
Graph-enhanced Prompts for LLMs. Relying solely on
manual prompts and self-prompting has limited capabilities
in improving model performance, as they only explore the
existing abilities of LLM. As shown in Section III-C, LLMs
can be trained as agents to utilize tools for graph tasks that are
hard to solve, like API call prompt [59]. GoT [130] is also
a graph reasoning paradigm that enables LLMs to provide
correct answers. Future work based on the graph reasoning
paradigm can consider cost-effective approaches for GoT,
such as pruning and tricks to reduce algorithm complexity.
In the future, it would be beneficial to explore simpler GoT
paradigms that can improve the effectiveness of LLMs.
F . Modal Alignment
Modal alignment refers to the alignment between two
modalities: text and graph. The input for LLMs is typically
sequential data, often text. Graph and text are two different
modalities, and studying the alignment between these two
modalities for LLMs involves finding a shared mapping feature
space for graphs and text. The shared mapping space allows
LLMs to understand graph data similarly to how they know
textual information if they comprehend text.
G. Explainabilily
GNNs are currently widely used for solving complex graph
problems. However, they need more interpretability, which
hinders their practical application. On the other hand, LLMs
possess reasoning capabilities and have succeeded in various
natural language processing tasks. The combination of LLMs
and GNNs has the potential to offer a more transparent ap-
proach to solving graph problems by leveraging the reasoning
abilities of LLMs. If the combination of LLMs and GNNs
is interpretable, it can be utilized for various tasks., including
recommendation systems, drug discovery, and fraud detection.
This combination can lead to the development of more reliable
and efficient decision-making systems across various domains.
H. Efficiency on Large-scale Graphs
Due to the limited input length of LLM, the graph sizes
inputted through prompts typically consist of dozens of nodes.
However, for large graphs with tens of thousands of nodes and
edges, how can LLMs with limited input length solve such
large graphs? A larger input window is required in the case of
attributed graphs, where both node and edge attributes need to
be considered along with the graph structure. How does LLMaddress this case? There are currently few effective methods
to enable LLM to handle them.
XI. C ONCLUSIONS
LLM-GGA has emerged as a promising field that has
garnered significant attention from researchers. This paper
introduces a comprehensive structural taxonomy based on
recent research, which classifies LLM-GGA research into three
main directions: LLM-GQP, LLM-GIL, and graph-LLM-based
applications. LLM-GQP encompasses graph understanding
and KG-based augmented retrieval, while LLM-GIL involves
graph learning, graph-formed reasoning, and graph represen-
tation. The motivation, challenges, and mainstream methods
of each direction are thoroughly examined.
For the six mentioned directions, a comparison of various
methods was conducted to explore their potential in each
area. It is observed that LLM shows preliminary capabilities
in structural understanding, addressing issues like maximum
flow and bipartite graph matching over small graphs. However,
it is susceptible to factors such as node degree and graph
density, leading to potential misjudgments in graph connec-
tivity. Additionally, LLM proves beneficial for graph learning
tasks due to its strong semantic understanding and reasoning
abilities, coupled with learning from extensive corpora, which
can provide external knowledge to GNNs and aid in semantic
information comprehension, learning, and reasoning. Thanks
to LLM’s semantic understanding capabilities, graph represen-
tation can achieve deeper semantic embeddings. The discus-
sion also delves into KG-based augmented retrieval to enhance
LLMs retrieval and factual knowledge-answering abilities. The
paper summarizes over 40 datasets, evaluation metrics for six
directions, and source code for over 30 mainstream methods in
these directions. It highlights the existing challenges in current
methods and proposes future directions to guide and motivate
further research in the LLM-GGA field.
REFERENCES
[1] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester,
N. Du, A. M. Dai, and Q. V . Le, “Finetuned Language Models
Are Zero-Shot Learners,” Feb. 2022, arXiv:2109.01652 [cs]. [Online].
Available: http://arxiv.org/abs/2109.01652
[2] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction Tuning
with GPT-4,” Apr. 2023, arXiv:2304.03277 [cs]. [Online]. Available:
http://arxiv.org/abs/2304.03277
[3] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,
and C. Finn, “Direct preference optimization: Your language
model is secretly a reward model,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
[4] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, Y . Li, C. Gao, Y . Huang,
W. Lyu, Y . Zhang, X. Li, Z. Liu, Y . Liu, Y . Wang, Z. Zhang, B. Vidgen,
B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. Xing, F. Huang, H. Liu,
H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang,
M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang,
J. Wang, J. Vanschoren, J. Mitchell, K. Shu, K. Xu, K.-W. Chang,
L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y . Chen, Q. Gu,
R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang,
X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y . Liu,Y . Ye, Y . Cao, Y . Chen, and Y . Zhao, “TrustLLM: Trustworthiness in
Large Language Models,” Mar. 2024, arXiv:2401.05561 [cs]. [Online].
Available: http://arxiv.org/abs/2401.05561
[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,
C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa,
I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra,
I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,
A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,
B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,
Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,
S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned
chat models,” CoRR , vol. abs/2307.09288, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2307.09288
[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,
P. F. Christiano, J. Leike, and R. Lowe, “Training language
models to follow instructions with human feedback,” in Advances in
Neural Information Processing Systems 35: Annual Conference on
Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022 , S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds.,
2022. [Online]. Available: http://papers.nips.cc/paper files/paper/2022/
hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html
[7] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa:
A dataset for LLM question answering with external tools,” in
Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023 , A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
Eds., 2023. [Online]. Available: http://papers.nips.cc/paper files/paper/
2023/hash/9cb2a7495900f8b602cb10159246a016-Abstract-Datasets
and Benchmarks.html
[8] Z. Li, S. Fan, Y . Gu, X. Li, Z. Duan, B. Dong, N. Liu, and J. Wang,
“Flexkbqa: A flexible llm-powered framework for few-shot knowledge
base question answering,” in Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 38, no. 17, 2024, pp. 18 608–18 616.
[9] B. Zhang, B. Haddow, and A. Birch, “Prompting large language model
for machine translation: A case study,” in International Conference on
Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
USA, ser. Proceedings of Machine Learning Research, A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202. PMLR, 2023, pp. 41 092–41 110. [Online]. Available:
https://proceedings.mlr.press/v202/zhang23m.html
[10] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code
generated by chatgpt really correct? rigorous evaluation of large
language models for code generation,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html
[11] A. Ni, S. Iyer, D. Radev, V . Stoyanov, W. Yih, S. I. Wang,
and X. V . Lin, “LEVER: learning to verify language-to-code
generation with execution,” in International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA ,
ser. Proceedings of Machine Learning Research, A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202. PMLR, 2023, pp. 26 106–26 128. [Online]. Available:
https://proceedings.mlr.press/v202/ni23b.html
[12] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and
T. Eliassi-Rad, “Collective Classification in Network Data,” AI
Magazine , vol. 29, no. 3, pp. 93–106, Sep. 2008. [Online]. Available:
https://onlinelibrary.wiley.com/doi/10.1609/aimag.v29i3.2157
[13] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[14] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.
Pappu, K. Leswing, and V . Pande, “Moleculenet: a benchmark for
molecular machine learning,” Chemical science , vol. 9, no. 2, pp. 513–
530, 2018.
[15] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan,
R. Stata, A. Tomkins, and J. Wiener, “Graph structure in the
Web,” Computer Networks , vol. 33, no. 1-6, pp. 309–320, Jun.
2000. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
S1389128600000839
[16] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net, 2017. [Online].
Available: https://openreview.net/forum?id=SJU4ayYgl
[17] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li `o,
and Y . Bengio, “Graph attention networks,” in 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings .
OpenReview.net, 2018. [Online]. Available: https://openreview.net/
forum?id=rJXMpikCZ
[18] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in Proceedings
of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017 , ser. Proceedings
of Machine Learning Research, D. Precup and Y . W. Teh,
Eds., vol. 70. PMLR, 2017, pp. 1263–1272. [Online]. Available:
http://proceedings.mlr.press/v70/gilmer17a.html
[19] Y . Hong, J. W. Lam, and B. Z. Tang, “Aggregation-induced emission:
phenomenon, mechanism and applications,” Chemical communications ,
no. 29, pp. 4332–4353, 2009.
[20] W. Cong, M. Ramezani, and M. Mahdavi, “On provable benefits
of depth in training graph convolutional networks,” in Advances
in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual , M. Ranzato, A. Beygelzimer, Y . N.
Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 9936–
9949. [Online]. Available: https://proceedings.neurips.cc/paper/2021/
hash/524265e8b942930fbbe8a5d979d29205-Abstract.html
[21] S. Fan, X. Wang, C. Shi, P. Cui, and B. Wang, “Generalizing Graph
Neural Networks on Out-of-Distribution Graphs,” IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 46, no. 1, pp.
322–337, Jan. 2024. [Online]. Available: https://ieeexplore.ieee.org/
document/10268633/
[22] J. Liu, Z. Shen, Y . He, X. Zhang, R. Xu, H. Yu, and P. Cui,
“Towards Out-Of-Distribution Generalization: A Survey,” Jul. 2023,
arXiv:2108.13624 [cs]. [Online]. Available: http://arxiv.org/abs/2108.
13624
[23] J. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han, “GPT4Graph:
Can Large Language Models Understand Graph Structured Data ? An
Empirical Evaluation and Benchmarking,” Jul. 2023, arXiv:2305.15066
[cs]. [Online]. Available: http://arxiv.org/abs/2305.15066
[24] C. Liu and B. Wu, “Evaluating Large Language Models on
Graphs: Performance Insights and Comparative Analysis,” Sep. 2023,
arXiv:2308.11224 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
11224
[25] Z. Zhang, X. Wang, Z. Zhang, H. Li, Y . Qin, and W. Zhu,
“LLM4DyG: Can Large Language Models Solve Spatial-Temporal
Problems on Dynamic Graphs?” Mar. 2024, arXiv:2310.17110 [cs].
[Online]. Available: http://arxiv.org/abs/2310.17110
[26] J. Huang, X. Zhang, Q. Mei, and J. Ma, “Can llms effectively
leverage graph structural information: When and why,” CoRR , vol.
abs/2309.16595, 2023. [Online]. Available: https://doi.org/10.48550/
arXiv.2309.16595
[27] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y . Tsvetkov, “Can
Language Models Solve Graph Problems in Natural Language?” Jan.
2024, arXiv:2305.10037 [cs]. [Online]. Available: http://arxiv.org/abs/
2305.10037
[28] Q. Dong, L. Dong, K. Xu, G. Zhou, Y . Hao, Z. Sui, and
F. Wei, “Large Language Model for Science: A Study on P
vs. NP,” Sep. 2023, arXiv:2309.05689 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.05689[29] L. Fan, W. Hua, L. Li, H. Ling, and Y . Zhang, “NPHardEval:
Dynamic Benchmark on Reasoning Ability of Large Language Models
via Complexity Classes,” Feb. 2024, arXiv:2312.14890 [cs]. [Online].
Available: http://arxiv.org/abs/2312.14890
[30] X. He, X. Bresson, T. Laurent, A. Perold, Y . LeCun, and
B. Hooi, “Harnessing Explanations: LLM-to-LM Interpreter for
Enhanced Text-Attributed Graph Representation Learning,” Mar. 2024,
arXiv:2305.19523 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
19523
[31] J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and J. Tang,
“Learning on Large-scale Text-attributed Graphs via Variational
Inference,” Mar. 2023, arXiv:2210.14709 [cs]. [Online]. Available:
http://arxiv.org/abs/2210.14709
[32] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y . Chen, and
M. Zhang, “One for All: Towards Training One Graph Model for
All Classification Tasks,” Dec. 2023, arXiv:2310.00149 [cs]. [Online].
Available: http://arxiv.org/abs/2310.00149
[33] X. Huang, K. Han, D. Bao, Q. Tao, Z. Zhang, Y . Yang, and Q. Zhu,
“Prompt-based Node Feature Extractor for Few-shot Learning on
Text-Attributed Graphs,” Sep. 2023, arXiv:2309.02848 [cs]. [Online].
Available: http://arxiv.org/abs/2309.02848
[34] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk,
and T. Hoefler, “Graph of Thoughts: Solving Elaborate Problems with
Large Language Models,” Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 38, no. 16, pp. 17 682–17 690, Mar. 2024,
arXiv:2308.09687 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
09687
[35] Y . Zhang, J. Yang, Y . Yuan, and A. C.-C. Yao, “Cumulative Reasoning
with Large Language Models,” Apr. 2024, arXiv:2308.04371 [cs].
[Online]. Available: http://arxiv.org/abs/2308.04371
[36] Y . Yao, Z. Li, and H. Zhao, “Beyond Chain-of-Thought, Effective
Graph-of-Thought Reasoning in Language Models,” Mar. 2024,
arXiv:2305.16582 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
16582
[37] J. Zhao, L. Zhuo, Y . Shen, M. Qu, K. Liu, M. Bronstein,
Z. Zhu, and J. Tang, “GraphText: Graph Reasoning in Text
Space,” Oct. 2023, arXiv:2310.01089 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.01089
[38] Y . Tan, Z. Zhou, H. Lv, W. Liu, and C. Yang,
“Walklm: A uniform language model fine-tuning framework for
attributed graph embedding,” in Advances in Neural Information
Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/2ac879d1865475a7abc8dfc7a9c15c27-Abstract-Conference.html
[39] Y . Qin, X. Wang, Z. Zhang, and W. Zhu, “Disentangled Representation
Learning with Large Language Models for Text-Attributed Graphs,”
Mar. 2024, arXiv:2310.18152 [cs]. [Online]. Available: http://arxiv.
org/abs/2310.18152
[40] S. Dernbach, K. Agarwal, A. Zuniga, M. Henry, and S. Choudhury,
“GLaM: Fine-Tuning Large Language Models for Domain Knowledge
Graph Alignment via Neighborhood Partitioning and Generative
Subgraph Encoding,” Apr. 2024, arXiv:2402.06764 [cs]. [Online].
Available: http://arxiv.org/abs/2402.06764
[41] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y . Gong, L. M. Ni,
H.-Y . Shum, and J. Guo, “Think-on-Graph: Deep and Responsible
Reasoning of Large Language Model on Knowledge Graph,” Mar.
2024, arXiv:2307.07697 [cs]. [Online]. Available: http://arxiv.org/abs/
2307.07697
[42] Y . Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V .
Chawla, and P. Xu, “Graph Neural Prompting with Large Language
Models,” Dec. 2023, arXiv:2309.15427 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.15427
[43] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on Graphs:
Faithful and Interpretable Large Language Model Reasoning,” Feb.
2024, arXiv:2310.01061 [cs]. [Online]. Available: http://arxiv.org/abs/
2310.01061
[44] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang,
D. Yin, and C. Huang, “LLMRec: Large Language Models with Graph
Augmentation for Recommendation,” Jan. 2024, arXiv:2311.00423
[cs]. [Online]. Available: http://arxiv.org/abs/2311.00423[45] H. Wang, Y . Gao, X. Zheng, P. Zhang, H. Chen, J. Bu,
and P. S. Yu, “Graph Neural Architecture Search with GPT-
4,” Mar. 2024, arXiv:2310.01436 [cs]. [Online]. Available: http:
//arxiv.org/abs/2310.01436
[46] L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen, “Exploring
large language model for graph data understanding in online job
recommendations,” in Thirty-Eighth AAAI Conference on Artificial
Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2024, Fourteenth
Symposium on Educational Advances in Artificial Intelligence, EAAI
2014, February 20-27, 2024, Vancouver, Canada , M. J. Wooldridge,
J. G. Dy, and S. Natarajan, Eds. AAAI Press, 2024, pp. 9178–9186.
[Online]. Available: https://doi.org/10.1609/aaai.v38i8.28769
[47] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,
B. Zhang, J. Zhang, Z. Dong, Y . Du, C. Yang, Y . Chen, Z. Chen,
J. Jiang, R. Ren, Y . Li, X. Tang, Z. Liu, P. Liu, J.-Y . Nie, and J.-R. Wen,
“A Survey of Large Language Models,” Nov. 2023, arXiv:2303.18223
[cs]. [Online]. Available: http://arxiv.org/abs/2303.18223
[48] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and
X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt
and beyond,” CoRR , vol. abs/2304.13712, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2304.13712
[49] M. Himsolt, “Gml: A portable graph file format,” Technical report,
Universitat Passau, Tech. Rep., 1997.
[50] U. Brandes, M. Eiglsperger, J. Lerner, and C. Pich, “Graph markup lan-
guage (graphml),” in Handbook on Graph Drawing and Visualization ,
R. Tamassia, Ed. Chapman and Hall/CRC, 2013, pp. 517–541.
[51] N. Francis, A. Green, P. Guagliardo, L. Libkin, T. Lindaaker,
V . Marsault, S. Plantikow, M. Rydberg, P. Selmer, and A. Taylor,
“Cypher: An Evolving Query Language for Property Graphs,” in
Proceedings of the 2018 International Conference on Management of
Data . Houston TX USA: ACM, May 2018, pp. 1433–1445. [Online].
Available: https://dl.acm.org/doi/10.1145/3183713.3190657
[52] M. A. Rodriguez, “The Gremlin graph traversal machine and language
(invited talk),” in Proceedings of the 15th Symposium on Database
Programming Languages . Pittsburgh PA USA: ACM, Oct. 2015,
pp. 1–10. [Online]. Available: https://dl.acm.org/doi/10.1145/2815072.
2815073
[53] J. P ´erez, M. Arenas, and C. Gutierrez, “Semantics and complexity of
SPARQL,” ACM Transactions on Database Systems , vol. 34, no. 3,
pp. 1–45, Aug. 2009. [Online]. Available: https://dl.acm.org/doi/10.
1145/1567274.1567278
[54] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, “RAT-
SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL
Parsers,” Aug. 2021, arXiv:1911.04942 [cs]. [Online]. Available:
http://arxiv.org/abs/1911.04942
[55] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,
“Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
Methods in Natural Language Processing,” ACM Computing Surveys ,
vol. 55, no. 9, pp. 1–35, Sep. 2023. [Online]. Available: https:
//dl.acm.org/doi/10.1145/3560815
[56] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta,
and J. Leskovec, “Open graph benchmark: Datasets for machine
learning on graphs,” in Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,
2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/
hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html
[57] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su,
“ArnetMiner: extraction and mining of academic social networks,”
inProceedings of the 14th ACM SIGKDD international conference
on Knowledge discovery and data mining . Las Vegas Nevada
USA: ACM, Aug. 2008, pp. 990–998. [Online]. Available: https:
//dl.acm.org/doi/10.1145/1401890.1402008
[58] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli,
E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom,
“Toolformer: Language models can teach themselves to use tools,”
inAdvances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023 , A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html[59] J. Zhang, “Graph-ToolFormer: To Empower LLMs with Graph
Reasoning Ability via Prompt Augmented by ChatGPT,” May 2023,
arXiv:2304.11116 [cs]. [Online]. Available: http://arxiv.org/abs/2304.
11116
[60] H. Face, “hivemind/gpt-j-6b-8bit.”
[61] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter
autoregressive language model,” 2021.
[62] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “LLaMA: Open and Efficient
Foundation Language Models,” 2023, publisher: [object Object]
Version Number: 1. [Online]. Available: https://arxiv.org/abs/2302.
13971
[63] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,
and W. Chen, “LoRA: Low-Rank Adaptation of Large Language
Models,” Oct. 2021, arXiv:2106.09685 [cs]. [Online]. Available:
http://arxiv.org/abs/2106.09685
[64] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y . Yang,
“GraphLLM: Boosting Graph Reasoning Ability of Large Language
Model,” Oct. 2023, arXiv:2310.05845 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.05845
[65] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. H. Chi, Q. V . Le, and D. Zhou, “Chain-of-thought prompting
elicits reasoning in large language models,” in Advances in Neural
Information Processing Systems 35: Annual Conference on Neural
Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022 , S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022.
[Online]. Available: http://papers.nips.cc/paper files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
[66] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-
hery, and D. Zhou, “Self-Consistency Improves Chain of Thought
Reasoning in Language Models,” Mar. 2023, arXiv:2203.11171 [cs].
[Online]. Available: http://arxiv.org/abs/2203.11171
[67] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu,
L. Li, and Z. Sui, “A Survey on In-context Learning,” Jun. 2023,
arXiv:2301.00234 [cs]. [Online]. Available: http://arxiv.org/abs/2301.
00234
[68] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal Policy Optimization Algorithms,” Aug. 2017, arXiv:1707.06347
[cs]. [Online]. Available: http://arxiv.org/abs/1707.06347
[69] H. Dong, W. Xiong, D. Goyal, Y . Zhang, W. Chow, R. Pan,
S. Diao, J. Zhang, K. Shum, and T. Zhang, “RAFT: Reward
rAnked FineTuning for Generative Foundation Model Alignment,”
Dec. 2023, arXiv:2304.06767 [cs, stat]. [Online]. Available: http:
//arxiv.org/abs/2304.06767
[70] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y . Li, and H. Wang, “Preference
Ranking Optimization for Human Alignment,” Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 38, no. 17, pp.
18 990–18 998, Mar. 2024. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/29865
[71] K. Duan, Q. Liu, T.-S. Chua, S. Yan, W. T. Ooi, Q. Xie, and J. He,
“SimTeG: A Frustratingly Simple Approach Improves Textual Graph
Learning,” Aug. 2023, arXiv:2308.02565 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.02565
[72] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu, and
J. Tang, “Label-free node classification on graphs with large language
models (LLMS),” CoRR , vol. abs/2310.04668, 2023. [Online].
Available: https://doi.org/10.48550/arXiv.2310.04668
[73] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang,
D. Yin, W. Fan, H. Liu, and J. Tang, “Exploring the Potential of
Large Language Models (LLMs) in Learning on Graphs,” Jan. 2024,
arXiv:2307.03393 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
03393
[74] Y . Hu, Z. Zhang, and L. Zhao, “Beyond Text: A Deep Dive
into Large Language Models’ Ability on Understanding Graph
Data,” Oct. 2023, arXiv:2310.04944 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.04944
[75] J. Yu, Y . Ren, C. Gong, J. Tan, X. Li, and X. Zhang, “Empower
Text-Attributed Graphs Learning with Large Language Models
(LLMs),” Oct. 2023, arXiv:2310.09872 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.09872
[76] Q. Wang, Z. Gao, and R. Xu, “Graph agent: Explicit reasoning agentfor graphs,” CoRR , vol. abs/2310.16421, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2310.16421
[77] B. Bi, S. Liu, Y . Wang, L. Mei, and X. Cheng, “LPNL: Scalable Link
Prediction with Large Language Models,” Feb. 2024, arXiv:2401.13227
[cs]. [Online]. Available: http://arxiv.org/abs/2401.13227
[78] R. Ye, C. Zhang, R. Wang, S. Xu, and Y . Zhang, “Language is All a
Graph Needs,” Feb. 2024, arXiv:2308.07134 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.07134
[79] J. Tang, Y . Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and
C. Huang, “GraphGPT: Graph Instruction Tuning for Large Language
Models,” Dec. 2023, arXiv:2310.13023 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.13023
[80] M. Sun, K. Zhou, X. He, Y . Wang, and X. Wang, “GPPT:
Graph Pre-training and Prompt Tuning to Generalize Graph Neural
Networks,” in Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining . Washington DC
USA: ACM, Aug. 2022, pp. 1717–1727. [Online]. Available:
https://dl.acm.org/doi/10.1145/3534678.3539249
[81] Z. Liu, X. Yu, Y . Fang, and X. Zhang, “GraphPrompt: Unifying
Pre-Training and Downstream Tasks for Graph Neural Networks,”
inProceedings of the ACM Web Conference 2023 . Austin
TX USA: ACM, Apr. 2023, pp. 417–428. [Online]. Available:
https://dl.acm.org/doi/10.1145/3543507.3583386
[82] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, “All in One: Multi-Task
Prompting for Graph Neural Networks,” in Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining .
Long Beach CA USA: ACM, Aug. 2023, pp. 2120–2131. [Online].
Available: https://dl.acm.org/doi/10.1145/3580305.3599256
[83] L. Cao, “GraphReason: Enhancing Reasoning Capabilities of Large
Language Models through A Graph-Based Verification Approach,”
Apr. 2024, arXiv:2308.09267 [cs]. [Online]. Available: http://arxiv.
org/abs/2308.09267
[84] J. Park, A. Patel, O. Z. Khan, H. J. Kim, and J.-K. Kim, “Graph-
Guided Reasoning for Multi-Hop Question Answering in Large
Language Models,” Nov. 2023, arXiv:2311.09762 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09762
[85] Z. Wen and Y . Fang, “Augmenting Low-Resource Text Classification
with Graph-Grounded Pre-training and Prompting,” in Proceedings
of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval , Jul. 2023, pp. 506–516,
arXiv:2305.03324 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
03324
[86] B. Fatemi, J. Halcrow, and B. Perozzi, “Talk like a Graph: Encoding
Graphs for Large Language Models,” Oct. 2023, arXiv:2310.04560
[cs]. [Online]. Available: http://arxiv.org/abs/2310.04560
[87] D. Das, I. Gupta, J. Srivastava, and D. Kang, “Which Modality
should I use – Text, Motif, or Image? : Understanding Graphs with
Large Language Models,” Mar. 2024, arXiv:2311.09862 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09862
[88] K. Sun, Y . E. Xu, H. Zha, Y . Liu, and X. L. Dong, “Head-to-Tail: How
Knowledgeable are Large Language Models (LLMs)? A.K.A. Will
LLMs Replace Knowledge Graphs?” Apr. 2024, arXiv:2308.10168
[cs]. [Online]. Available: http://arxiv.org/abs/2308.10168
[89] L. Yang, H. Chen, Z. Li, X. Ding, and X. Wu, “Give us the facts:
Enhancing large language models with knowledge graphs for fact-
aware language modeling,” IEEE Transactions on Knowledge and Data
Engineering , 2024.
[90] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, “Unifying
large language models and knowledge graphs: A roadmap,” CoRR ,
vol. abs/2306.08302, 2023. [Online]. Available: https://doi.org/10.
48550/arXiv.2306.08302
[91] S. Zheng, H. Bai, Y . Zhang, Y . Su, X. Niu, and N. Jaitly, “KGLens:
A Parameterized Knowledge Graph Solution to Assess What an LLM
Does and Doesn’t Know,” Feb. 2024, arXiv:2312.11539 [cs]. [Online].
Available: http://arxiv.org/abs/2312.11539
[92] R. Zhang, Y . Su, B. D. Trisedya, X. Zhao, M. Yang, H. Cheng,
and J. Qi, “AutoAlign: Fully Automatic and Effective Knowledge
Graph Alignment enabled by Large Language Models,” Nov. 2023,
arXiv:2307.11772 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
11772
[93] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and
T. Derr, “Knowledge Graph Prompting for Multi-Document Question
Answering,” Dec. 2023, arXiv:2308.11730 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.11730[94] Z. Chen, Z. Jiang, F. Yang, E. Cho, X. Fan, X. Huang, Y . Lu,
and A. Galstyan, “Graph Meets LLM: A Novel Approach to
Collaborative Filtering for Robust Conversational Understanding,”
Jun. 2023, arXiv:2305.14449 [cs]. [Online]. Available: http://arxiv.org/
abs/2305.14449
[95] C. Sun, J. Li, Y . R. Fung, H. P. Chan, T. Abdelzaher, C. Zhai, and
H. Ji, “Decoding the Silent Majority: Inducing Belief Augmented
Social Graph with Large Language Model for Response Forecasting,”
Oct. 2023, arXiv:2310.13297 [cs]. [Online]. Available: http://arxiv.org/
abs/2310.13297
[96] R. Su, T.-W. Wu, and B.-H. Juang, “Schema Graph-Guided Prompt for
Multi-Domain Dialogue State Tracking,” Nov. 2023, arXiv:2311.06345
[cs]. [Online]. Available: http://arxiv.org/abs/2311.06345
[97] Y . Peng, S. Lin, Q. Chen, L. Xu, X. Ren, Y . Li, and J. Xu,
“ChatGraph: Chat with Your Graphs,” Jan. 2024, arXiv:2401.12672
[cs]. [Online]. Available: http://arxiv.org/abs/2401.12672
[98] Y . Shen, R. Liao, Z. Han, Y . Ma, and V . Tresp, “GraphextQA:
A Benchmark for Evaluating Graph-Enhanced Large Language
Models,” Oct. 2023, arXiv:2310.08487 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.08487
[99] H. Yan, C. Li, R. Long, C. Yan, J. Zhao, W. Zhuang, J. Yin,
P. Zhang, W. Han, H. Sun, W. Deng, Q. Zhang, L. Sun,
X. Xie, and S. Wang, “A comprehensive study on text-attributed
graphs: Benchmarking and rethinking,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets and
Benchmarks.html
[100] A. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating
the construction of internet portals with machine learning,” Inf.
Retr., vol. 3, no. 2, pp. 127–163, 2000. [Online]. Available:
https://doi.org/10.1023/A:1009953814988
[101] C. L. Giles, K. D. Bollacker, and S. Lawrence, “CiteSeer: an
automatic citation indexing system,” in Proceedings of the third ACM
conference on Digital libraries - DL ’98 . Pittsburgh, Pennsylvania,
United States: ACM Press, 1998, pp. 89–98. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=276675.276685
[102] Y . Zhang, H. Dai, Z. Kozareva, A. Smola, and L. Song,
“Variational Reasoning for Question Answering With Knowledge
Graph,” Proceedings of the AAAI Conference on Artificial Intelligence ,
vol. 32, no. 1, Apr. 2018. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/12057
[103] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and
J. Tang, “KEPLER: A unified model for knowledge embedding
and pre-trained language representation,” Trans. Assoc. Comput.
Linguistics , vol. 9, pp. 176–194, 2021. [Online]. Available: https:
//doi.org/10.1162/tacl a00360
[104] K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V . N. Vishwanathan,
A. J. Smola, and H.-P. Kriegel, “Protein function prediction via
graph kernels,” Bioinformatics , vol. 21, no. Suppl 1, pp. i47–i56, Jun.
2005. [Online]. Available: https://academic.oup.com/bioinformatics/
article-lookup/doi/10.1093/bioinformatics/bti1007
[105] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shuster-
man, and C. Hansch, “Structure-activity relationship of mutagenic aro-
matic and heteroaromatic nitro compounds. correlation with molecular
orbital energies and hydrophobicity,” Journal of medicinal chemistry ,
vol. 34, no. 2, pp. 786–797, 1991.
[106] N. Wale, I. A. Watson, and G. Karypis, “Comparison
of descriptor spaces for chemical compound retrieval and
classification,” Knowledge and Information Systems , vol. 14,
no. 3, pp. 347–375, Mar. 2008. [Online]. Available:
http://link.springer.com/10.1007/s10115-007-0103-5
[107] H. Toivonen, A. Srinivasan, R. D. King, S. Kramer,
and C. Helma, “Statistical evaluation of the Predictive
ToxicologyChallenge 2000–2001,” Bioinformatics , vol. 19,
no. 10, pp. 1183–1193, Jul. 2003. [Online]. Available:
https://academic.oup.com/bioinformatics/article/19/10/1183/184239
[108] X. Kong, J. Zhang, and P. S. Yu, “Inferring anchor links
across multiple heterogeneous social networks,” in Proceedings
of the 22nd ACM international conference on Conference on
information & knowledge management - CIKM ’13 . San Francisco,California, USA: ACM Press, 2013, pp. 179–188. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2505515.2505531
[109] M. Wan and J. McAuley, “Item recommendation on monotonic
behavior chains,” in Proceedings of the 12th ACM Conference on
Recommender Systems . Vancouver British Columbia Canada: ACM,
Sep. 2018, pp. 86–94. [Online]. Available: https://dl.acm.org/doi/10.
1145/3240323.3240369
[110] J. Ni, J. Li, and J. McAuley, “Justifying Recommendations
using Distantly-Labeled Reviews and Fine-Grained Aspects,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) . Hong Kong,
China: Association for Computational Linguistics, 2019, pp. 188–197.
[Online]. Available: https://www.aclweb.org/anthology/D19-1018
[111] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,
R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[112] A. Bojchevski and S. G ¨unnemann, “Deep Gaussian Embedding of
Graphs: Unsupervised Inductive Learning via Ranking,” Feb. 2018,
arXiv:1707.03815 [cs, stat]. [Online]. Available: http://arxiv.org/abs/
1707.03815
[113] O. Shchur, M. Mumme, A. Bojchevski, and S. G ¨unnemann, “Pitfalls
of Graph Neural Network Evaluation,” Jun. 2019, arXiv:1811.05868
[cs, stat]. [Online]. Available: http://arxiv.org/abs/1811.05868
[114] R. Rossi and N. Ahmed, “The Network Data Repository with
Interactive Graph Analytics and Visualization,” Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 29, no. 1, Mar.
2015. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/9277
[115] H. Huang, H. Wang, and X. Wang, “An analysis framework
of research frontiers based on the large-scale open academic
graph,” Proceedings of the Association for Information Science and
Technology , vol. 57, no. 1, p. e307, Oct. 2020. [Online]. Available:
https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.307
[116] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
J. Schulman, “Training Verifiers to Solve Math Word Problems,” Nov.
2021, arXiv:2110.14168 [cs]. [Online]. Available: http://arxiv.org/abs/
2110.14168
[117] A. Patel, S. Bhattamishra, and N. Goyal, “Are NLP Models really able
to Solve Simple Math Word Problems?” Apr. 2021, arXiv:2103.07191
[cs]. [Online]. Available: http://arxiv.org/abs/2103.07191
[118] S. Han, H. Schoelkopf, Y . Zhao, Z. Qi, M. Riddell, L. Benson,
L. Sun, E. Zubova, Y . Qiao, M. Burtell, D. Peng, J. Fan, Y . Liu,
B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,
S. Joty, A. R. Fabbri, W. Kryscinski, X. V . Lin, C. Xiong, and
D. Radev, “FOLIO: Natural Language Reasoning with First-Order
Logic,” Sep. 2022, arXiv:2209.00840 [cs]. [Online]. Available:
http://arxiv.org/abs/2209.00840
[119] Y . Zhang, B. Jin, Q. Zhu, Y . Meng, and J. Han, “The effect of metadata
on scientific literature tagging: A cross-field cross-model study,” in
Proceedings of the ACM Web Conference 2023 , 2023, pp. 1626–1637.
[120] A. Johnson, T. Pollard, and R. Mark, “MIMIC-III Clinical Database,”
2015. [Online]. Available: https://physionet.org/content/mimiciii/1.4/
[121] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor,
“Freebase: a collaboratively created graph database for structuring
human knowledge,” in Proceedings of the 2008 ACM SIGMOD
international conference on Management of data . Vancouver
Canada: ACM, Jun. 2008, pp. 1247–1250. [Online]. Available:
https://dl.acm.org/doi/10.1145/1376616.1376746
[122] K. Toutanova and D. Chen, “Observed versus latent features
for knowledge base and text inference,” in Proceedings of the
3rd Workshop on Continuous Vector Space Models and their
Compositionality . Beijing, China: Association for Computational
Linguistics, 2015, pp. 57–66. [Online]. Available: http://aclweb.org/
anthology/W15-4007
[123] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, “The
value of semantic parse labeling for knowledge base question
answering,” in Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics, ACL 2016, August
7-12, 2016, Berlin, Germany, Volume 2: Short Papers . The
Association for Computer Linguistics, 2016. [Online]. Available:
https://doi.org/10.18653/v1/p16-2033
[124] A. Talmor and J. Berant, “The Web as a Knowledge-base for
Answering Complex Questions,” Mar. 2018, arXiv:1803.06643 [cs].
[Online]. Available: http://arxiv.org/abs/1803.06643
[125] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”
inText summarization branches out , 2004, pp. 74–81.
[126] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in Proceedings
of the 40th Annual Meeting on Association for Computational
Linguistics - ACL ’02 . Philadelphia, Pennsylvania: Association
for Computational Linguistics, 2001, p. 311. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=1073083.1073135
[127] C. Goutte and ´E. Gaussier, “A probabilistic interpretation of precision,
recall and F-score, with implication for evaluation,” in Advances in
Information Retrieval, 27th European Conference on IR Research,
ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005,
Proceedings , ser. Lecture Notes in Computer Science, D. E. Losada and
J. M. Fern ´andez-Luna, Eds., vol. 3408. Springer, 2005, pp. 345–359.
[Online]. Available: https://doi.org/10.1007/978-3-540-31865-1 25
[128] J. M. Kleinberg, R. Kumar, P. Raghavan, S. Rajagopalan, and
A. Tomkins, “The web as a graph: Measurements, models,
and methods,” in Computing and Combinatorics, 5th Annual
International Conference, COCOON ’99, Tokyo, Japan, July 26-
28, 1999, Proceedings , ser. Lecture Notes in Computer Science,
T. Asano, H. Imai, D. T. Lee, S. Nakano, and T. Tokuyama,
Eds., vol. 1627. Springer, 1999, pp. 1–17. [Online]. Available:
https://doi.org/10.1007/3-540-48686-0 1
[129] S. M. Iacus, G. King, and G. Porro, “Causal inference without balance
checking: Coarsened exact matching,” Political analysis , vol. 20, no. 1,
pp. 1–24, 2012.
[130] H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z. Deng, L. Kong, and
Q. Liu, “GIMLET: A unified graph-text model for instruction-
based molecule zero-shot learning,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
129033c7c08be683059559e8d6bfd460-Abstract-Conference.html

Title: TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series
Abstract: This work summarizes two ways to accomplish Time-Series (TS) tasks in today's
Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and
trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;
TS-for-LLM (data-centric) converts TS into a model-friendly representation to
enable the pre-trained LLM to handle TS data. Given the lack of data, limited
resources, semantic context requirements, and so on, this work focuses on
TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a
TS embedding method suitable for LLM. The proposed method is named TEST. It
first tokenizes TS, builds an encoder to embed TS via instance-wise,
feature-wise, and text-prototype-aligned contrast, where the TS embedding space
is aligned to LLM embedding layer space, then creates soft prompts to make LLM
more open to that embeddings, and finally implements TS tasks using the frozen
LLM. We also demonstrate the feasibility of TS-for-LLM through theory and
experiments. Experiments are carried out on TS classification, forecasting, and
representation tasks using eight frozen LLMs with various structures and sizes.
The results show that the pre-trained LLM with TEST strategy can achieve better
or comparable performance than today's SOTA TS models and offer benefits for
few-shot and generalization. By treating LLM as the pattern machine, TEST can
endow LLM's ability to process TS data without compromising language ability.
We hope that this study will serve as a foundation for future work to support
TS+LLM progress.
Full Text: Published as a conference paper at ICLR 2024
TEST: T EXT PROTOTYPE ALIGNED EMBEDDING TO
ACTIVATE LLM’ SABILITY FOR TIMESERIES
Chenxi Sun1,2,3, Hongyan Li1,2,3,4,∗, Yaliang Li5, Shenda Hong6,7,∗
1National Key Laboratory of General Artificial Intelligence, Peking University
2Key Laboratory of Machine Perception (Ministry of Education), Peking University
3School of Intelligence Science and Technology, Peking University
4PKU-WUHAN Institute for Artificial Intelligence
5Alibaba Group
6National Institute of Health Data Science, Peking University
7Institute of Medical Technology, Health Science Center of Peking University
{chenxi sun,leehy }@pku.edu,cn
yaliang.li@alibaba-inc.com, hongshenda@pku.edu.cn
ABSTRACT
This work summarizes two ways to accomplish Time-Series (TS) tasks in today’s
Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and
trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;
TS-for-LLM (data-centric) converts TS into a model-friendly representation to
enable the pre-trained LLM to handle TS data. Given the lack of data, limited
resources, semantic context requirements, and so on, this work focuses on TS-
for-LLM, where we aim to activate LLM’s ability for TS data by designing a TS
embedding method suitable for LLM. The proposed method is named TEST . It
first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise,
and text-prototype-aligned contrast, where the TS embedding space is aligned to
LLM’s embedding layer space, then creates soft prompts to make LLM more open
to that embeddings, and finally implements TS tasks using the frozen LLM. We
also demonstrate the feasibility of TS-for-LLM through theory and experiments.
Experiments are carried out on TS classification, forecasting, and representation
tasks using eight frozen LLMs with various structures and sizes. The results show
that the pre-trained LLM with TEST strategy can achieve better or comparable
performance than today’s SOTA TS models and offer benefits for few-shot and
generalization. By treating LLM as the pattern machine, TEST can endow LLM’s
ability to process TS data without compromising language ability. We hope that
this study will serve as a foundation for future work to support TS+LLM progress.
1 I NTRODUCTION
Implementing Time-Series (TS) tasks, such as medical, industrial, and meteorological, is a research-
intensive field Sun et al. (2020). The relevant models evolved from statistical models to RNNs,
CNNs, and Transformers. Nowadays, we see a fast growth and remarkable performances of Large-
scale pre-trained Language Models (LLM) in NLP and CV fields Zhao et al. (2023). Consequently,
it seems natural to inquire whether LLMs can be used for TS tasks. However, according to experi-
ments, most pre-trained LLMs have not made significant progress in relation to abstract TS.
In answer to this requirement, we envision two ways to achieve the paradigm of TS+LLM1:
• LLM-for-TS (model-centric, modify LLM). For TS data, design and train a fundamental Large
Model from scratch (LM-of-TS), then fine-tune the model accordingly for various downstream
tasks. Or, fine-tune the existing pre-trained LLM and convert it from text tasks to TS tasks;
∗Corresponding authors
1This categorization focuses on the requirement for changing the model. But from technology, LLM+TS
can be achieved by pre-training, fine-tuning, tool-augmented methods, external encoders, and their ensemble.
1arXiv:2308.08241v2  [cs.CL]  22 Feb 2024Published as a conference paper at ICLR 2024
• TS-for-LLM (data-centric, modify TS). Based on the existing LLMs, furthest freezing them,
design some mechanisms to customize TS for them by creating LLM-friendly TS representation.
We acknowledge that the first way, particularly developing and training a model from scratch, is the
most essential solution since pre-training is the crucial step of instilling knowledge to the model.
And the second way is actually challenging to break beyond the model’s original capabilities. How-
ever, in this work, we still focus on the second way due to the following three considerations:
Data perspective . LLM-for-TS methods, especially when building a foundation model, necessitate
large dataset, but TS is professional, the largest dataset is less than 10GB, which is much smaller
than that for NLP Zhou et al. (2023); TS-for-LLM methods can use a relatively small dataset as
its objective is solely to assist the existing LLM in inferring TS; Model perspective . LLM-for-TS
methods focus on vertical industries. Because of the major disparities in TS across domains, various
large models targeting medical TS, industrial TS, etc. must be built and trained from the start;
TS-for-LLM methods need little or even no training. By utilizing plug-in modules, it makes the
utilization more general and convenient; Usage perspective . LLM-for-TS methods are appropriate
for instances involving specialists; TS-for-LLM methods maintain LLM’s textual capabilities while
providing rich complementing semantics, being easily accessible and user-friendly.
Without changing the existing model, the most natural approach is treating TS as text data. For
example, a possible dialogue is: [Q] Diagnose if a patient has sepsis through the following mean
arterial pressure sequence in mm Hg: 88, 95, 78, 65, 52, 30. [A] Yes. However, TS is often
multivariate while text is univariate. For example, excepting mean arterial pressure, dozens of vital
signs, and laboratory values, such as heart rate, lactic acid, etc., need to be included when diagnosing
sepsis. One intuitive method is to divide a multivariate TS into multiple univariate sequences and
input them into LLM one by one. However, this will lead to three drawbacks. First, different prompt
sentences, data order, and connection statements will produce different results; Second, a long input
sequence likely to make LLM inefficient and hard to remember the previous univariate TS; Third,
the crucial aspects of multivariate dependency in TS will be ignored.
To address the above issues and achieve TS-for-LLM, we do not directly input TS into LLM, but
instead, we first tokenize TS, then design an encoder to embed them, finally skip the embedding layer
to input them into LLM. In this way, the core is to create embeddings that the LLM can understand.
High-quality TS embedding can be employed as the computational phenotype that the deep learning
model can understand Hong et al. (2023). To make the embedding understandable by language
models. Most multimodal approaches use alignment, for example, aligning text embedding and
image embedding through text descriptions of the image Wang et al. (2023). However, TS lacks
visual cues and has an annotation bottleneck caused by its complex characteristics. Only a few
specific TS, such as ECG, have text descriptions in each segment, where the image-text matching
route could be implemented. But in most cases, it’s not feasible.
Contrastive Learning (CL) can avoid the annotation bottleneck through designing pretext tasks by
utilizing intrinsic information instead of relying on pre-defined prior knowledge. Currently, CL
methods for TS data has also advanced Meng et al. (2023b). These methods evaluate the effec-
tiveness of TS embedding through follow-up classification, prediction, or clustering models, such
as SVM Franceschi et al. (2019b). However, these simple and newly-trained models are consid-
erably different from the complex and pre-trained LLM. The representation vector generated by
unconstrained CL is likely to deviate greatly from the LLM’s cognitive embedding space.
To address the above issues, we propose an embedding method for TimE Series tokens to align
theText embedding space of LLM ( TEST ). Based on CL, TEST uses text embedding vectors as
prototypes to constrain TS’ embedding space and highlights feature-wise patterns. We show that
TEST can activate LLM’s ability as pattern machine. The contributions of this work are:
• Summarize two TS+LLM paradigms, LLM-for-TS, TS-for-LLM, with their potential methods;
• Propose TEST for TS-for-LLM. TEST can produce the similarity-based, instance-wise, feature-
wise, and text-prototype-aligned embedding for TS tokens. We prove that prompt tuning is al-
most equivalent to supervised fine-tuning when TS embedding and word embedding are aligned;
• Experiments on TS classification, forecasting, few-shot, and representation tasks demonstrate
thatTEST can activate LLM’s capability to archive TS tasks, where the random and unsatisfac-
tory results produced by original LLMs can be elevated to the baseline.
2Published as a conference paper at ICLR 2024
Category Means Pros Cons Work
LM-of-TS TrainingSpecialized, Not universal, Pre-training Ma et al. (2023)
accurate large datasets Earth transformer Bi et al. (2023)
LLM-for-TSTuningEnd-to-end, More experiments, GPT4TSZhou et al. (2023)
accurate lose language ability LLM4TSChang et al. (2023)
Tool
augmentedParameter-efficient,
less experimentsNeed experts,
need annotationPromptCast Xue & Salim (2023)
Health Learner Liu et al. (2023)
METS Li et al. (2024)
Text2ECGChung et al. (2023)
TS-for-LLMExternal
encoderParameter-efficient,Weak robust TESTmultiple abilities
Table 1: Existing Work about TS+LLM
As the name of TEST implies, it’s a forward-looking test that we hope to lay the groundwork for
future study. And it does give LLM new capabilities and highlight its qualities as a pattern machine.
2 R ELATED WORK
2.1 T IMESERIES AND LARGE LANGUAGE MODEL
There hasn’t been much research done on TS+LLM because this field is still in its infancy. We
summarize the existing work in Table 1. LLM-for-TS with changing the model can be achieved
through tuning or tool augmented means; TS-for-LLM with changing the data can be achieved
through building the external encoder.
LM-of-TS Ma et al. (2023) trains a fundamental and accurate model based on accumulated domain
TS data, but it can be difficult to construct a large well-labeled dataset due to data acquisition and
annotation costs. By comparison, Supervised Fine-Tuning (SFT) in LLM-for-TS Chang et al. (2023)
has a relatively smaller workload than pre-training, but it can make the LLM lose its language
capabilities and its advantages over a sophisticated model designed specifically for TS tasks are
unclear. Regarding TS as the text sequence and using prompts as the augmented tool Liu et al.
(2023) could input numerical TS into LLM directly, but it is inaccurate, requires more experience,
and will fail for multivariate TS. The multimodal methods Li et al. (2024) could align the text and
TS, but apart from ECG, most TS datasets have no segment annotation.
2.2 T IMESEIRES EMBEDDING
TS embedding can provide identities by including typical, associated, and dependant attributes.
CL-based methods can get the data representation Chen et al. (2020), employing the instance dis-
crimination pretext task to bring similar pairs closer while pushing dissimilar pairs apart in the
embedding space. Some efforts have been made to implement instance-level contrast Woo et al.
(2022b); Zheng et al. (2023), temporal-level contrast Meng et al. (2023c); Franceschi et al. (2019b),
and clustering-level contrast Meng et al. (2023a) on TS data, with promising results. However, the
direct contrast cannot bridge TS embedding and the LLM’s comprehensible space. In our setting,
we prefer to freeze the pre-trained LLM and let the embedding compromise. That is, we use the text
token embedding in LLM to limit and guide the TS token embedding.
Inspired by the prototype-level contrast Caron et al. (2020a), which goes beyond the independence
assumption and exploits latent cluster information present within samples. We can select some text
embeddings as basic prototypes to lead the learning. However, in addition to the alignment, we still
need to consider issues of prototype selection, differentiation Meng et al. (2023c), uniformity Wang
& Isola (2020), stability Huang et al. (2023) and etc.
3 M ETHODS
TEST has two key steps: In Figure 1, build an encoder to embed TS; In Figure 2, create prompts to
make the LLM can accept TS embeddings as input.
3Published as a conference paper at ICLR 2024
EncoderInstance contrastText prototypeAnchorPositiveNegativeAugmentationProjectorFeature contrast
Text alignmentSimilarity
Value19ShapeupdownFrequencyhighlowFrequencyValueShape
ValueFrequencyShapeDecoderAutoencodingFeature matrix
Figure 1: Text-prototype-aligned TS Embedding by Instance-wise and Feature-wise Contrast
3.1 TS T OKEN AUGMENTATION AND ENCODING
Definition 1 (Token Embedding of Time Series) A multivariate time series x={xd
t}T,D
t=1,d=1has
Dvariables and Ttime points. It can be segmented to a list of Knon-overlapping subsequences
s={sk}K
k=1by a segmentation function fs:x→s, where the length of sk=xti:tjis arbitrary,
1≤ti< tj≤T. We call sas the token list of time series x. Further, each token can be embeded
to aM-dimensional representation space by an embedding function fe:sk∈RD×T→ek∈RM.
Finally, the token embedding list of xise={ek}K
k=1=fe(s) =fe(fs(x)).
We first tokenize TS into some segmentation/subsequences/tokens/instances through the classical
sliding window method in representation learning Yue et al. (2022) s=fs(x). We define a TS
token sas the anchor instance. Its positives s+are the augmented instances, sweak∼ Tweak (jitter-
and-scale strategy, adding random variations to the signal and scale up its magnitude), sstrong∼
Tstrong (permutation-and-jitter strategy, splitting the sequence into a random number of segments
and randomly shuffling them) Eldele et al. (2021b). Its negatives s−are from non-overlapping
instances which do not have the same subsequence as s.
After getting anchor-positive-negative, we built a neural network as the encoder to embed in-
stance into vector e=fe(s). We also trained a decoder fdby using the auto-encoding loss
Lae=1
NPN
i=1sim(s, fd(e))to ensure the representativeness of the embedding and subsequent
verification. Because our primary goal is to retrieve the encoder, this decoder can likewise be un-
built without harming the future process.
3.2 I NSTANCE -WISE AND FEATURE -WISE CONTRAST
The basic instance-wise CL treats each instance independently and design the instance discrimina-
tion pretext task to keep similar instances close and dissimilar instances far away. To prevent embed-
ding space collapse, we treat augmented views of the same instance as the unique positive pair, and
all remaining ones within the Bsize minibatch as negative pairs He et al. (2020). The instance-wise
contrastive loss is shown in Equation 1. Where given the instance embedding e, e+/−, we construct
a projection head fp, which is a one-layer MLP to obtain fp(e).σ(e, e+/−)is used to calculate
the similarity between two projected vectors through a similarity function simlike cosine similarity
with the instance-level temperature parameter τ.
Lins=−logexp(σ(e, e+))
exp(σ(e, e+)) +PB
i=1exp(σ(e, e−
i))
σ(e, e+/−) =sim(fp(e), fp(e+/−))
τ(1)
4Published as a conference paper at ICLR 2024
We also propose a feature-wise contrast method to break the independence between instances. As
shown in Figure 1, after embedding, a feature matrix RB×Mis formed by the representation vectors
of instances in a minibatch. Where each row is an embedding of a instance, thus rows could be
regarded as soft labels of instances which are used in Equation 1. In addition to rows, columns of
feature matrix also have semantic information. Li et al. (2021c) proposed that the columns could
be further regarded as cluster representations. However such cluster-wise methods require prior
knowledge to pre-specify the number of clusters, which is non-trivial for the unlabeled TS data
in this work. Thus, we propose to regard the columns as the soft labels of features and perform
discrimination between groups of similar features.
For an anchor feature matrix m, where mis the B-th row copy of the vector e, we obtain a positive
feature matrix m+and a negative feature matrix m−, where m+/−= [ei]B
i=1∈RB×M. We mark
the columns in the matrix as m∈mT. As expressed by the item before the right arrow in the
Equation 2, the feature-wise contrast mainly align and differentiate the same feature column among
the positive and negative. However, this may cause the representation space to shrink within a small
area. We find that ensuring differences between features can better address this issue. That is, we
suggest the contrast between different feature columns as shown in the item after the right arrow.
Lfea=−MX
i=1(σ(mi, m+
i)|{z}
Alignment−σ(mi, m−
i)|{z}
Difference)⇒ −MX
i=1logexp(σ(mi, m+
i))PM
j=1[exp(σ(mi, m+
j)) + exp( σ(mi, m−
j))]
| {z }
Feature category uniformity(2)
More importantly, the injection of feature column differences can also greatly assist in the sub-
sequent implementation of text-prototype-aligned contrast. Because that contrast will apply the
selected text token embedding to the feature columns, like coordinate axes.
3.3 T EXT-PROTOTYPE -ALIGNED CONTRAST
The pre-trained LLM has its own token embedding, e.g., small, medium, and big GPT-2 embed
text tokens from word dictionaries into representation spaces with 768, 1024, and 1280 dimensions.
Naively, we can align the token embedding of TS and text using the similarity estimation. Although
TS tokens lack text annotation, we can place their embedding near typical text descriptions of TS,
such as value, shape, and frequency. In this fashion, it is intuitively expected that various TS tokens
can represent various descriptive terms such as small, big, up, down, stable, fluctuating, and so
on. Naturally, the example above is based on the closest neighbor principle because the embedding
space of a text token is discrete, akin to a vector table, but that of our TS token is continuous.
However, of course, the actual outcomes will not match what we expect because we are not providing
the supervised label or ground truth. For example, the embedding of a subsequence with an upward
trend may be very close to that of a decline word, or even that does not describe the trend. But it
is irrelevant whether semantics can be understood by us. As usual, the fact is that humans cannot
comprehend the model’s perceptual mode.
Recently, researchers proved that LLMs are pattern machines Mirchandani et al. (2023). Thus, in
this work, we achieve “TS →pattern →text” to activate LLM’s ability for TS tasks. The choice of
text prototype can be relaxed, not necessarily the description related to TS.
In this work, we choose Prepresentative text embedding tpas pivots/prototypes, and map TS em-
bedding to them. In high dimensional space, almost all vectors are pairwise orthogonal Hopcroft &
Kannan (2013), thus the number of prototypes rather than the type does matter, and their differences
can be reflected in a single dimension/feature. Thus, the modeling function of the text prototype tp
is realized by feature-wise contrast. As expressed by Equation 3, the alignment term guarantees that
the two space ranges are roughly the same through the similarity constraint, the contrast term uses tp
as the coordinate axis to map the TS embedding, making the representation values in text coordinate
axes of similar instance similar. The feature matrix is no longer obtained through the projector but
through the prototype mapping e·tp→m.
Ltext=−PX
i=1[sim(tpi, e)|{z}
Text alignment−Lfea(e·tp, e+·tp, e−·tp)]| {z }
Text contrast(3)
5Published as a conference paper at ICLR 2024
3.4 L EARNABLE PROMPT EMBEDDING
Even TS has been described using an embedded representation that the LLM can understand, LLM
still has to be instructed on how to do subsequent TS tasks.
TokenEncoderLanguage model
TS embedding…
QuestionAnswerTrainDecoderFine-tuneFine-tuneTrainable layerSoft promptClassifierClassificationRegression[cls]
Figure 2: Framework of LLM for TS TasksPrompt engineering like template and
chain-of-thought is intuitive. Their con-
texts are coherent in human semantics, but
a TS embedding list has no human se-
mantics, it is more about a pattern se-
quence. Thus, to create a more consistent
prompt pattern, we train a soft prompt by
p-tuning Lester et al. (2021) make LLM
be easier to understand the input. These
soft prompts are task-specific embedding,
learning through the loss from LLM’s out-
put and task ground truth in Equation 4.
Lpromp =Lreg/cls (concat( pe, e))(4)
GPT4TS Zhou et al. (2023)has proved the
feasibility that SFT can make LLM apply
to TS. Based on this, we demonstrate the
feasibility of TEST by proving the equiva-
lence between soft prompt and SFT.
Consider a conditional generation task
where the input xis a context and the output yis a sequence of tokens. Assume an autoregres-
sion LLM pϕ(y|x)with parameter ϕ,z= [x;y]. The inference of a pre-trained LLM is computing
hias a function of ziand the past activations in its left context, Y=LM ϕ(zi, hi). The past hiin
the soft prompt turning with prompt peθishi=peθ[i,:], ifi∈peidx
LM ϕ(zi, hi),otherwise. The SFT from LLM
to TS-LLM is Equation 5. Its transformation shows that the soft prompt tuning is approximately
equivalent to SFT.
max
ϕpϕ(y′|x) = max
ϕX
i∈Yidxlogpϕ(z′
i|h<i) =X
i∈Yidxlogpϕ+∆(zi+δzi|h<i)
≈X
i∈Yidxlogpϕ(zi|h<i)·X
i∈peidxlogp∆(δzi|h<i)
=X
i∈Yidxlogpϕ(zi| fe(s)|{z}
Text−TS alignment| {z }
Frozen LLM)·X
i∈peidxlogp∆(δzi|h<i)
| {z }
Prompt peθ(5)
Equation 5 also suggests that the projection space of TS tokens should preferably cover the complete
set of text embedding space. Thus, we utilize clustering to find Prepresentative text prototypes. The
process of using LLM to infer TS is shown in Figure 2. In this framework, the text data is input into
the embedding layer of LLM, while the prompts and TS embeddings skip this layer.
4 E XPERIMENTS
The core of TEST is to train an encoder feand a soft prompt peas described in Algorithm 1. The
encoder must can extract relevant information from TS, needs to be time- and memory-efficient,
and has to allow variable-length inputs. Thus, we build a causal TCN with 10 layers of convolution
blocks. Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, Dilated-
Conv, with skip connections across each block. The DilatedConvs have dilation of 2iin each layer
iof convolution block. A final convolution block is used to map the hidden channels to the output
channel whose size is the same as the LLM’s embedding size.
6Published as a conference paper at ICLR 2024
Algorithm 1 Training TEST
1:fore in epochs do
2: // U PDATE ENCODER
3: θfe=θfe−η▽θfe(Lins+Ltext)
4: // U PDATE DECODER (OPTIMAL )
5: θfd=θfd−η▽θfdLae
6: // U PDATE PROJECTOR
7: θfp=θfp−η▽θfpLins
8:end for9:fore in epochs do
10: // U PDATE PROMPT
11: pe=pe−η▽θpeLpromp
12: // F INE TUNE DECODER (OPTIMAL )
13: θfd=θfd−η′▽θfdLreg
14: // U PDATE CLASSIFIER (OPTIMAL )
15: θfc=θfc−η▽θfcLcls
16:end for
Model Size Embed. dimension
Bert Devlin et al. (2018) 110M, 335M 748, 1024
GPT2 Radford et al. (2019) 117M, 345M, 774M 768, 1024, 1280
ChatGLM Du et al. (2022) 6B 4096
LLaMa2 Touvron et al. (2023) 7B, 13B 4096
Table 2: The Used Language ModelThe used LLMs are as listed
in Table 2. Each encoder and
soft prompt of LLM are trained
using the Adam optimizer on
20 NVIDIA Tesla V100-SXM2
GPU with CUDA 11.3.
We compare our method to 5 kinds of methods including 12 baselines: 1) LLM-QA methods Xue &
Salim (2023); Liu et al. (2023) with the classification template Classify the given [domain] sequence
as either [class label] or [class label]: [numerical sequence]. [A] and the forecasting template [Q]
Forecast the next value of the given [domain] sequence: [numerical sequence]. [A] ; 2) SFT LLM-
for-TS method GPT4TS Zhou et al. (2023); 3) classical TS models DWT, DWTD Bagnall et al.
(2018), 1NNED, and TCN Tan et al. (2021); 4) SOTA TS models Informer Zhou et al. (2021),
DLinear Zeng et al. (2023), and TimesNet Wu et al. (2023); 5) SOTA CL-based TS models Tloss
Franceschi et al. (2019b), TS2Vec Yue et al. (2022), and CoST Woo et al. (2022a).
The overall results are shown in Figure 3 (The appendix has more compared classical SOTA models
and detailed results about long-term, short-term, few-shot, and zero-shot forecasting, multivariate
time series classification, and representation tasks.). Overall, after using TEST , when the size of
LLM reaches about 300M, their accuracy comparable to SOTA model.
4.1 C LASSIFICATION
We present accuracy scores for all 128 kinds of univariate TS datasets in UCR archive Dau et al.
(2019) and all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al. (2018).
Accuracy. In Figure 3 (a-b), TEST makes the classification accuracy of LLM increase significantly.
LLM’s original classification performances are demonstrated through two QA results. It almost
guesses the classification labels at random, especially for multivariate TS. After using TEST , GPT2-
774M, which has the median accuracy among all models, can improve accuracy by at least 18% for
univariate TS and 25% for multivariate TS. TEST makes most LLMs comparable to, if not better
than, the existing models. When the size reaches about 300M, the accuracy can exceed TS baselines;
When the size reaches about 700M, the accuracy can exceed SOTA TS transformers.
Ablation. In Figure 3 (c-d), different text prototypes will lead to different results. We set 3 groups of
text prototypes: embeddings of value ,shape ,frequency , and embeddings of 3 or 10 cluster centers.
Choosing a prototype group that more accurately represents LLM’s entire text embedding space can
improve the performance. This is also suggested by Equation 5. Different prompt types, initializa-
tion, and length will lead to different results. We compare the soft prompt with the hard prompt of
Classify the given [domain] sequence as either [class label] or [class label]: [TS embedding] . The
accuracy differs by at least 10%. We set random initialization from uniform distribution and task
description initialization from Classify the given sequence . The latter makes the training converge
faster. When the model reaches 1B, a prompt length of 10 can achieve excellent results.
4.2 F ORECASTING
We present short-forecasting MSE scores for all 19 kinds of varied time series datasets in TSER
archive Tan et al. (2021), and long-forecasting MSE scores for 8 popular real-world benchmark
datasets including weather, traffic, electricity, ILI, and ETT from Wu et al. (2023).
7Published as a conference paper at ICLR 2024
Univariate ClassificationMultivariate ClassificationShot-term ForecastingLong-term ForecastingFew-shot ForecastingGeneral ForecastingRepresentationTEST (GPT2-774M)TimesNetDLinear InformerTCNTS2Vec
LLM+TEST (ours)ClassicalSOTA modelsQACL models
LLM+TEST (ours)ClassicalSOTA modelsQACL models𝑏LLM+TEST (ours)ClassicalSOTA modelsQA
LLM+TEST (ours)SOTA modelsQASFT𝑒
𝑓
UCR classification accuracyUCR classification accuracy𝑐LLM+TEST (ours)ClassicalSOTA models
LLM+TEST (ours)SOTA modelsQASFTℎ
Embedding before / after inputting LLM (ours)  + SVMCL models + SVM𝑎
𝑖𝑑𝑔LLM+TEST (ours)ClassicalSOTA modelsQACL modelsUCR classification accuracy using representationUCR classification accuracy
Figure 3: Experiment Results. (a-d) shows the classification results; (e-h) shows the forecasting
results; (i) shows the representation results. The red dashed line represents the best result.
Accuracy. In Figure 3 (e-f), TEST makes the forecasting accuracy of LLM increase significantly
and comparable to SOTA models. When the size reaches about 300M, the accuracy can exceed
SOTA TS transformers.
Generalization. We fuse 19 datasets into 1 dataset and test the method on this fused dataset. As
shown in Figure 3 (g), compared with baselines, LLM-based models have better generality.
Few-shot. LLM has demonstrated remarkable performance in few-shot learning. Based on the
settings in Zhou et al. (2023), we present few-shot forecasting for 10% time steps in training datasets.
As shown in Figure 3 (h), TEST achieves the best performance and demonstrates a relative average
MSE reduction of 23.5%.
8Published as a conference paper at ICLR 2024
4.3 R EPRESENTATION
Active silence silent absent important final night voicedWhite importantchange loop happy actively limit finally
Figure 4: Matching TS Embedding to WordsRepresentation learning. Learning universal rep-
resentations for TS is a fundamental but challeng-
ing problem. Both TEST ’s first step (creating TS
embedding) and second step (LLM’s output) can
achieve this task. Based on the classical representa-
tion learning task, we evaluated the effectiveness of
TEST representation using SVM classifier on UCR
dataset. Note that using a simple classifier can bet-
ter reflect the presentation effect. In Figure 3 (i),
the embedding in TEST ’s first step is comparable to
SOTA representation methods, and the embedding
inTEST ’s second step can outperform them. This
indicates that after using LLM, the representation of
TS becomes more discriminative.
Case. We use nearest neighbor method to find the text that a TS token matches to in the word
embedding space of frozen LLM. In Figure 4, the majority of the identified words are sentiment-
related adjectives and nouns. We speculate that by prompting, the model will treat TS classification
task as an sentiment classification task. Thus, introducing prompt is like introducing a shortcut
for LLM. Besides, the matched words are like a kind of textual Shapelet for TS segmentation,
representing TS through a series of patterns. Instead of regarding TS as a sequence of numbers,
we suggest using words to identify patterns in TS as LLMs without SFT are not good for math
when performing digital tasks, but they are good at extracting knowledge as a pattern machine. The
semantics of the patterns be perplexing to us, but it makes sense to LLM.
5 D ISCUSSION AND CONCLUSION
This paper proposes an instance-wise, feature-wise, and text-prototype-aligned TS embedding
method to achieve TS-for-LLM. It can activate LLM’s ability for TS tasks while maintaining its
original language ability. Experiments on classification, forecasting, and representation tasks show
that using TEST , LLM can archive comparable performance to SOTA methods.
TS-for-LLM can enrich LLM’s capabilities. SFT LLM may be more effective than TS-for-LLM,
yet its superiority over customized TS models remains unclear; Training customized models may be
more accurate in TS tasks, yet TS-for-LLM offers all notable benefits of LLM additionally.
TS-for-LLM can explore LLM’s mechanism as a pattern machine. The essence of TS-for-LLM
is: TS ↔TS embeddings ↔patterns ↔text/word embedding ↔text. Although TEST gives the
impression of a forcibly aligning operations between TS and text, it dose convert TS into an under-
standable pattern sequence for LLMs, that clearly demonstrates that the essence of LLM is pattern
recognition. In fact, TS is objective data, whereas images, text, and speech are subjective data that
can be perceived by human senses. TEST aligns objective TS data and subjective text data at the
machine level, but how to align them at the human perception level requires future research.
Meanwhile, in addition to text prototypes and prompts, LLM size and type also affect the results.
The impact of model type is intuitive, it is related to downstream tasks, where the bidirectional
structure is beneficial for classification, and the generated structure is beneficial for forecasting. The
impact of model size, where a larger model produces more accurate results, can be attributed to
various reasons. Aside from the impact of additional parameters, we believe that the datasets used
in the pre-training process are also important, with the size, diversity, and corpus type all having an
impact. We conjecture that more training data will provide the model with more opportunities to
learn temporal patterns. As a result, we intend to conduct more experiments to investigate deeper
correlations between corpora and TS data Chen et al. (2023).
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of China (No.62172018,
No.62102008) and Wuhan East Lake High-Tech Development Zone National Comprehensive Ex-
perimental Base for Governance of Intelligent Society.
9Published as a conference paper at ICLR 2024
REFERENCES
Anthony J. Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom,
Paul Southam, and Eamonn J. Keogh. The UEA multivariate time series classification archive,
2018. CoRR , abs/1811.00075, 2018.
Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-
range global weather forecasting with 3d neural networks. Nature , pp. 1476–4687, 2023. doi:
10.1038/s41586-023-06545-z.
Aaron Bostrom, Anthony Bagnall, Eamonn Keogh, Hoang Anh Dau, James Large, Jason Lines,
Michael Flynn, and Paul Southam. The uea multivariate time series classification archive, 2018,
2018.
Eoin Brophy, Zhengwei Wang, Qi She, and Tom ´as Ward. Generative adversarial networks in time
series: A systematic literature review. ACM Comput. Surv. , 55(10):199:1–199:31, 2023.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems , 2020a.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. 2020b.
CDC. Illness. 2021. doi: https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html.
Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. LLM4TS: two-stage fine-tuning for time-series
forecasting with pre-trained llms. CoRR , abs/2308.08469, 2023.
Daoyuan Chen, Yilun Huang, and et al. Data-juicer: A one-stop data processing system for large
language models. CoRR , abs/2309.0203, 2023.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In Proceedings of International Conference on
Machine Learning , volume 119, pp. 1597–1607, 2020.
Hyunseung Chung, Jiho Kim, Joon-Myoung Kwon, Ki-Hyun Jeon, Min Sung Lee, and Edward
Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports. In
IEEE International Conference on Acoustics, Speech and Signal Processing , pp. 1–5, 2023.
Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh
Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive.
IEEE/CAA Journal of Automatica Sinica , 6:1293–1305, 2019. doi: 10.1109/JAS.2019.1911747.
Angus Dempster, Daniel F. Schmidt, and Geoffrey I. Webb. Minirocket: A very fast (almost) de-
terministic transform for time series classification. In ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , pp. 248–257, 2021. doi: 10.1145/3447548.3467231.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR , abs/1810.04805, 2018.
Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm:
A simple pre-training framework for masked time-series modeling. CoRR , abs/2302.00861, 2023.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:
General language model pretraining with autoregressive blank infilling. In Proceedings of Annual
Meeting of the Association for Computational Linguistics , volume 1, pp. 320–335, 2022.
Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and
Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In
Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence , pp. 2352–
2359, 2021a.
10Published as a conference paper at ICLR 2024
Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and
Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In
International Joint Conference on Artificial Intelligence , pp. 2352–2359, 2021b.
Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation
learning for multivariate time series. In Advances in Neural Information Processing Systems , pp.
4652–4663, 2019a.
Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation
learning for multivariate time series. In Advances in Neural Information Processing Systems , pp.
4652–4663, 2019b.
Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. A reinforcement learning-informed pat-
tern mining framework for multivariate time series classification. In Proceedings of International
Joint Conference on Artificial Intelligence , pp. 2994–3000, 2022. doi: 10.24963/IJCAI.2022/415.
Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo ´Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, R ´emi Munos, and Michal Valko. Bootstrap your own latent - A
new approach to self-supervised learning. In Advances in Neural Information Processing Systems ,
2020.
Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-
shot time series forecasters. CoRR , abs/2310.07820, 2023. doi: 10.48550/ARXIV .2310.07820.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. In Computer Vision and Pattern Recognition , pp.
9726–9735, 2020.
Shenda Hong, Hongyan Li, Chenxi Sun, and Junyuan Shang. Research and applications of extracting
computational phenotype from vital sign time series. China Seience and Technology Achivements ,
10, 2023. doi: 10.3772/j.issn.1009-5659.223.10.002.
John Hopcroft and Ravindran Kannan. Computer science theory for the information age . Cambridge
University press, 2013.
Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clus-
tering via prototype scattering and positive sampling. IEEE Trans. Pattern Anal. Mach. Intell. , 45
(6):7509–7524, 2023. doi: 10.1109/TPAMI.2022.3216454.
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y . Zhang, Xiaoming Shi, Pin-Yu Chen,
Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting
by reprogramming large language models. CoRR , abs/2310.01728, 2023. doi: 10.48550/ARXIV .
2310.01728.
Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-fcns
for time series classification. Neural Networks , 116:237–245, 2019. doi: 10.1016/J.NEUNET.
2019.04.014.
Salar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. SLIC: self-supervised learning with
iterative clustering for human action videos. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 16070–16080, 2022. doi: 10.1109/CVPR52688.2022.01562.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
International Conference on Learning Representations , 2020.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. In Proceedings of Conference on Empirical Methods in Natural Language Processing ,
pp. 3045–3059, 2021. doi: 10.18653/v1/2021.emnlp-main.243.
Guozhong Li, Byron Choi, Jianliang Xu, Sourav S. Bhowmick, Kwok-Pan Chun, and Grace Lai-
Hung Wong. Shapenet: A shapelet-neural network approach for multivariate time series
classification. In AAAI Conference on Artificial Intelligence , pp. 8375–8383, 2021a. doi:
10.1609/AAAI.V35I9.17018.
11Published as a conference paper at ICLR 2024
Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model helps
ecg zero-shot learning. In Medical Imaging with Deep Learning , pp. 402–415, 2024.
Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. Prototypical contrastive learning of
unsupervised representations. In International Conference on Learning Representations , 2021b.
Yunfan Li, Peng Hu, Jerry Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive
clustering. In AAAI Conference on Artificial Intelligence, , pp. 8547–8555, 2021c.
Xin Liu, Daniel McDuff, Geza Kovacs, Isaac R. Galatzer-Levy, Jacob E. Sunshine, Jiening Zhan,
Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak N. Patel. Large language models are
few-shot health learners. CoRR , abs/2305.15525, 2023. doi: 10.48550/arXiv.2305.15525.
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring
the stationarity in time series forecasting. In Advances in Neural Information Processing Systems ,
2022.
Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, and James T.
Kwok. A survey on time-series pre-trained models. CoRR , abs/2305.10716, 2023. doi: 10.48550/
arXiv.2305.10716.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. MHCCL:
masked hierarchical cluster-wise contrastive learning for multivariate time series. CoRR ,
abs/2212.01141, 2022.
Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, and Zhiqi Shen. MHCCL:
masked hierarchical cluster-wise contrastive learning for multivariate time series. In AAAI Con-
ference on Artificial Intelligence , pp. 9153–9161, 2023a. doi: 10.1609/aaai.v37i8.26098.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised
representation learning for time series: A review. CoRR , abs/2308.01578, 2023b. doi: 10.48550/
arXiv.2308.01578.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised
representation learning for time series: A review. CoRR , abs/2308.01578, 2023c.
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Are-
nas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern
machines. In Conference on Robot Learning , volume 229 of Proceedings of Machine Learning
Research , pp. 2498–2518, 2023.
Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth
64 words: Long-term forecasting with transformers. In International Conference on Learning
Representations , 2023.
Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis
expansion analysis for interpretable time series forecasting. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 , 2020.
PeMS. Traffic. 2021. doi: http://pems.dot.ca.gov/.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI , 2019.
Patrick Sch ¨afer and Ulf Leser. Multivariate time series classification with WEASEL+MUSE. CoRR ,
abs/1711.11343, 2017.
Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, and Rainer Stiefelhagen. Clustering based
contrastive learning for improving face representations. In IEEE International Conference on Au-
tomatic Face and Gesture Recognition , pp. 109–116, 2020. doi: 10.1109/FG47880.2020.00011.
Taylor SJ and Letham B. Forecasting at scale. In PeerJ Preprints , pp. 5:e3190v2, 2017. doi:
10.7287/peerj.preprints.3190v2.
12Published as a conference paper at ICLR 2024
Chenxi Sun, Shenda Hong, and et al. A review of deep learning methods for irregularly sampled
medical time series data. CoRR , abs/2010.12493, 2020. doi: 10.48550/arXiv.2010.12493.
Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Time series extrinsic
regression. Data Mining and Knowledge Discovery , pp. 1–29, 2021. doi: https://doi.org/10.1007/
s10618-021-00745-9.
Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for
time series with temporal neighborhood coding. In International Conference on Learning Repre-
sentations , 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and et al. Llama 2:
Open foundation and fine-tuned chat models. CoRR , abs/2307.09288, 2023.
A¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR , abs/1807.03748, 2018.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In Proceedings of International Conference on Machine
Learning , volume 119, pp. 9929–9939, 2020.
Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang,
Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive
survey. Mach. Intell. Res. , 20(4):447–482, 2023. doi: 10.1007/s11633-022-1410-8.
Wetterstation. Weather. 2017. doi: https://www.bgc-jena.mpg.de/wetter/.
Kristoffer Wickstrøm, Michael Kampffmeyer, Karl Øyvind Mikalsen, and Robert Jenssen. Mixing
up contrastive learning: Self-supervised representation learning for time series. Pattern Recognit.
Lett., 155:54–61, 2022.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive
learning of disentangled seasonal-trend representations for time series forecasting. In Interna-
tional Conference on Learning Representations , 2022a.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive
learning of disentangled seasonal-trend representations for time series forecasting. In The Inter-
national Conference on Learning Representations , 2022b.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Expo-
nential smoothing transformers for time-series forecasting. CoRR , abs/2202.01381, 2022c.
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-
formers with auto-correlation for long-term series forecasting. In Advances in Neural Information
Processing Systems , pp. 22419–22430, 2021.
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:
Temporal 2d-variation modeling for general time series analysis. In International Conference on
Learning Representations , 2023.
Hao Xue and Flora D. Salim. Promptcast: A new prompt-based learning paradigm for time series
forecasting. CoRR , abs/2210.08964, 2023.
Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilin-
ear temporal-spectral fusion. In International Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pp. 25038–25054, 2022.
Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. Timeclr: A self-supervised contrastive learning
framework for univariate time series representation. Knowl. Based Syst. , 245:108606, 2022.
Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial net-
works. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada , pp. 5509–5519, 2019.
13Published as a conference paper at ICLR 2024
Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and
Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI Conference on
Artificial Intelligence , pp. 8980–8987, 2022.
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series
forecasting? In AAAI Conference on Artificial Intelligence , pp. 11121–11128, 2023. doi: 10.
1609/aaai.v37i9.26317.
George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff.
A transformer-based framework for multivariate time series representation learning. In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 2114–2124, 2021. doi:
10.1145/3447548.3467401.
Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, Kathleen R. McKeown,
Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. Supporting clustering with contrastive
learning. In Proceedings of Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pp. 5419–5430, 2021. doi:
10.18653/V1/2021.NAACL-MAIN.427.
Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. Tapnet: Multivariate time series
classification with attentional prototypical network. In AAAI Conference on Artificial Intelligence ,
pp. 6845–6852, 2020. doi: 10.1609/AAAI.V34I04.6165.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,
Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-
Rong Wen. A survey of large language models. CoRR , abs/2303.18223, 2023. doi: 10.48550/
arXiv.2303.18223.
Xiaochen Zheng, Xingyu Chen, Manuel Sch ¨urch, Amina Mollaysa, Ahmed Allam, and Michael
Krauthammer. Simts: Rethinking contrastive representation learning for time series forecasting.
CoRR , abs/2303.18205, 2023.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI Con-
ference on Artificial Intelligence , pp. 11106–11115, 2021. doi: 10.1609/aaai.v35i12.17325.
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency
enhanced decomposed transformer for long-term series forecasting. In International Conference
on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 27268–
27286, 2022.
Tian Zhou, PeiSong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all:power general time
series analysis by pretrained lm. In Conference and Workshop on Neural Information Processing
Systems , 2023.
Rundong Zuo, Guozhong Li, Byron Choi, Sourav S. Bhowmick, Daphne Ngar-yin Mah, and
Grace Lai-Hung Wong. SVP-T: A shape-level variable-position transformer for multivariate time
series classification. In AAAI Conference on Artificial Intelligence , pp. 11497–11505, 2023. doi:
10.1609/AAAI.V37I9.26359.
14Published as a conference paper at ICLR 2024
A A PPENDIX
A.1 R ELATED WORK
Our work mainly involves two research fields: Universal Representation Learning (URL) for time
series based on Contrastive Learning (CL) and Large Language Model (LLM) + Time Series (TS).
A.1.1 CL- BASED URL FOR TS
Unsupervised URL approaches aim to learn discriminative feature representations from unlabeled
data, without the requirement of annotating every sample. Enabling URL is extremely crucial for
time series data, due to its unique annotation bottleneck caused by its complex characteristics and
lack of visual cues compared with other data modalities.
Contrastive methods learn meaningful representations from time series by optimizing self-
discrimination tasks. Instead of directly modeling the complex raw data, they employ pretext
tasks that leverage the underlying similarity between samples, which eliminates the need for re-
constructing the complete input and allows for the discovery of contextualized underlying factors of
variations. Contrastive methods typically generate augmented views of the raw data through vari-
ous transformations and then learn representations by contrasting positive samples against negative
samples.The existing CL-based URL for TS are listed in Table 4.
Instance-level contrastive models treat individual samples independently for the purpose of instance
discrimination. They utilize data augmentations to transform original inputs into a new embedding
space. Within this space, augmentations derived from the same sample are considered as positive
pairs, while those from different samples are treated as negative pairs. During training, these models
are optimized by maximizing the similarity between representations of positive pairs, while simul-
taneously minimizing the similarity between representations of negative pairs.
Prototype-level contrastive models break the independence between samples and explore to exploit
the implicit semantics shared by samples in the same cluster. They can address the limitation that
instance-level contrastive learning models tend to treat semantically similar samples as negatives.
Temporal-level contrastive models instead focus on capturing scale- invariant representations at each
individual timestamp. By cosidering both instance-level and temporal-level representation learning
strategies, researchers aim to enhance the capability of contrastive learning methods in capturing the
complexities inherent in time series data.
A.1.2 LLM+TS
Large models, specifically referred to as large language models (LLMs) and pre-trained foundation
models (PFMs), have witnessed remarkable success across a multitude of tasks and domains, such
as natural language processing (NLP), computer vision (CV). Given the remarkable achievements of
large models in these diverse fields, an intriguing question emerges: can large models be effectively
employed to analyze TS data?
TS data has long been studied and proven to be indispensable in a myriad of real-world applica-
tions, encompassing fields such as geoscience, transportation, energy, healthcare, environment, and
Category Pros Cons Methods
Reconstruction-based Disregard insignificant data Collapse of embedding space; TimeNetWu et al. (2023)
that may contain noise Unable to measure feature relations SimMTM Dong et al. (2023)
Adversarial Eliminate the need for expensive Difficulty in model convergence; TimeGAN Yoon et al. (2019)
manual labeling Unable to measure feature relations TS-GAN Brophy et al. (2023)
Predicative Self-supervised Affected by noise TST Zerveas et al. (2021)
TS-TCCEldele et al. (2021a)
Contrastive Self-supervised Different datasets require different Table 4
data augmentation methods and
similarity evaluations
Table 3: Representation Learning Methods of Time Series Methods
15Published as a conference paper at ICLR 2024
Type Methods
Instance-level SimCLR Chen et al. (2020) TimeCLR Yang et al. (2022) MoCo He et al. (2020) BYOL Grill et al. (2020)
CPC van den Oord et al. (2018) SimSiam Zheng et al. (2023) MCL Wickstrøm et al. (2022)
Prototype-level SwA V Caron et al. (2020b) PCL Li et al. (2021b) CCL Sharma et al. (2020) SCCL Zhang et al. (2021)
CC Li et al. (2021c) SLIC Khorasgani et al. (2022) MHCCL Meng et al. (2022)
Temporal-level TS2Vec Yue et al. (2022) TS-TCC Eldele et al. (2021b) TNC Tonekaboni et al. (2021) TCL
T-Loss Franceschi et al. (2019b) BTSF Yang & Hong (2022) CoST Woo et al. (2022a)
Table 4: Contrastive Learning based Universal Representation Methods for Time Series
Means Pros Cons Work
TrainingSpecialized, Not universal, Pre-training Ma et al. (2023)
accurate large datasets Earth transformer Bi et al. (2023)
TS Transformers Wu et al. (2023)
TuningEnd-to-end, More experiments, GPT4TSZhou et al. (2023)
accurate lose language ability LLM4TSChang et al. (2023)
LLMTime Gruver et al. (2023)
Time-LLM Jin et al. (2023)
Tool AugmentedParameter-efficient,
less experimentsNeed experts,
need annotationPromptCast Xue & Salim (2023)
Health Learner Liu et al. (2023)
METS Li et al. (2024)
Text2ECGChung et al. (2023)
External EncoderParameter-efficient, Weak robust TEST
multiple abilities
Table 5: Existing Work about TS+LLM
Figure 5: Technical Route of LLM+TS
finance. While large models have made significant progress in various fields, the arena of time se-
ries analysis has followed a more gradual path. Traditional analytical methods have predominantly
relied on statistical models. The advent of deep learning has galvanized the research community to
explore more potent data-driven models, typically built on the basis of Recurrent Neural Networks
(RNNs), Convolutional Neural Networks (CNNs), and Transformers. Nonetheless, the majority of
these models remain relatively small in scale and are tailored for specific tasks, thereby lacking the
capacity to acquire comprehensive semantic and knowledge representations from large-scale data
for multi-task reasoning.
There hasn’t been much research done on TS+LLM because this field is still in its infancy. We
summarize the existing work in Table 5. Different from the main text, we category work here
through technical means.
16Published as a conference paper at ICLR 2024
A.2 M ODEL
https://github.com/SCXsunchenxi/TEST
A.2.1 E NCODER
The core of TEST is to train an encoder and a soft prompt. The encoder must can extract relevant
information from TS, needs to be time- and memory-efficient, and has to allow variable-length
inputs. Thus, as shown in Figure 6, we build a causal TCN with 10 layers of convolution blocks.
Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, DilatedConv,
with skip connections across each block. The DilatedConvs have dilation of 2iin each layer iof
convolution block. A final convolution block is used to map the hidden channels to the output
channel whose size is the same as the LLM’s embedding size.
The detailed architecture is: Number of channels in the intermediary layers of the causal network
is40; Number of layers (depth of the causal network) is 10; Kernel size of all convolutions is
3; Negative slope of the leaky ReLU activation is 0.01; Number of output channels of the causal
network (before max pooling) is 640; Dimension of the representations is the same as the LLM’s
embedding size (e.g. 1024 for gpt2).
Figure 6: Illustration of Three Stacked Dilated Causal Convolutions and Composition of the i-th
Layer of The Chosen Architecture
We train our models with the following parameters for time series classification. Note that no hy-
perparameter optimization was performed on the encoder hyperparameters: Optimizer is Adam
with learning rate α= 0.001and decay rates β= (0.9,0.999) ; Number of negative samples is
K∈ {1,2,5,10}for for univariate time series, K∈ {5,10,20}for multivariate ones; Batch size is
10; Number of optimizations steps is 2000 forK≤10(i.e., 20epochs for a dataset of size 1000 ),
1500 otherwise.
A.2.2 LLM
The used LLMs are as listed in Table 6. Each encoder and soft prompt of LLM are trained using the
Adam optimizer on 20 NVIDIA Tesla V100-SXM2 GPU with CUDA 11.3.
A.3 F ORECASTING TASKS
All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB
GPUs. We use mean square error (MSE) and mean absolute error (MAE) as metrics. For zero-
shot learning, mean absolute percentage error (MAPE) is used for TOURISM; symmetric MAPE
(sMAPE) is used for M3 and M4; normalized deviation (ND) is used for ELECTR. All experiments
are repeated 3 times and the mean of the metrics is used in the final results.
17Published as a conference paper at ICLR 2024
Model Size Embed. dimension
Bert Devlin et al. (2018) 110M, 335M 748, 1024
GPT2 Radford et al. (2019) 117M, 345M, 774M 768, 1024, 1280
ChatGLM Du et al. (2022) 6B 4096
LLaMa2 Touvron et al. (2023) 7B, 13B 4096
Table 6: The Used Language Model
A.3.1 D ATASET DETAILS
The details of long-term forecasting and few-shot forecasting datasets are: ETT datasets Zhou et al.
(2021) contain electricity load of various resolutions (ETTh & ETTm) from two electricity stations;
Weather datasetWetterstation (2017) contains 21 meteorological indicators of Germany within 1
year; Illness datasetCDC (2021) contains the influenza-like illness patients in the United States.
ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition
of few-shot; Electricity dataset SJ & B (2017) contains the electricity consumption; Traffic dataset
PeMS (2021) contains the occupation rate of freeway system across the State of California. Table 7
summarizes details of feature statistics.
Dataset Length Dimension Frequency
ETTh 17420 7 1 hour
ETTm 69680 7 15 min
Weather 52696 22 10 min
ILI 966 7 7 days
Electricity 26304 321 1 hour
Traffic 17544 862 1 hour
Table 7: Long-term Forecasting and Few-shot Forecasting Dataset Details
Dataset Mapping
Length Horizon M4 M3
M3 Yearly 645 6 Yearly -
M3 Quarterly 756 8 Quarterly -
M3 Monthly 1428 18 Monthly -
M3 Others 174 8 Monthly -
M4 Yearly 23000 18 - Yearly
M4 Quarterly 6 24000 - Quarterly
M4 Monthly 8 48000 - Monthly
M4 Weekly 359 13 - Monthly
M4 Daily 4227 14 - Monthly
M4 Hourly 414 48 - Monthly
TOURISM Yearly 518 4 Yearly Yearly
TOURISM Quarterly 427 8 Quarterly Quarterly
TOURISM Monthly 366 24 Monthly Monthly
ELECTR 1311 168 Hourly Monthly
Table 8: Zero-term Forecasting Datasets and Mapping Details of Zero-shot Learning
The details of zero-shot forecasting datasets are: M4 is a large and diverse dataset that contains time
series of various frequencies and fields, including business, financial and economic forecasting; M3
is smaller than M4, but also contains time series from diverse domains and frequencies; TOURISM
is the dataset of tourism activities with different frequencies and contains a much higher fraction of
erratic series compared with M4; ELECTR represents the electricity usage monitoring of 370 cus-
tomers over three years. Table 8 summarizes details of the datasets and zero-shot mapping between
source and target.
A.3.2 B ASELINE DETAILS
For long-shot forecasting, we refer to the SOTA methods reported in Wu et al. (2023): TimesNet
Wu et al. (2023), ETSformer Woo et al. (2022c), DLinear Zeng et al. (2023), FEDformer Zhou et al.
(2022), Informer Zhou et al. (2021), and LLM for TS method GPT4TS Zhou et al. (2023).
18Published as a conference paper at ICLR 2024
For few-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): DLinear
Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al. (2023), FEDformer Zhou et al.
(2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022), ETSformer Woo et al. (2022c),
Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)
For zero-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): N-BEATS
Oreshkin et al. (2020), DLinear Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al.
(2023), FEDformer Zhou et al. (2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022),
ETSformer Woo et al. (2022c), Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)
Methods TEST GPT4TS TimesNet ETSformer DLinear FEDformer Informer TCN LSTM
ETTm196 0.293 0.346 0.292 0.346 0.325 0.398 0.338 0.375 0.345 0.372 0.375 0.398 0.672 0.571 0.863 0.664 0.863 0.664
192 0.332 0.369 0.332 0.372 0.324 0.387 0.408 0.410 0.380 0.389 0.426 0.441 0.795 0.669 0.837 0.700 1.113 0.776
336 0.368 0.392 0.366 0.394 0.360 0.411 0.435 0.428 0.413 0.413 0.445 0.459 1.212 0.871 1.124 0.832 1.267 0.832
720 0.418 0.420 0.417 0.421 0.428 0.450 0.499 0.462 0.474 0.453 0.543 0.490 1.166 0.823 1.153 0.820 1.324 0.858
Avg 0.353 0.382 0.352 0.383 0.350 0.406 0.429 0.425 0.403 0.407 0.448 0.452 0.961 0.734 0.929 0.725 1.142 0.782
ETTh196 0.372 0.400 0.376 0.397 0.384 0.402 0.494 0.479 0.386 0.400 0.376 0.419 0.865 0.713 0.878 0.740 1.044 0.773
192 0.414 0.422 0.416 0.418 0.436 0.429 0.538 0.504 0.437 0.432 0.420 0.448 1.008 0.792 1.037 0.824 1.217 0.832
336 0.422 0.437 0.442 0.433 0.491 0.469 0.574 0.521 0.481 0.459 0.459 0.465 1.107 0.809 1.238 0.932 1.259 0.841
720 0.447 0.467 0.477 0.456 0.521 0.500 0.562 0.535 0.519 0.516 0.506 0.507 1.181 0.865 1.135 0.852 1.271 0.838
Avg 0.414 0.431 0.427 0.426 0.458 0.450 0.542 0.510 0.456 0.452 0.440 0.460 1.040 0.795 1.072 0.837 1.198 0.821
ETTh296 0.275 0.338 0.285 0.342 0.340 0.374 0.340 0.391 0.333 0.387 0.358 0.397 3.755 1.525 2.116 1.197 2.522 1.278
192 0.340 0.379 0.354 0.389 0.402 0.414 0.430 0.439 0.477 0.476 0.429 0.439 5.602 1.931 4.315 1.635 3.312 1.384
336 0.329 0.381 0.373 0.407 0.452 0.452 0.485 0.559 0.594 0.541 0.496 0.487 4.721 1.835 1.124 1.604 3.291 1.388
720 0.381 0.423 0.406 0.441 0.462 0.468 0.500 0.497 0.831 0.657 0.463 0.474 3.647 1.625 3.188 1.540 3.257 1.357
Avg 0.331 0.380 0.354 0.394 0.414 0.427 0.439 0.452 0.559 0.515 0.4370.449 4.431 1.729 2.686 1.494 3.095 1.352
Electricity96 0.132 0.223 0.139 0.238 0.168 0.222 0.187 0.304 0.197 0.282 0.193 0.308 0.274 0.368 0.258 0.357 0.375 0.437
192 0.158 0.241 0.153 0.251 0.184 0.239 0.199 0.196 0.285 0.201 0.315 0.296 0.386 0.266 0.368 0.348 0.442 0.473
336 0.163 0.260 0.169 0.266 0.198 0.260 0.212 0.329 0.209 0.301 0.214 0.329 0.300 0.394 0.280 0.380 0.439 0.473
720 0.199 0.291 0.206 0.297 0.220 0.300 0.233 0.345 0.245 0.333 0.246 0.355 0.373 0.439 0.283 0.376 0.980 0.814
Avg 0.162 0.253 0.167 0.263 0.192 0.245 0.208 0.323 0.212 0.300 0.214 0.327 0.311 0.397 0.313 0.401 0.559 0.549
Traffic96 0.407 0.282 0 0.388 0.282 0.593 0.321 0.607 0.392 0.650 0.396 0.587 0.366 0.719 0.391 0.684 0.384 0.843 0.453
192 0.423 0.287 0.407 0.290 0.617 0.336 0.621 0.399 0.598 0.370 0.604 0.373 0.696 0.379 0.685 0.390 0.847 0.453
336 0.430 0.296 0.412 0.294 0.629 0.336 0.622 0.396 0.605 0.373 0.621 0.383 0.777 0.420 0.734 0.408 0.853 0.455
720 0.463 0.315 0.450 0.312 0.640 0.350 0.632 0.396 0.645 0.394 0.626 0.382 0.864 0.472 0.717 0.396 1.500 0.805
Avg 0.430 0.295 0.414 0.294 0.620 0.336 0.621 0.396 0.625 0.383 0.610 0.376 0.764 0.416 0.705 0.395 1.011 0.541
Weather96 0.150 0.202 0.162 0.212 0.152 0.220 0.197 0.281 0.196 0.255 0.217 0.296 0.300 0.384 0.458 0.490 0.369 0.406
192 0.198 0.246 0.204 0.248 0.209 0.261 0.237 0.312 0.237 0.296 0.276 0.336 0.598 0.544 0.658 0.589 0.416 0.435
336 0.245 0.286 0.254 0.286 0.280 0.306 0.298 0.353 0.283 0.335 0.339 0.380 0.578 0.521 0.797 0.652 0.455 0.454
720 0.324 0.342 0.326 0.337 0.365 0.359 0.352 0.288 0.345 0.381 0.403 0.428 1.059 0.741 0.869 0.675 0.535 0.520
Avg 0.229 0.271 0.237 0.270 0.236 0.287 0.271 0.334 0.265 0.317 0.309 0.360 0.634 0.548 0.696 0.602 0.444 0.454
ILI24 1.974 0.886 2.063 0.881 2.317 0.934 2.527 1.000 2.398 1.040 3.228 1.260 5.764 1.677 4.480 1.444 5.914 1.734
36 2.028 0.976 1.868 0.892 1.972 0.900 2.615 1.007 2.646 1.088 2.679 1.080 4.755 1.467 4.799 1.467 6.631 1.845
48 2.353 1.115 1.790 0.884 2.238 0.900 2.359 0.972 2.614 1.086 2.622 1.078 4.763 1.469 4.800 1.468 6.736 1.857
60 2.425 1.203 1.979 0.957 2.027 0.928 2.487 1.016 2.804 1.146 2.857 1.15 5.264 1.564 5.278 1.560 6.870 1.879
Avg 2.195 1.045 1.925 0.903 2.139 0.901 2.497 1.004 2.616 1.090 2.847 1.144 5.137 1.544 4.839 1.485 6.538 1.829
1stcount 5 5 4 0 0 0 0 0 0
Table 9: Long-term Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone.
The past sequence length is set as 36 for ILI and 96 for the others. All the results are averaged from
4 different prediction lengths, that is {24, 36, 48, 60 }for ILI and {96, 192, 336, 720 }for the others.
A.3.3 L ONG -TERM FORECASTING
We follow the classical experiment settings and the results of SOTA models in Wu et al. (2023)
(ICLR 2023). The results are shown in Table 9. Overall, TEST achieves comparable performance
to SOTA models TimesNet and Dlinear, and outperforms other baselines.
A.3.4 F EW-SHOT FORECASTING
For the few-shot forecasting task, only 10% percentage timesteps of training data are used, and the
other two parts remain unchanged. We follow the classical experiment settings and the results of
SOTA models in Zhou et al. (2023) (NeurIPS 2023). The results are shown in Table 10. Overall,
TEST has comparable performance with the SOTA baselines PatchTST and Dlinear, and SOTA
LLM for TS method GPT4TS.
19Published as a conference paper at ICLR 2024
Methods TEST GPT4TS DLinear PatchTST TimesNet FEDformer Autoformer Stationary ETSformer LightTS Informer Reformer
Weather96 0.163 0.213 0.163 0.215 0.171 0.224 0.165 0.215 0.184 0.230 0.188 0.253 0.221 0.297 0.192 0.234 0.199 0.272 0.217 0.269 0.374 0.401 0.335 0.380
192 0.230 0.263 0.210 0.254 0.215 0.263 0.210 0.257 0.245 0.283 0.250 0.304 0.270 0.322 0.269 0.295 0.279 0.332 0.259 0.304 0.552 0.478 0.522 0.462
336 0.278 0.282 0.256 0.292 0.258 0.299 0.259 0.297 0.305 0.321 0.312 0.346 0.320 0.351 0.370 0.357 0.356 0.386 0.303 0.334 0.724 0.541 0.715 0.535
720 0.301 0.328 0.321 0.339 0.320 0.346 0.332 0.346 0.381 0.371 0.387 0.393 0.390 0.396 0.441 0.405 0.437 0.448 0.377 0.382 0.739 0.558 0.611 0.500
Avg 0.243 0.272 0.238 0.275 0.241 0.283 0.242 0.279 0.279 0.301 0.284 0.324 0.300 0.342 0.318 0.323 0.318 0.360 0.289 0.322 0.597 0.495 0.546 0.469
ETTh196 0.455 0.457 0.458 0.456 0.492 0.495 0.516 0.485 0.861 0.628 0.512 0.499 0.613 0.552 0.918 0.639 1.112 0.806 1.298 0.838 1.179 0.792 1.184 0.790
192 0.572 0.519 0.570 0.516 0.565 0.538 0.598 0.524 0.797 0.593 0.624 0.555 0.722 0.598 0.915 0.629 1.155 0.823 1.322 0.854 1.199 0.806 1.295 0.850
336 0.611 0.531 0.608 0.535 0.721 0.622 0.657 0.550 0.941 0.648 0.691 0.574 0.750 0.619 0.939 0.644 1.179 0.832 1.347 0.870 1.202 0.811 1.294 0.854
720 0.723 0.594 0.725 0.591 0.986 0.743 0.762 0.610 0.877 0.641 0.728 0.614 0.721 0.616 0.887 0.645 1.273 0.874 1.534 0.947 1.217 0.825 1.223 0.838
Avg 0.479 0.525 0.590 0.525 0.691 0.600 0.633 0.542 0.869 0.628 0.639 0.561 0.702 0.596 0.915 0.639 1.180 0.834 1.375 0.877 1.199 0.809 1.249 0.833
ETTh296 0.332 0.374 0.331 0.374 0.357 0.411 0.353 0.389 0.378 0.409 0.382 0.416 0.413 0.451 0.389 0.411 0.678 0.619 2.022 1.006 3.837 1.508 3.788 1.533
192 0.401 0.433 0.402 0.411 0.569 0.519 0.403 0.414 0.490 0.467 0.478 0.474 0.474 0.477 0.473 0.455 0.785 0.666 2.329 1.104 3.856 1.513 3.552 1.483
336 0.408 0.440 0.406 0.433 0.671 0.572 0.426 0.441 0.537 0.494 0.504 0.501 0.547 0.543 0.507 0.480 0.839 0.694 2.453 1.122 3.952 1.526 3.395 1.526
720 0.459 0.480 0.449 0.464 0.824 0.648 0.477 0.480 0.510 0.491 0.499 0.509 0.516 0.523 0.477 0.472 1.273 0.874 3.816 1.407 3.842 1.503 3.205 1.401
Avg 0.401 0.432 0.397 0.421 0.605 0.538 0.415 0.431 0.479 0.465 0.466 0.475 0.488 0.499 0.462 0.455 0.894 0.713 2.655 1.160 3.872 1.513 3.485 1.486
ETTm196 0.392 0.401 0.390 0.404 0.352 0.392 0.410 0.419 0.583 0.501 0.578 0.518 0.774 0.614 0.761 0.568 0.911 0.688 0.921 0.682 1.162 0.785 1.442 0.847
192 0.423 0.426 0.429 0.423 0.382 0.412 0.437 0.434 0.630 0.528 0.617 0.546 0.754 0.592 0.781 0.574 0.955 0.703 0.957 0.701 1.172 0.793 1.444 0.862
336 0.471 0.444 0.469 0.439 0.419 0.434 0.476 0.454 0.725 0.568 0.998 0.775 0.869 0.677 0.803 0.587 0.991 0.719 0.998 0.716 1.227 0.908 1.450 0.866
720 0.552 0.501 0.569 0.498 0.490 0.477 0.681 0.556 0.769 0.549 0.693 0.579 0.810 0.630 0.844 0.581 1.062 0.747 1.007 0.719 1.207 0.797 1.366 0.850
Avg 0.574 0.443 0.464 0.441 0.411 0.429 0.501 0.466 0.677 0.537 0.722 0.605 0.802 0.628 0.797 0.578 0.980 0.714 0.971 0.705 1.192 0.821 1.426 0.856
ETTm296 0.233 0.262 0.188 0.269 0.213 0.303 0.191 0.274 0.212 0.285 0.291 0.399 0.352 0.454 0.229 0.308 0.331 0.430 0.813 0.688 3.203 1.407 4.195 1.628
192 0.303 0.302 0.251 0.309 0.278 0.345 0.252 0.317 0.270 0.323 0.307 0.379 0.694 0.691 0.291 0.343 0.400 0.464 1.008 0.768 3.112 1.387 4.042 1.601
336 0.359 0.341 0.307 0.346 0.338 0.385 0.306 0.353 0.323 0.353 0.543 0.559 2.408 1.407 0.348 0.376 0.469 0.498 1.031 0.775 3.255 1.421 3.963 1.585
720 0.452 0.419 0.426 0.417 0.436 0.440 0.433 0.427 0.474 0.449 0.712 0.614 1.913 1.166 0.461 0.438 0.589 0.557 1.096 0.791 3.909 1.543 3.711 1.532
Avg 0.317 0.309 0.293 0.335 0.316 0.368 0.296 0.343 0.320 0.353 0.463 0.488 1.342 0.930 0.332 0.366 0.447 0.487 0.987 0.756 3.370 1.440 3.978 1.587
Electricity96 0.143 0.235 0.139 0.237 0.150 0.253 0.140 0.238 0.299 0.373 0.231 0.323 0.261 0.348 0.420 0.466 0.599 0.587 0.350 0.425 1.259 0.919 0.993 0.784
192 0.158 0.255 0.156 0.252 0.164 0.264 0.160 0.255 0.305 0.379 0.261 0.356 0.338 0.406 0.411 0.459 0.620 0.598 0.376 0.448 1.160 0.873 0.938 0.753
336 0.176 0.275 0.175 0.270 0.181 0.282 0.180 0.276 0.319 0.391 0.360 0.445 0.410 0.474 0.434 0.473 0.662 0.619 0.428 0.485 1.157 0.872 0.925 0.745
720 0.230 0.311 0.233 0.317 0.223 0.321 0.241 0.323 0.369 0.426 0.530 0.585 0.715 0.685 0.510 0.521 0.757 0.664 0.611 0.597 1.203 0.898 1.004 0.790
Avg 0.176 0.269 0.176 0.269 0.180 0.280 0.180 0.273 0.323 0.392 0.346 0.427 0.431 0.478 0.444 0.480 0.660 0.617 0.441 0.489 1.195 0.891 0.965 0.768
Traffic96 0.415 0.317 0.414 0.297 0.419 0.298 0.403 0.289 0.719 0.416 0.639 0.400 0.672 0.405 1.412 0.802 1.643 0.855 1.157 0.636 1.557 0.821 1.527 0.815
192 0.425 0.300 0.426 0.301 0.434 0.305 0.415 0.296 0.748 0.428 0.637 0.416 0.727 0.424 1.419 0.806 1.641 0.854 1.207 0.661 1.454 0.765 1.538 0.817
336 0.436 0.310 0.434 0.303 0.449 0.313 0.426 0.304 0.853 0.471 0.655 0.427 0.749 0.454 1.443 0.815 1.711 0.878 1.334 0.713 1.521 0.812 1.550 0.819
720 0.489 0.338 0.487 0.337 0.484 0.336 0.474 0.331 1.485 0.825 0.722 0.456 0.847 0.499 1.539 0.837 2.660 1.157 1.292 0.726 1.605 0.846 1.588 0.833
Avg 0.441 0.316 0.440 0.310 0.447 0.313 0.430 0.305 0.951 0.535 0.663 0.425 0.749 0.446 1.453 0.815 1.914 0.936 1.248 0.684 1.534 0.811 1.551 0.821
1stcount 5 5 4 0 0 0 0 0 0 0 0 0
Table 10: Few-shot Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone.
All the results are averaged from 4 different prediction lengths, that is {96, 192, 336, 720 }.
Methods M4 M3 TOURISM ELECTR
Metric sMAPE sMAPE MAPE ND×100 Average 1stcount
N-BEATS 11.70 12.44 18.82 17.8 15.19 2
DLinear 15.33 14.03 28.51 17.6 18.86 0
TimesNet 13.55 14.17 28.84 19.3 18.96 0
PatchTST 13.22 13.06 27.10 17.3 17.67 0
ETSformer 27.74 16.03 180.40 44.2 67.09 0
LightTS 13.62 17.90 66.99 19.6 29.52 0
Stationary 13.32 15.29 43.75 22.0 23.59 0
FEDformer 15.04 13.53 31.55 18.4 19.63 0
Autoformer 20.02 15.87 40.39 33.9 27.54 0
Informer 19.04 15.82 35.82 21.2 22.97 0
Reformer 14.09 13.37 25.48 21.6 18.63 0
GPT2(6) 13.12 13.06 22.14 17.2 16.38 1
TEST 13.10 12.56 18.17 17.9 15.93 1
Table 11: Zero-shot learning results. Dataset-specific metrics aggregated over each dataset. A lower
value indicates better performance. The source dataset of M3, Tourism, Electricity are M4. For M4,
the source data for N-BEATS is FRED, and M3 for other models.
A.3.5 Z ERO-SHOT FORECASTING
Zero-shot Forecasting task can evaluate the cross datasets adaption ability. Which means that the
method is evaluated to perform on a dataset (without any training data from this dataset) when it is
trained from another dataset. The results are summarized in Table 11. TEST outperforms all recent
SOTA methods. TEST is comparable to N-BEATS without any meta-learning design and GPT4TS.
20Published as a conference paper at ICLR 2024
A.4 C LASSIFICATION TASKS
All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB
GPUs. We use Area Under Curve of Receiver Operating Characteristic (AUC-ROC) as metrics.
Meanwhile, we compute the average rank, the number of Top-1, Top-3, and Top-5 accuracy to show
the robustness of different methods. All experiments are repeated 3 times and the mean of the
metrics is used in the final results.
A.4.1 D ATASET DETAILS
We present accuracy scores for all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al.
(2018). UEA consists of 30 different datasets. Details of these datasets are shown in Table 12
Dataset Train Cases Test Cases Dimensions Length Classes
ArticularyWordRecognition 275 30 9 144 25
AtrialFibrillation 15 15 2 640 3
BasicMotions 40 40 4 100 4
CharacterTrajectories 1422 1436 3 182 20
Cricket 108 72 6 17984 5
DuckDuckGeese 60 40 1345 270 5
EigenWorms 128 131 6 17984 5
Epilepsy 137 138 3 206 4
EthanolConcentration 261 263 3 1751 4
ERing 30 20 4 65 6
FaceDetection 5890 3524 144 62 2
FingerMovements 316 100 28 50 2
HandMovementDirection 320 147 10 400 4
Handwriting 150 850 3 152 26
Heartbeat 204 105 61 495 2
JapaneseV owels 270 370 12 29 9
Libras 180 280 2 45 15
LSST 2459 2466 6 36 14
InsectWingbeat 30000 20000 200 78 10
MotorImagery 278 100 64 3000 2
NATOPS 180 180 24 51 6
PenDigits 7494 3498 2 8 10
PEMS-SF 267 173 963 144 7
Phoneme 3315 3353 11 217 39
RacketSports 151 152 6 30 4
SelfRegulationSCP1 268 293 6 896 2
SelfRegulationSCP2 200 180 7 1152 2
SpokenArabicDigits 6599 2199 13 93 10
StandWalkJump 12 15 4 2500 3
UWaveGestureLibrary 120 320 3 315 8
Table 12: UEA Classification Dataset Details
A.4.2 B ASELINE DETAILS
For classification, we refer to the SOTA methods: Three benchmarks Bostrom et al. (2018) (EDI,
DTWI, and DTWD) are based on Euclidean Distance, dimension-independent dynamic time warp-
ing, and dimension-dependent dynamic time warping; MLSTM-FCNs Karim et al. (2019) applies an
LSTM layer and stacked CNN layers to generate features; WEASEL-MUSE Sch ¨afer & Leser (2017)
is a bag-of-pattern based approach which extracts and represents features to words. Scalable Rep-
resentation Learning (SRL) Franceschi et al. (2019a) employs negative sampling techniques with
an encoder-based architecture to learn the representation; TapNet Zhang et al. (2020) is a recent
model with an attentional prototype learning in its deep learning-based network; ShapeNet Li et al.
(2021a) projects the subsequences into a unified space and applies clustering to find the shapelets;
Rocket and MiniRocket Dempster et al. (2021) use random convolutional kernels to extract features
from univariate time series; RL-PAM Gao et al. (2022) introduces reinforcement learning to the
pattern mining; TStamp Transformer Zerveas et al. (2021) takes the values at each timestamp as the
input for a transformer encoder; SVP-T Zuo et al. (2023) uses differnt variables and positions (time
interval) as the inputs (shape-level).
21Published as a conference paper at ICLR 2024
A.4.3 M ULTIVARIATE TIMESERIES CLASSIFICATION
We follow the classical experiment settings in multivariate time series classification tasks Bostrom
et al. (2018). The results are shown in Table 13. Overall, TEST achieves comparable performance
to SOTA models and outperforms most baselines.
EDI DTWI DTWD MLSTM-FCNs WEASEL+MUSE SRL TapNet ShapeNet Rocket MiniRocket RLPAM TStamp SVP-T TEST
AWR 0.970 0.980 0.987 0.973 0.990 0.987 0.987 0.987 0.996 0.992 0.923 0.983 0.993 0.994
AF 0.267 0.267 0.220 0.267 0.333 0.133 0.333 0.400 0.249 0.133 0.733 0.200 0.400 0.420
BM 0.676 1.000 0.975 0.950 1.000 1.000 1.000 1.000 0.990 1.000 1.000 0.975 1.000 1.000
CT 0.964 0.969 0.989 0.985 0.990 0.994 0.997 0.980 N/A 0.993 0.978 N/A 0.990 0.989
CK 0.944 0.986 1.000 0.917 1.000 0.986 0.958 0.986 1.000 0.986 1.000 0.958 1.000 1.000
DDG 0.275 0.550 0.600 0.675 0.575 0.675 0.575 0.725 0.461 0.650 0.700 0.480 0.700 0.675
EW 0.549 N/A 0.618 0.504 0.890 0.878 0.489 0.878 0.863 0.962 0.908 N/A 0.923 0.878
EP 0.666 0.978 0.964 0.761 1.000 0.957 0.971 0.987 0.991 1.000 0.978 0.920 0.986 0.985
ER 0.133 0.914 0.929 0.133 0.133 0.133 0.133 0.133 0.981 0.981 0.819 0.933 0.937 0.937
EC 0.293 0.304 0.323 0.373 0.430 0.236 0.323 0.312 0.447 0.468 0.369 0.337 0.331 0.373
FD 0.519 0.000 0.529 0.545 0.545 0.528 0.556 0.602 0.694 0.620 0.621 0.681 0.512 0.512
FM 0.550 0.520 0.530 0.580 0.490 0.540 0.530 0.580 0.553 0.550 0.640 0.776 0.600 0.770
HMD 0.278 0.306 0.231 0.365 0.365 0.270 0.378 0.338 0.446 0.392 0.635 0.608 0.392 0.444
HW 0.200 0.316 0.286 0.286 0.605 0.533 0.357 0.452 0.567 0.507 0.522 0.305 0.433 0.431
HB 0.619 0.658 0.717 0.663 0.727 0.737 0.751 0.756 0.718 0.771 0.779 0.712 0.790 0.791
IW 0.128 N/A N/A 0.167 N/A 0.160 0.208 0.250 N/A 0.595 0.352 0.684 0.184 0.572
JV 0.924 0.959 0.949 0.976 0.973 0.989 0.965 0.984 0.965 0.989 0.935 0.994 0.978 0.991
LB 0.833 0.894 0.870 0.856 0.878 0.867 0.850 0.856 0.906 0.922 0.794 0.844 0.883 0.884
LSST 0.456 0.575 0.551 0.373 0.590 0.558 0.568 0.590 0.632 0.643 0.643 0.381 0.666 0.595
MI 0.510 N/A 0.500 0.510 0.500 0.540 0.590 0.610 0.531 0.550 0.610 N/A 0.650 0.650
NT 0.850 0.850 0.883 0.889 0.870 0.944 0.939 0.883 0.885 0.928 0.950 0.900 0.906 0.902
PD 0.705 0.939 0.977 0.978 0.948 0.983 0.980 0.977 0.996 N/A 0.982 0.974 0.983 0.979
PM 0.973 0.734 0.711 0.699 0.000 0.688 0.751 0.751 0.856 0.522 0.632 0.919 0.867 0.860
PH 0.104 0.151 0.151 0.110 0.190 0.246 0.175 0.298 0.284 0.292 0.175 0.088 0.176 0.196
RS 0.868 0.842 0.803 0.803 0.934 0.862 0.868 0.882 0.928 0.868 0.868 0.829 0.842 0.851
SCP1 0.771 0.765 0.775 0.874 0.710 0.846 0.652 0.782 0.866 0.925 0.802 0.925 0.884 0.870
SCP2 0.483 0.533 0.539 0.472 0.460 0.556 0.550 0.578 0.514 0.522 0.632 0.589 0.600 0.579
SAD 0.967 0.959 0.963 0.990 0.982 0.956 0.983 0.975 0.630 0.620 0.621 0.993 0.986 0.982
SWJ 0.200 0.333 0.200 0.067 0.333 0.400 0.400 0.533 0.456 0.333 0.667 0.267 0.467 0.468
UGL 0.881 0.868 0.903 0.891 0.916 0.884 0.894 0.906 0.944 0.938 0.944 0.903 0.941 0.933
Avg.Rank 10.933 9.480 8.821 8.756 6.890 7.120 6.956 5.523 5.423 5.013 5.059 7.484 4.032 4.012
Num.Top-1 1 1 1 0 5 1 2 3 5 5 6 4 4 6
Num.Top-3 1 2 1 1 6 6 3 7 12 14 16 9 17 18
Num.Top-5 2 2 3 5 15 12 13 17 16 20 19 10 23 24
P-value 0.000 0.000 0.000 0.000 0.006 0.003 0.000 0.118 0.217 0.765 0.967 0.047 0.044 0.040
Table 13: Accuracies on All Datasets of the UEA Archive
A.5 R EPRESENTATION TASKS
We assess the quality of our learned representations on supervised tasks in a standard manner by
using them for time series classification Franceschi et al. (2019b). All the deep learning networks
are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use Area Under Curve
of Receiver Operating Characteristic (AUC-ROC) as metrics.
A.5.1 D ATASET DETAILS
We represent the results for all 128 kinds of univariate TS datasets in UCR archive Dau et al. (2019),
which is a standard set of varied univariate datasets.
A.5.2 B ASELINE DETAILS
The compared method includes SOTAs of unsupervised time series representation: T-Loss
Franceschi et al. (2019b), TS-TCC Eldele et al. (2021b), TST Zerveas et al. (2021) and TNC Tonek-
aboni et al. (2021), TS2Vec Yue et al. (2022).
A.5.3 C LASSIFICATION BASED ON REPRESENTATION
We assess the quality of our learned representations on supervised tasks in a standard manner by
using them for time series classification Franceschi et al. (2019b). In this setting, we show that our
22Published as a conference paper at ICLR 2024
method outperforms SOTA unsupervised methods, and notably achieves performance close to the
supervised SOTA method as shown in Table 14.
For each considered dataset with a train / test split, we unsupervisedly train an encoder using its train
set. We then train an SVM with radial basis function kernel on top of the learned features using the
train labels of the dataset, and output the corresponding classification score on the test set.
TEST TCN TS2Vec T-Loss TNC
Adiac 0.776 0.768 0.765 0.675 0.726
ArrowHead 0.825 0.857 0.817 0.766 0.703
Beef 0.766 0.768 0.633 0.667 0.733
BeetleFly 0.853 0.900 0.900 0.800 0.850
BirdChicken 0.808 0.803 0.800 0.850 0.750
Car 0.883 0.834 0.700 0.833 0.683
CBF 1.000 1.000 1.000 0.983 0.983
ChlorineConcentration 0.810 0.832 0.812 0.749 0.760
CinCECGTorso 0.815 0.829 0.825 0.713 0.669
Coffee 1.000 1.000 1.000 1.000 1.000
Computers 0.632 0.660 0.660 0.664 0.684
CricketX 0.802 0.787 0.805 0.713 0.623
CricketY 0.754 0.749 0.769 0.728 0.597
CricketZ 0.787 0.794 0.790 0.708 0.682
DiatomSizeReduction 0.980 0.985 0.987 0.984 0.993
DistalPhalanxOutlineCorrect 0.776 0.761 0.757 0.775 0.754
DistalPhalanxOutlineAgeGroup 0.714 0.727 0.719 0.727 0.741
DistalPhalanxTW 0.662 0.698 0.683 0.676 0.669
Earthquakes 0.746 0.748 0.748 0.748 0.748
ECG200 0.893 0.920 0.880 0.940 0.830
ECG5000 0.935 0.935 0.934 0.933 0.937
ECGFiveDays 1.000 1.000 1.000 1.000 0.999
ElectricDevices 0.714 0.721 0.719 0.707 0.700
FaceAll 0.789 0.771 0.805 0.786 0.766
FaceFour 0.834 0.932 0.932 0.920 0.659
FacesUCR 0.939 0.924 0.926 0.884 0.789
FiftyWords 0.781 0.771 0.774 0.732 0.653
Fish 0.937 0.926 0.937 0.891 0.817
FordA 0.940 0.936 0.948 0.928 0.902
FordB 0.789 0.794 0.807 0.793 0.733
GunPoint 0.983 0.980 0.987 0.980 0.967
Ham 0.714 0.714 0.724 0.724 0.752
HandOutlines 0.918 0.925 0.930 0.922 0.930
Haptics 0.510 0.526 0.536 0.490 0.474
Herring 0.625 0.644 0.609 0.594 0.594
InlineSkate 0.389 0.418 0.407 0.371 0.378
InsectWingbeatSound 0.620 0.630 0.624 0.597 0.549
ItalyPowerDemand 0.969 0.925 0.960 0.954 0.928
LargeKitchenAppliances0 0.855 0.845 0.875 0.789 0.776
Lightning2 0.846 0.869 0.820 0.869 0.869
Lightning7 0.866 0.863 0.822 0.795 0.767
Mallat 0.915 0.944 0.873 0.951 0.871
Meat 0.950 0.952 0.967 0.950 0.917
MedicalImages 0.792 0.789 0.793 0.750 0.754
MiddlePhalanxOutlineCorrect 0.811 0.838 0.825 0.825 0.818
MiddlePhalanxOutlineAgeGroup 0.636 0.636 0.630 0.656 0.643
MiddlePhalanxTW 0.591 0.584 0.578 0.591 0.571
MoteStrain 0.857 0.861 0.863 0.851 0.825
NonInvasiveFetalECGThorax1 0.923 0.930 0.919 0.878 0.898
NonInvasiveFetalECGThorax2 0.940 0.938 0.935 0.919 0.912
OliveOil 0.903 0.901 0.940 0.867 0.833
OSULeaf 0.872 0.851 0.843 0.760 0.723
PhalangesOutlinesCorrect 0.794 0.809 0.823 0.784 0.787
Phoneme 0.296 0.312 0.309 0.276 0.180
Plane 1.000 1.000 0.990 0.990 1.000
ProximalPhalanxOutlineCorrect 0.876 0.887 0.900 0.859 0.866
ProximalPhalanxOutlineAgeGroup 0.844 0.837 0.829 0.844 0.854
ProximalPhalanxTW 0.785 0.824 0.805 0.771 0.810
RefrigerationDevices 0.587 0.586 0.589 0.515 0.565
ScreenType 0.405 0.414 0.397 0.416 0.509
ShapeletSim 0.989 1.000 0.994 0.672 0.589
ShapesAll 0.897 0.902 0.905 0.848 0.788
SmallKitchenAppliances 0.723 0.731 0.733 0.677 0.725
SonyAIBORobotSurface1 0.874 0.903 0.900 0.902 0.804
SonyAIBORobotSurface2 0.893 0.871 0.889 0.889 0.834
StarLightCurves 0.970 0.968 0.971 0.964 0.968
Strawberry 0.962 0.966 0.965 0.954 0.951
SwedishLeaf 0.939 0.945 0.942 0.914 0.880
Symbols 0.973 0.977 0.972 0.963 0.885
SyntheticControl 0.997 0.997 0.993 0.987 1.000
23Published as a conference paper at ICLR 2024
ToeSegmentation1 0.933 0.917 0.947 0.939 0.864
ToeSegmentation2 0.915 0.899 0.900 0.900 0.831
Trace 1.000 1.000 1.000 0.990 1.000
TwoLeadECG 0.982 0.986 0.987 0.999 0.993
TwoPatterns 1.000 1.000 1.000 0.999 1.000
UWaveGestureLibraryX 0.810 0.795 0.801 0.785 0.781
UWaveGestureLibraryY 0.729 0.719 0.720 0.710 0.697
UWaveGestureLibraryZ 0.761 0.774 0.768 0.757 0.721
UWaveGestureLibraryAll 0.935 0.930 0.934 0.896 0.903
Wafer 0.995 0.998 0.998 0.992 0.994
Wine 0.788 0.880 0.889 0.815 0.759
WordSynonyms 0.699 0.679 0.704 0.691 0.630
Worms 0.704 0.701 0.701 0.727 0.623
WormsTwoClass 0.805 0.806 0.753 0.792 0.727
Yoga 0.883 0.883 0.877 0.837 0.812
ACSF1 0.849 0.910 0.910 0.900 0.730
AllGestureWiimoteX 0.744 0.777 0.751 0.763 0.703
AllGestureWiimoteY 0.754 0.796 0.774 0.726 0.699
AllGestureWiimoteZ 0.744 0.749 0.770 0.723 0.646
BME 0.979 0.992 0.980 0.993 0.973
Chinatown 0.969 0.964 0.959 0.951 0.977
Crop 0.753 0.754 0.758 0.722 0.738
EOGHorizontalSignal 0.544 0.569 0.522 0.605 0.442
EOGVerticalSignal 0.467 0.503 0.472 0.434 0.392
EthanolLevel 0.480 0.468 0.484 0.382 0.424
FreezerRegularTrain 0.983 0.996 0.983 0.956 0.991
FreezerSmallTrain 0.893 0.875 0.872 0.933 0.982
Fungi 0.967 0.958 0.946 1.000 0.527
GestureMidAirD1 0.637 0.608 0.615 0.608 0.431
GestureMidAirD2 0.508 0.479 0.515 0.546 0.362
GestureMidAirD3 0.346 0.492 0.300 0.285 0.292
GesturePebbleZ1 0.878 0.930 0.884 0.919 0.378
GesturePebbleZ2 0.842 0.873 0.848 0.899 0.316
GunPointAgeSpan 0.994 0.987 0.968 0.994 0.984
GunPointMaleVersusFemale 1.000 1.000 1.000 0.997 0.994
GunPointOldVersusYoung 1.000 1.000 1.000 1.000 1.000
HouseTwenty 0.944 0.917 0.941 0.933 0.782
InsectEPGRegularTrain 1.000 1.000 1.000 1.000 1.000
InsectEPGSmallTrain 1.000 1.000 1.000 1.000 1.000
MelbournePedestrian 0.954 0.959 0.956 0.944 0.942
MixedShapesRegularTrain 0.915 0.917 0.922 0.905 0.911
MixedShapesSmallTrain 0.884 0.861 0.856 0.860 0.813
PickupGestureWiimoteZ 0.800 0.823 0.760 0.740 0.620
PigAirwayPressure 0.524 0.630 0.683 0.510 0.413
PigArtPressure 0.962 0.966 0.966 0.928 0.808
PigCVP 0.803 0.815 0.870 0.788 0.649
PLAID 0.551 0.561 0.549 0.555 0.495
PowerCons 0.967 0.961 0.972 0.900 0.933
Rock 0.660 0.700 0.700 0.580 0.580
SemgHandGenderCh2 0.952 0.963 0.962 0.890 0.882
SemgHandSubjectCh2 0.897 0.860 0.891 0.789 0.593
SemgHandMovementCh2 0.944 0.952 0.942 0.920 0.820
SmoothSubspace 0.967 0.980 0.993 0.960 0.913
UMD 1.000 1.000 0.993 0.993 0.993
Avg 0.826 0.832 0.827 0.806 0.761
Table 14: Accuracies on All Datasets of the UCR Archive
A.6 A BLATION
TEST contains two contrastive learning strategies: instance-wise contrast and feature-wise contrast,
and can use different text embedding vectors as prototypes, we show the impact of these strategies.
A.6.1 C ONTRASTIVE LEARNING STRATEGIES
As shown in Table 15 and 16, both two contrastive learning strategies can increase the accuracy.
A.6.2 T EXT PROTOTYPES
The number and the type of text prototypes will lead to different results.
As shown in Table 17. We randomly select 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22 prototypes. The
accuracy and number are basically positively correlated. The results of 10 prototypes are almost
optimal.
24Published as a conference paper at ICLR 2024
ETTm1 ETTm2 ETTh1 ETTh2 Electricity Traffic Weather ILI
Instance-wise 0.621 0.550 0.755 0.630 0.493 0.453 0.580 0.612 0.293 0.396 0.788 0.620 0.463 0.349 3.301 4.535
Feature-wise 0.741 0.559 0.793 0.634 0.699 0.493 0.585 0.628 0.286 0.390 0.821 0.629 0.453 0.388 3.139 5.931
TEST 0.353 0.382 0.293 0.334 0.414 0.431 0.331 0.380 0.162 0.253 0.430 0.295 0.229 0.271 2.195 1.045
Table 15: Long-term Forecasting Results (MSE, MAE). TEST uses different contrastive learning
stragegy. All the results are averaged from 4 different prediction lengths, that is {24, 36, 48, 60 }for
ILI and {96, 192, 336, 720 }for the others. The results are average.
TEST Instance-wise Feature-wise TimesNet N-BEATS ETSformer DLinear FEDformer Stationary Autoformer Informer Reformer
SMAPE 11.927 13.525 16.987 11.829 11.851 14.718 13.639 12.840 12.780 12.909 14.086 18.200
MASE 1.613 2.111 3.265 1.585 1.599 2.408 2.095 1.701 1.756 1.771 3.010 4.223
OWA 0.861 1.051 1.480 0.851 0.855 1.172 1.051 0.918 0.930 0.939 1.230 1.775
Table 16: Short-term Forecasting Task on M4. The prediction lengths are in [6, 48] and results are
averaged from several datasets.
As shown in Table 18. We randomly select 10 prototypes 10 times. The accuracy is basically
consistent. Therefore, the type of prototypes has almost no impact on the results.
1 2 4 6 8 10 12 14 16 18 20 22
SMAPE 30.901 20.201 17.415 16.997 13.820 11.927 11.710 11.638 11.094 11.098 10.953 10.885
MASE 6.590 4.515 3.910 3.595 2.580 1.613 1.408 1.195 1.301 1.306 1.471 1.310
OWA 3.779 2.050 1.451 1.484 0.990 0.861 0.872 0.801 0.910 0.902 0.838 0.830
Table 17: Short-term Forecasting Task on M4. The results are reported with different number of text
prototypes.
1 2 3 4 5 6 7 8 9 10 Avg. Std.
SMAPE 11.907 11.920 11.927 11.926 11.925 11.925 11.950 11.890 11.728 11.910 11.901 0.059
MASE 1.612 1.610 1.653 1.603 1.619 1.620 1.625 1.623 1.613 1.591 1.617 0.016
OWA 0.870 0.872 0.872 0.872 0.872 0.872 0.849 0.862 0.876 0.870 0.868, 0.009
Table 18: Short-term Forecasting Task on M4. The results are reported with different types of text
prototypes.
Considering why the type of text prototype does not significantly affect results, we figure that in high
dimensional space, almost all vectors are pairwise orthogonal Hopcroft & Kannan (2013). Which
means that, in high-dimensional space, it is easy to generate a large number of almost orthogonal
vectors to represent different attributes. Thus, randomly selecting the same number of vectors, the
represented space size and expressed number of features are almost the same. Therefore, the key is
the number rather than the type.
In terms of probability, “two vectors orthogonal” is equivalent to “two vectors perpendicular” is
equivalent to “two vectors uncorrelated” is equivalent to “ cosθ= 0”. For a n-dimensional space,
randomly two vectors have: ∀ϵ,limn→∞P(|cosθ|> ϵ) = 0 . As shown in Figure 7, as the
dimension increases, the probability of two random vectors being similar decreases. For LLM,
n >1024, P(θ= 0) <0.00001 .
Figure 7: Probability Density of the Angle between Two Random Vectors in n-dimensional Space
25

Title: From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
Abstract: With the rise of large language models (LLMs), researchers are increasingly
exploring their applications in var ious vertical domains, such as software
engineering. LLMs have achieved remarkable success in areas including code
generation and vulnerability detection. However, they also exhibit numerous
limitations and shortcomings. LLM-based agents, a novel tech nology with the
potential for Artificial General Intelligence (AGI), combine LLMs as the core
for decision-making and action-taking, addressing some of the inherent
limitations of LLMs such as lack of autonomy and self-improvement. Despite
numerous studies and surveys exploring the possibility of using LLMs in
software engineering, it lacks a clear distinction between LLMs and LLM based
agents. It is still in its early stage for a unified standard and benchmarking
to qualify an LLM solution as an LLM-based agent in its domain. In this survey,
we broadly investigate the current practice and solutions for LLMs and
LLM-based agents for software engineering. In particular we summarise six key
topics: requirement engineering, code generation, autonomous decision-making,
software design, test generation, and software maintenance. We review and
differentiate the work of LLMs and LLM-based agents from these six topics,
examining their differences and similarities in tasks, benchmarks, and
evaluation metrics. Finally, we discuss the models and benchmarks used,
providing a comprehensive analysis of their applications and effectiveness in
software engineering. We anticipate this work will shed some lights on pushing
the boundaries of LLM-based agents in software engineering for future research.
Full Text: JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1
From LLMs to LLM-based Agents for Software
Engineering: A Survey of Current, Challenges and
Future
Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen
Abstract —With the rise of large language models (LLMs),
researchers are increasingly exploring their applications in var-
ious vertical domains, such as software engineering. LLMs have
achieved remarkable success in areas including code generation
and vulnerability detection. However, they also exhibit numerous
limitations and shortcomings. LLM-based agents, a novel tech-
nology with the potential for Artificial General Intelligence (AGI),
combine LLMs as the core for decision-making and action-taking,
addressing some of the inherent limitations of LLMs such as lack
of autonomy and self-improvement. Despite numerous studies
and surveys exploring the possibility of using LLMs in software
engineering, it lacks a clear distinction between LLMs and LLM-
based agents. It is still in its early stage for a unified standard
and benchmarking to qualify an LLM solution as an LLM-based
agent in its domain. In this survey, we broadly investigate the
current practice and solutions for LLMs and LLM-based agents
for software engineering. In particular we summarise six key
topics: requirement engineering, code generation, autonomous
decision-making, software design, test generation, and software
maintenance. We review and differentiate the work of LLMs
and LLM-based agents from these six topics, examining their
differences and similarities in tasks, benchmarks, and evaluation
metrics. Finally, we discuss the models and benchmarks used,
providing a comprehensive analysis of their applications and
effectiveness in software engineering. We anticipate this work
will shed some lights on pushing the boundaries of LLM-based
agents in software engineering for future research.
Index Terms —Large Language Models, LLM-based Agents,
Software Engineering, Benchmark, Software Security, AI System
Development
I. I NTRODUCTION
SOFTWARE engineering (SE) has seen its booming re-
search and development with the aid of artificial intel-
ligence techniques. Traditional approaches leveraging neural
networks and machine learning have facilitated various SE
topics such as bug detection, code synthesis, and requirements
analysis [1] [2]. However, they often present limitations, in-
cluding the need for exclusive feature engineering, scalability
issues, and the adaptability across diverse codebases. The
rise of Large Language Models (LLMs) has embarked on
new solutions and findings in this landscape. LLMs, such
Haolin Jin, Linghan Huang and Huaming Chen are with the School of
Electrical and Computer Engineering, The University of Sydney, Sydney,
2006, Australia. (email: huaming.chen@sydney.edu.au)
Haipeng Cai is with the School of Electrical Engineering and Computer
Science at Washington State University, US
Jun Yan is with the School of Computing and Information Technology at
University of Wollongong, Australia
Bo Li is with the Computer Science Department at the University of
Chicago, USas GPT [3] and Codex [4], have demonstrated remarkable
capabilities in handling downstream tasks in SE, including
code generation, debugging, and documentation. These models
leverage vast amounts of training data to generate human-like
text, offering unprecedented levels of fluency and coherence.
Studies have shown that LLMs can enhance productivity in
software projects by providing intelligent code suggestions,
automating repetitive tasks, even generating entire code snip-
pets from natural language descriptions [5].
Despite their potential, there are significant challenges in
applying LLMs to SE. One major issue is their limited context
length [6], which restricts the model’s ability to comprehend
and manage extensive codebases, making it challenging to
maintain coherence over prolonged interactions. Hallucina-
tions is another main concern, where the model generates code
that appears plausible but is actually incorrect or nonsensi-
cal [7], potentially introducing bugs or vulnerabilities if not
carefully reviewed by experienced developers. Additionally,
the inability of LLMs to use external tools restricts their access
to real-time data and prevent them from performing tasks
outside their training scope. It diminishes their effectiveness in
dynamic environments. These limitations significantly impact
the application of LLMs in SE, and also highlight the need
for expert developeers to critically refine and validate LLM-
generated code for accuracy and security [8]. In complex
projects, the static nature of LLMs can hinder their ability
to adapt to changing requirements or efficiently incorporate
new information. Moreover, LLMs typically cannot interact
with external tools or databases, further limits their utility in
dynamic and evolving SE contexts.
To address these challenges, LLM-based agents have
emerged [9] [10], combining the strengths of LLMs with
external tools and resources to enable more dynamic and
autonomous operations. These agents leverage recent advance-
ments in AI, such as Retrieval-Augmented Generation (RAG)
and tool utilization, to perform more complex and contextually
aware tasks [11]. For instance, OpenAI’s Codex has been
integrated into GitHub Copilot [12], enabling real-time code
suggestions and completion within development environments.
Unlike static LLMs, LLM-based agents can perform a wide
range of tasks, such as autonomously debugging code by
identifying and fixing errors, proactively refactoring code to
enhance efficiency or readability, and generating adaptive test
cases that evolve alongside the codebase. These features make
LLM-based agents a powerful tool for SE, capable of handling
more complex and dynamic workflows than traditional LLMs.arXiv:2408.02479v1  [cs.SE]  5 Aug 2024JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2
FIG. 1: P APER NUMBER FOR LLM S AND LLM- BASED AGENT
BETWEEN 2020-2024
Historically, AI agents focused on autonomous actions
based on predefined rules or learning from interactions [13]
[14]. The integration of LLMs has presented new opportuni-
ties in this area, providing the language understanding and
generative capabilities needed for more sophisticated agent
behaviors. [10] shows that LLM-based agents are capable
of autonomous reasoning and decision-making, achieving the
third and fourth levels of WS (World Scope) [15], which
outlines the progression from natural language processing
(NLP) to general AI. In software engineering, LLM-based
agents show promise in areas such as autonomous debugging,
code refactoring, and adaptive test generation, demonstrating
capabilities that approach artificial general intelligence(AGI).
In this work, we present, to the best of our knowledge, a first
survey outlining the integration and transformation of LLMs
to LLM-based agents in the domain of SE. Our survey covers
six key themes in SE:
1)Requirement Engineering and Documentation : Cap-
turing, analyzing, and documenting software require-
ments, as well as generating user manuals and technical
documentation.
2)Code Generation and Software Development : Au-
tomating code generation, assisting in the development
lifecycle, refactoring code, and providing intelligent code
recommendations.
3)Autonomous Learning and Decision Making : High-
lighting the capabilities of LLM-based agents in au-
tonomous learning, decision-making, and adaptive plan-
ning within SE contexts.
4)Software Design and Evaluation : Contributing to design
processes, architecture validation, performance evalua-
tion, and code quality assessment.
5)Software Test Generation : Generating, optimizing, and
maintaining software tests, including unit tests, integra-
tion tests, and system tests.
6)Software Security & Maintenance : Enhancing security
protocols, facilitating maintenance tasks, and aiding in
vulnerability detection and patching.
In detail, we aim to address following research questions:
•RQ1: What are the state-of-the-art techniques and prac-
tices in LLMs and LLM-based agents for SE? (Sec-
FIG. 2: P APER DISTRIBUTION
tion. IV- IX)
•RQ1: What are the key differences in task performance
between LLMs and LLM-based agents in SE applica-
tions? (Section. IV- IX)
•RQ2: Which benchmark datasets and evaluation metrics
are most commonly used for assessing the performance of
LLMs and LLM-based agents in SE tasks? (Section. IV-
IX and Section. X)
•RQ3: What are the predominant experimental models and
methodologies employed when utilizing LLMs in SE?
(Section. X)
II. EXISTING WORKS AND THE SURVEY
STRUCTURE
A. Existing works
In recent years, large language models have been primarily
applied to help programmers generate code and fix bugs.
These models understand and complete code or text based
on the user’s input, leveraging their training data and reason-
ing capabilities. In previous survey papers, such as Angela
Fan’s research [8], there has not been much elaboration on
requirement engineering. As mentioned in the paper, software
engineers are generally reluctant to rely on LLMs for higher-
level design goals. However, with LLMs achieving remarkable
improvements in contextual analysis and reasoning abilities
through various methods like prompt engineering and Chain-
of-Thought (COT) [16], their applications in requirement
engineering are gradually increasing. Table I summarizes and
categorizes the tasks in requirement engineering. Many studies
utilize models for requirement classification and generation.
Since the collection primarily focuses on the latter half of
2023 and before April 2024, and some papers address multiple
tasks, the table does not reflect the exact number of papers we
have collected.
While other works have surveyed LLMs applications in
some SE tasks [17] [8] [18], they lack a wider coverage of the
general SE area to incorporate recent research developments.
More importantly, a focus of LLMs is the main contributions
of these works, but there is no distinguish the capabilitiesJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3
TABLE I: D ISTRIBUTION OF SE TASKS
Category LLMs LLM-based agents Total
Requirement
Engineering and
DocumentationRequirement Classification and Extraction (3)
Requirement Generation and Description (4)
Requirements Satisfaction Assessment (1)
Specification Generation (3)
Quality Evaluation (2)
Ambiguity Detection (2)Generation of Semi-structured Documents (1)
Generate safety requirements (1)
Automatically generating use cases based on
high-level requirements (1)
Automated User Story Quality Enhancement (2)19
Code Generation
and
software
developmentCode Generation Debugging (3)
Code Evaluation (2)
Implement HTTP server (1)
Enhancing Code Generation Capabilities (3)
Specialized Code Generation (2)
Human Feedback Preference Simulation (1)Automating the Software Development Process (5)
Large-Scale Code and Document Generation (1)
Tool and External API Usage (2)
Multi-Agent Collaboration and Code Refine (2)
Improving Code Generation Quality (2)23
Autonomous
Learning
and Decision
MakingMulti-LLMs Decision-Making (1)
Creativity Evaluation (1)
Self-Identify and Correct Code (1)
Judge Chatbot Response (1)
Mimics Human Scientific Debugging (1)
Deliberate Problem Solving(1)Collaborative Decision-Making and Multi-Agent
Systems (5)
Autonomous Reasoning and Decision-Making (7)
Learning and Adaptation Through Feedback (4)
Simulation and Evaluation of Human-like
Behaviors (2)24
Software Design
and EvaluationCreative Capabilities Evaluation (1)
Performance in SE Tasks (1)
Educational Utility and Assessment (1)
Efficiency Optimization (2)Automation of Software Engineering Processes (3)
Enhancing Problem Solving and Reasoning (4)
Integration and Management of AI Models and
Tools (3)
Optimization and Efficiency Improvement (2)
Performance Assessment in Dynamic Environments
(2)19
Software Test
GenerationBug Reproduction and Debugging (2)
Security Test (2)
Test Coverage (2)
Universal Fuzzing (1)Multi-agent Collaborative Test Generation (2)
Autonomous Testing and Conversational Interfaces
(3)11
Software Security
&
MaintainanceVulnerability Detection (6)
Vulnerability Repair (2)
Program Repair (4)
Robustness Testing (1)
Requirements Analysis (1)
Fuzzing (1)
Duplicate Entry (1)
Code Generation and Debugging (4)
Penetration Testing and Security Assessment (2)
Program Analysis and Debugging (1)Autonomous Software Development and
Maintenance (4)
Debugging and Fault Localization (4)
Vulnerability Detection and Penetration
Testing (3)
Smart Contract Auditing and Repair (2)
Safety and Risk Analysis (2)
Adaptive and Communicative Agents (1)39
between LLMs and LLM-based agents. We summarize the
difference between our work and others in Table II, this sur-
vey addresses these limitations by distinctly analyzing LLMs
and LLM-based agents applications across six SE domains,
providing a thorough and up-to-date review. From previous
research, it is evident that the performance of LLMs in various
applications and tasks heavily depends on the model’s inherent
capabilities [10]. More importantly, earlier surveys often
present findings from papers spanning in a wide range of
publication dates, leading to significant content disparities
for LLMs in different SE tasks. For instance, research in
requirement engineering was relatively nascent, resulting in
sparse content in this area in previous surveys. The recent rise
of LLM-based agents, with their enhanced capabilities and
autonomy, fills these gaps. By focusing on the latest researches
and clearly differentiating between LLMs and LLM-basedagents, our survey provides a thorough and in-depth overview
of how these technologies are applied and new opportunities
they bring to SE.
In summary, we have collected a total of 117 papers directly
relevant to this topic, covering the six SE domains men-
tioned earlier as shown in Figure.1. Our analysis distinguishes
between LLM and LLM-based agent contributions, offering
a comparative overview and addressing the limitations of
previous surveys. Considering the novelty nature of the LLM-
based agents field and the lack of standardized benchmarks,
this work seeks to offer a detailed review that can guide future
research and provide a clearer view of the potential of these
technologies in SE.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4
TABLE II: C OMPARISON BETWEEN OURWORK AND THE EXIST -
INGWORK FOR LLM INSE
Paper Year Domain Benchmarks MetricsAgent
in SEAgent
Distinction
[19] 2023 GenAI in SE ✓ ✓ ✓ ✗
[8] 2023 LLM in SE ✓ ✓ ✓ ✗
[18] 2023 Generation task by LLM in SE ✓ ✓ ✗ ✗
[20] 2023 LLM in syntax comprehension ✓ ✓ ✗ ✗
[21] 2024 LLM4Code in SE ✓ ✓ ✗ ✗
[17] 2024 LLM for process optimization in SE ✓ ✓ ✗ ✗
[22] 2024 Generation task by LLM in SE ✓ ✓ ✗ ✗
Ours 2024 LLM & LLM-based Agent in SE ✓ ✓ ✓ ✓
B. Methodology
The paper collection process primarily involved searching
DBLP and arXiv databases, focusing on recent studies from
the latter half of 2023 to May 2024. This approach ensured
the inclusion of the latest research. We filtered out non-LLM-
related papers and those with fewer than seven pages. To
further refine our selection, we used keywords in Table III
to search SE-related works. We then manually screened the
remaining papers to remove any with formatting errors or
student projects. Additionally, we employed a snowballing
search technique to capture significant works that might have
been missed initially. Overall, we identified 117 relevant
papers. Figure. 2 presents the distribution of these papers
across the six SE domains and the proportion of LLM-based
agents studies. However, some papers can be counted as multi-
class fields so the literature review in the figure is more than
117 in total.
C. Overall Structure of the Work
The remainder of this paper is organized as follows Sec-
tion 2 Introduces the architectures and background of LLMs
and LLM-based agents, including an overview of RAG, tool
utilization, and their implications for SE. Section 3-8 is the
comparative analysis which Summarizes and compares the
datasets, tasks, benchmarks, and metrics used in LLM and
LLM-based agents studies across the six SE domains. Section
9 is the general discussion, section 10 is the final conclusion.
III. P RELIMINARIES
In this section, we introduce the foundational concepts
of large language models, including the evolution of their
frameworks and an overview of their architectures. Subsequent
to this, we will discuss LLM-based agents, exploring both
single-agent and multi-agent systems. We will also covers
the background of these systems and their applications and
distinctions in the field of software engineering.
A. Large Language Model
There is an inherent connection between large language
models and natural language processing (NLP), with the
historical development of natural language technologies trac-
ing back to the 1950s. The earliest attempts to generate
language dialogues through machines using specific rules
can be traced to the period between 1950 and 1970. Theadvent of machine learning technologies in the 1980s and
the groundbreaking introduction of neural networks in the
1990s indicated a new era for NLP [23]. These advancements
facilitated significant progress in the NLP field, especially
in the development of technologies for text translation and
generation. The development of Long Short-Term Memory
(LSTM) and Recurrent Neural Networks (RNN) during this
period enabled more effective handling of the sequential nature
of language data [24] [25]. These models addressed challenges
associated with the lack of dependency in context, thereby
enhancing the application of NLP in various domains.
In 2017 the new framework called ”Transformer” introduced
by Google’s research team [26]. The transformer model based
on the self-attention mechanism which significantly improved
the effectiveness of language models. The inclusion of posi-
tional encoding not only solved the long-sequence dependency
issue but also enabled parallel computation, which was a con-
siderable improvement over previous models. In 2018, OpenAI
developed the Generative Pre-trained Transformer (GPT) [3],
a model based on the transformer architecture. The core idea
behind GPT-1 was to utilize a large corpus of unlabelled text
for pre-training to learn the patterns and structures of language,
followed by fine-tuning for specific tasks. Over the next two
years, OpenAI released GPT-2 and GPT-3 which increased the
parameter count to 175 billions and also demonstrated strong
capabilities in context understanding and text generation [27].
GPT-4 launched by OpenAI in 2023, represents a milestone
following GPT-3.5. Although GPT-4 maintains a similar pa-
rameter count of approximately 175 billion, its performance
and diversity have seen considerable improvements. Through
more refined training techniques and algorithm optimizations,
GPT-4 enhanced the capability of language understanding
and generation, particularly outperformed in handling complex
texts and special contexts. Compared to other contemporary
models like Google’s PaLM or Meta’s OPT, GPT-4 continues
to stand out in multi-task learning and logical consistency in
the text generation. While Google’s PaLM model boasts up
to 54 billion parameters, GPT-4 shows superior generalization
abilities across a broader range of NLP tasks [28]. On the
open-source large models, Meta’s OPT model with a parameter
size similar to the GPT-4 offers direct competition. Despite
OPT’s advantages in openness and accessibility, GPT-4 still
maintains a lead in specific application areas such as creative
writing and complex problem solving [29].
B. Model Architecture
There are three common LLM architectures, the Encoder-
Decoder architecture, exemplified by the traditional trans-
former model. This architecture comprises six encoders and six
decoders, data input into the system will first passes through
the encoder, where it undergoes sequential feature extraction
via the model’s self-attention mechanism. Subsequently, the
decoders utilize the word vectors produced by the encoders to
generate outputs, this technique is common to see in machine
translation tasks, where the encoder processes word vectors
from one language through several attention layers and feed-
forward networks, thereby creating representations of the con-
text. The decoder then uses this information to incrementallyJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5
TABLE III: K EYWORDS FOR SOFTWARE ENGINEERING TOPICS
Topic Keywords
Software Security & Maintenance Software security, Vulnerability detection, Automated Program repair, Self-debugging,
Vulnerability reproduction
Code Generation and Software Development Code generation, Automatic code synthesis, Code refactoring, Programming language
translation, Software development automation, Code completion, AI-assisted coding,
Development lifecycle automation
Requirement Engineering and Documentation Requirement engineering, Software requirements analysis, Automated requirement
documentation, Technical documentation generation, User manual generation, Doc-
umentation maintenance, Requirements modeling, Requirements elicitation
Software Design and Evaluation Software design automation, Architectural validation, Design optimization, Perfor-
mance evaluation, Code quality assessment, Software metrics, Design pattern recogni-
tion, Architectural analysis, Code structure analysis
Software Test Generation Test case generation, Automated testing, Unit test generation, Integration test genera-
tion, System test generation, Test suite optimization, Fault localization, Test mainte-
nance, Regression testing, Adaptive testing
Autonomous Learning and Decision Making Autonomous learning systems, Decision making, Adaptive planning, Project manage-
ment automation, Self-improving software, Autonomous software agents
construct the correct translated text. A recent example of this
architecture is the CodeT5+ model, launched by Salesforce
AI Research in 2023 [30]. This model is an enhancement
of the original T5 architecture, which designed to improve
performance in code understanding and generation tasks. It
incorporates a flexible architecture and diversified pre-training
objectives to optimize its effectiveness in these specialized ar-
eas. This development highlights the competency of Encoder-
Decoder architectures in tackling increasingly complex NLP
challenges.
The Encoder-only architecture, as the name suggests it
eliminates the decoder from the entire structure making the
data more compact. Unlike RNNs, this architecture is stateless
and uses a masking mechanism that allows input process-
ing without relying on hidden states, and also accelerating
parallel processing speeds and providing excellent contextual
awareness. BERT (Bidirectional Encoder Representations from
Transformers) is a representative model of this architecture,
this model is a large language model built solely on the
encoder architecture. BERT leverages the encoder’s powerful
feature extraction capabilities and pre-training techniques to
learn bidirectional representations of text, achieving outstand-
ing results in sentiment analysis and contextual analysis [31].
TheDecoder-only archiecture, in the transformer framework
primarily involves the decoder receiving processed word vec-
tors and generating output. Utilizing the decoder to directly
generate text accelerates tasks such as text generation and
sequence prediction. This characteristic with high scalability
is known as auto-regressiveness, which is why popular mod-
els like GPT use this architecture. In 2020, the exceptional
performance of GPT-3 and its remarkable few-shot learning
capabilities demonstrated the vast potential of the decoder-
only architecture [32]. Given the enormous computational
cost and time required to train a model from scratch, and
the exponential increase in the number of parameters, many
researchers now prefer to leverage pre-trained models for
further research. The most popular open-source pre-trained
language model LLaMA, developed by Meta AI also employs
the decoder-only architecture [33], as mentioned earlier, the
autoregressiveness and simplicity of this structure make themodel easier to train and fine-tune.
C. Large Language Model Based Agent
The concept of agents even trace back to the 19th century
and is often referred to as intelligent agents, envisioned to
possess intelligence comparable to humans. Over the past
few decades, as AI technology has evolved, the capabilities
of AI agents have significantly advanced, particularly with
the reinforcement learning. This development has enabled AI
agents to autonomously handle tasks and learn and improve
based on specified reward/punishment rules. Notable mile-
stones include AlphaGo [34], which leveraged reinforcement
learning to defeat the world champion in Go competition.
The success of GPT has further propelled the field, with
researchers exploring the use of large language models as the
”brain” of AI agents, thanks to GPT’s powerful text under-
standing and reasoning capabilities. In 2023, a research team
from Fudan University [10] conducted a comprehensive survey
on LLM-based agents, examining their perception, behavior,
and cognition. Traditional LLMs typically generate responses
based solely on given natural language descriptions, lacking
the ability for independent thinking and judgment. LLM-based
agents able to employ multiple rounds of interaction and
customized prompts to gather more information, which enable
the model to think and make decisions autonomously. In 2023,
Andrew Zhao proposed the ExpeL framework [35], which
utilizes ReAct as the planning framework combined with an
experience pool [36]. This allows the LLM to extract insights
from past records to aid in subsequent related queries, by
letting the LLM analyze why previous answers were incorrect,
it learns from experience to identify the problems.
At the same time, the application of LLM-based embodied
agents has also become a hot research area in recent years.
LLM-based Embodied Agents are intelligent systems that inte-
grate LLMs with embodied agents [37]. These systems can not
only process natural language but also complete tasks through
perception and actions in physical or virtual environments. By
combining language understanding with actual actions, these
agents can perform tasks in more complex environments. This
integration often involves using visual domain technologiesJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 6
to process and understand visual data and reinforcement
learning algorithms to train agents to take optimal actions in
the environment. These algorithms guide the agent through
reward mechanisms to learn how to make optimal decisions
in different situations, while the LLM acts as the brain to
understand user instructions and generate appropriate feed-
back. In 2023, Guanzhi Wang introduced VOYAGER, an open-
ended embodied agent with large language models [38]. It uses
GPT-4 combined with input prompts, an iterative prompting
mechanism, and a skill library enabling the LLM-based agents
to autonomously learn and play the game Minecraft, becoming
the first lifelong learning agent in the game.
FIG. 3: I LLUSTRATION OF COMMON DATA AUGMENTATION
METHODS
Nowadays, various agent systems are emerging and they
relies on large language models to make judgments, com-
bined with techniques such as few-shot learning and multi-
turn dialogue for model fine-tuning. However, due to the
lack of datasets and the novelty of LLM-based agents, many
researchers employ different methods for data augmentation.
Common approaches include synonym replacement, where
words in the text are replaced with synonyms from the same
domain to increase textual diversity; back-translation, where
the text is translated into another language and then back to the
original language to generate new texts with slightly different
grammatical structures and word choices; Paraphrasing refers
to the new dialogue which similar in context but slightly
different in expression created through manual or automated
means; Synthetic Data Generation refers to use pretrained
model to make a synthetic data generation as shown in Fig-
ure.3. In 2023, Chenxi Whitehouse explored using LLMs for
data augmentation to enhance the performance of multilingual
commonsense reasoning datasets, especially under conditions
of extremely limited training data [39]. The study employed
various LLMs (such as Dolly-v2, StableVicuna, ChatGPT, and
GPT-4) to generate new data. These models were prompted
to create new examples similar to the original data, thereby
increasing the diversity and quantity of training data. Prompt
engineering was mentioned as an essential skill for effectively
interacting with LLMs. By applying these prompt patterns,
users can efficiently customize their dialogues with LLMsensuring the generation of high-quality outputs and achieving
complex automated tasks. In 2023, Jules White introduced a
set of methods and patterns to enhance prompt engineering,
optimizing interactions with LLMs such as ChatGPT [40].
This study categorizes prompt engineering into five main
areas: Input Semantics, Output Customization, Error Identi-
fication, Prompt Improvement, and Interaction, to address a
wide range of problems and adapt to different fields.
One notable technique is Retrieval-Augmented Generation
(RAG), the input question undergoes similarity matching with
documents in an index library, attempting to find relevant re-
sults. If similar documents are retrieved, they are organized in
conjunction with the input question to generate a new prompt,
which is then fed into the large language model. Currently,
large language models possess long-text memory capabilities,
numerous studies have tested Gemini v1.5 against the Needle
In A Haystack (NIAH) evaluation to explore whether RAG
has become obsolete [41]. However, from various perspec-
tives such as cost and expense, RAG still holds significant
advantages (RAG could be 99 percent cheaper than utilizing
all tokens). Additionally, long texts can negatively impact
response performance, causing LLM to respond more slowly
when the input text is too long. Therefore, the advancements
in context length for LLM will not fully replace the role of
RAG but treated as complements between each other.
D. Single and Multi-Agents
Single agent systems leverage the capabilities of a LLM to
perform various tasks, these agents typically use a single LLM
to understand and respond to user queries, generate content,
or execute automated tasks based on predefined instructions.
Single agents are commonly used in scenarios where tasks
accept a general answer and do not require complex decision-
making. Examples include customer service chatbots, virtual
assistants for scheduling, and automated content generation
tools. However, single agents may struggle with dealing long
context inputs, leading to inconsistent or irrelevant responses.
The scalability of these systems is also limited when dealing
with tasks that require extensive knowledge or context, this
issue is often exacerbated by long texts as large language
models cannot fully comprehend and analyze overly lengthy
information in one turn. One of the primary issues with large
language models is hallucination [7]. Hallucination refers to
the generation of fabricated information or definitions by
LLMs, presented in seemingly logical and reasonable language
to the user. Most research papers on LLMs have stated this
problem, while prompt engineering or tool interventions can
mitigate the affect caused by hallucination, but it cannot be
entirely eliminated. In 2023, Ziwei Ji conducted an in-depth
study on hallucination in natural language generation [42].
This survey reviewed the progress and challenges in addressing
hallucinations in NLG, providing a comprehensive analysis of
hallucination phenomena across different tasks, including their
definitions and classifications, causes, evaluation metrics, and
mitigation methods.
Multi-agent systems involve the collaboration of multiple
LLMs or agents to tackle complex tasks effectively. TheseJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 7
systems fully utilize the advantages of multiple models, with
each model specializing in specific aspects of the task to re-
duce overhead caused by multi-processes in single agents, the
collaboration among agents allows for more sophisticated and
robust problem-solving capabilities. Due to their exceptional
capabilities, more researchers are beginning to explore the field
of Multi-LLM based agents and start applying into software
engineering domains. In 2024, a lot of researchers adopt the
multi-agent system into the practical experiments [43] [44].
Multi-agent systems address the limitations of single-agent
systems in the following ways:
•Enhanced Context Management : Multiple agents can
maintain and share context, generating more coherent and
relevant responses over long interactions.
•Specialization and Division of Labor : Different agents
can focus on specific tasks or aspects of a problem,
improving efficiency and effectiveness.
•Robustness and Error Correction : Collaborative agents
can cross-check and validate each other’s outputs, re-
ducing the likelihood of errors and improving overall
reliability.
•Contextual Consistency : Multi-agent systems can better
manage context over long dialogues. The collaboration
of multiple agents improves the efficiency of incident
mitigation.
•Scalability and Flexibility : These systems can integrate
specialized agents to scale and handle more complex
tasks. Through the division of labor among multiple
agents, the quality of code generation is improved.
•Dynamic Problem Solving : By integrating agents with
different expertise, multi-agent systems can adapt to a
wider range of problems and provide more accurate
solutions.
E. LLM in Software Engineering
Recently, there has been a shift towards applying general
AI models to specific vertical domains such as medical and
finance. In software engineering, new AI agents are emerging
that are more flexible and intelligent compared to previous
applications of LLMs, although they utilize different data
and experiments. This continuous innovation underscores the
transformative potential of AI agents across various fields,
these models excel in text understanding and generation,
promoting innovative applications in software development
and maintenance.
LLMs profoundly impact software engineering by facili-
tating tasks such as code generation, defect prediction, and
automated documentation. Integrating these models into devel-
opment workflows not only simplifies the coding process but
also reduces human errors. LLM-based agents enhance basic
LLM capabilities by integrating decision-making and interac-
tive problem-solving functions. These agents can understand
and generate result by interacting with other software tools
which optimize workflows, and make autonomous decisions
to improve software development practices. In 2023, Yann
Dubois introduced the AlpacaFarm framework [45], where
LLMs are used to simulate the behavior of software agentsin complex environments. Moreover, significant research has
been conducted in the field of automated program repair
(APR). In 2024, Islem Bouzenia introduced RepairAgent [46],
another LLM-based tool designed for automatic software
repair, this tool reduced the time developers spent on fixing
issues. Additionally, in 2023, Emanuele Musumeci demon-
strated a multi-agent LLM system [47], which involved a
multi-agent architecture where each agent had a specific role
in the generation of documents. This system significantly
improved handling complex document structures without ex-
tensive human supervision. Besides these, LLMs have made
outstanding contributions in software testing, software design,
and emerging fields such as software security and mainte-
nance.
Currently, there is no comprehensive and accurate definition
of the capabilities an LLM must exhibit to be considered an
llm-based agent. Since the application of LLMs in software
engineering is relatively broad and some frameworks already
behave certain levels of autonomy and intelligent, this study
defines the distinction between LLM and LLM-based agents
based on mainstream definitions and literature from the first
half of 2024. In this survey, an LLM architecture can be called
an agent when it satisfy the Table IV.
IV. R EQUIREMENT ENGINEERING AND AND
DOCUMENTATION
Requirement Engineering is a critical field within software
engineering and plays an essential role in the software devel-
opment process, its primary task is to ensure that the software
system meets the needs of all relevant stakeholders. Typically,
requirement engineering in project development involves many
steps, where developers need to fully understand the users’
needs and expectations to ensure that the development direc-
tion of the software system aligns with actual requirements.
The collected requirements are then organized and evaluated
by the development group. Requirements Specification is the
process of formally documenting the analyzed requirements,
the specification must be accurate and concise, and the
requirement verification must be conducted to ensure that
developers are building what users need and that it aligns
with the specifications. Requirement engineering also includes
requirement management, a task that spans the entire software
development life-cycle, developers need to continuously track,
control, and respond to any changes occurring during devel-
opment, ensuring that these changes do not negatively impact
the project’s progress and overall quality.
A. LLMs Tasks
In the field of requirement engineering, LLMs have demon-
strated significant potential in automating and enhancing tasks
such as requirement elicitation, classification, generation, spec-
ification generation, and quality assessment. Requirement
classification and extraction is a crucial task in requirement
engineering during the development process. It is common
to encounter situations where clients present multiple re-
quirements at once, necessitating manual classification by
developers. By categorizing requirements into functional andJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 8
TABLE IV: C RITERIA FOR LLM- BASED AGENT
Criteria
1) The LLM serves as the brain (the center of information processing and generation of thought).
2) The framework not only relies on the language understanding and generation capabilities of LLMs but also possesses decision-
making and planning abilities.
3) If tools are available, the model can autonomously decide when and which tools to use and integrate the results into its predictions
to enhance task completion efficiency and accuracy.
4) The model can select the optimal solution from multiple homogeneous results (the ability to evaluate and choose among various
possible solutions).
5) The model can handle multiple interactions and maintain contextual understanding.
6) The model has autonomous learning capabilities and adaptability.
non-functional requirements, developer can better understand
and manage them, thanks to the strong performance of LLMs
in classification tasks, many relevant frameworks have been
developed. The PRCBERT framework, utilizing the BERT
pre-trained language model, transforms classification problems
into a series of binary classification tasks through flexi-
ble prompt templates, significantly improving classification
performance [48]. Studies have shown that the PRCBERT
achieved an F1 score of 96.13% on the PROMISE dataset
which outperform the previous state-of-arts NoRBERT [49]
and BERT-MLM models [31]. Additionally, the application
of ChatGPT in requirement information retrieval has shown
promising results, by classifying and extracting information
from requirement documents, ChatGPT achieved comparable
or even better Fβscores under zero-shot settings, particularly
in feature extraction tasks, where its performance surpassed
baseline models [50]. As seen in Table I, there is also
substantial literature and research on using LLMs to automat-
ically generate requirements and descriptions in requirement
engineering.
By automating the generation and description of require-
ments, the efficiency and accuracy of requirement elicitation
can be improved. Research indicates that LLMs hold signif-
icant potential in requirements generation task. For example,
using ChatGPT to generate and gather user requirements,
studies found that participants with professional knowledge
could use ChatGPT more effectively, indicating the influence
of domain expertise on the effectiveness of LLM-assisted
requirement elicitation [51]. The study employed qualitative
assessments of the LLMs’ output against predefined criteria for
requirements matches, including full matches, partial matches,
and the relevancy of the elicited requirements, although their
success varied depending on the complexity of the task and the
experience of the users, the result showing that LLMs could
effectively assist in eliciting requirements, and its particularly
useful in identifying, and suggesting requirements based on the
large corpus of training data they provided. The SRS (Software
Requirement Specification) generation is an important task
which the developer normally spent a lot of time to refine
and verified. In [52], researchers use both iterative prompting
and a single comprehensive prompt to assess the performanceof LLMs to generate SRS. The experiment conducted on GPT-
4 and CodeLlama-34b one close-source LLM and one open-
source LLM for comprehensive evaluation, the generated SRS
will compare with human-crafted SRS and finally scored by
the likert scale. The result indicate that, the human-generated
SRS was overall superior, but CodeLlama often came close,
sometimes outperforming in specific categories. The CodeL-
lama scored higher in completeness and internal consistency
than GPT-4 but less concise, so this stuy demonstrated the
potential of using fine-tuned LLMs to generate SRS and
increase the overall project productivity. Another paper also
explores using LLMs for generating specifications. In [53], the
authors introduce a framework called SpecGen for generating
program specifications. The framework primarily uses GPT-
3.5-turbo as the base model and employs prompt engineering
combined with multi-turn dialogues to generate the specifica-
tions. SpecGen applies four mutation operators to modify these
specifications and finally uses a heuristic selection strategy to
choose the optimal variant. The results show that SpecGen
can generate 70% of the program specifications, outperforming
traditional tools like Houdini [54] and Daikon1.
Furthermore, designing prompt patterns can significantly
enhance LLMs’ capabilities in tasks such as requirement
elicitation and system design. The paper provides a catalog
of 13 prompt patterns, each aimed at addressing specific
challenges in software development [55]. The experiments
test the efficacy of these patterns in real world scenarios to
validate their usefulness. By applying different prompt pat-
terns, the study found that these patterns could help generate
more structured and modular results and reduce common
errors. Automated requirement completeness enhancement is
another important benefit brought by the LLMs in requirement
generation. The study [56] use BERT’s Masked Language
Model (MLM) can detect and fill in missing parts in natural
language requirements, significantly improving the complete-
ness of requirements. BERT’s MLM achieved a precision of
82%, indicating that 82% of the predicted missing terms were
correct.
There is also the application of LLMs in ambiguity detection
tasks, aimed at detecting ambiguities in natural language
1https://github.com/codespecs/daikonJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 9
requirement documents to improve clarity and reduce misun-
derstandings. This study primarily aims to address the issue
of detecting term ambiguities within the same application
domain (where the same term has different meanings in
different domains). Although current models generally possess
excellent contextual understanding capabilities, this was a
common problem in machine learning at that time. This study
provides an excellent paradigm for the subsequent application
of LLMs in requirements engineering, study demonstrated the
transformer-based machine learning models can effectively
detect and identify ambiguities in requirement documents,
thereby enhancing document clarity and consistency. The
framework utilizes BERT and K-means clustering to identify
terms used in different contexts within the same applica-
tion domain or interdisciplinary project requirements docu-
ments [57]. In recent two years, more and more researchers
use LLMs to help them to evaluate the requirement docu-
mentations, quality assessment tasks ensure that the generated
requirements and code meet expected quality standards. The
application of ChatGPT in user story quality evaluation has
shown potential in identifying quality issues, but it requires
further optimization and improvement [58]. A similar study
use LLM to automatically process the requirement satisfac-
tion assessment, and evaluate whether design elements fully
covered by the given requirements, but the the researcher
indicated the necessity of further verification and optimization
in practical applications [59].
B. LLM-based Agents Tasks
Currently the application of LLM-based agents in the re-
quirement engineering is till quite nascent, but there are some
useful researches to help us to see the potential possibility.
LLM-based agents bring both efficiency and accuracy for
tasks like requirement elicitation, classification, generation,
and verification. Compared to traditional LLMs, these systems
exhibit higher levels of automation and precision through
task division and collaboration. The application of multi-agent
systems in semi-structured document generation has shown
significant effectiveness. In [60], a multi-agent framework
is introduced that combines semantic recognition, informa-
tion retrieval, and content generation tasks to streamline the
creation and management of semi-structured documents in
the public administration domain. The proposed framework
involves three main types of agents: Semantics Identification
Agent, Information Retrieval Agent, and Content Generation
Agent. By avoiding the overhead of a single model, each agent
is assigned a specific task with minimal user intervention,
following the designed framework and workflow.
Additionally, the AI-assisted software development frame-
work (AISD) also showcases the autonomy brought by the
LLM-based agents in requirement engineering. [61] proposes
the AISD framework, which continuously improves and op-
timizes generated use cases and code through ongoing user
feedback and interaction. In the process of the experiment, hu-
mans need to first give a fuzzy requirement definition, and then
LLM-based agent will improve the requirement case according
to this information, and then design the model and generatethe system according to the case, and then the generated
results will let humans judge whether the requirements are
met or not. The study results indicate that AISD significantly
increased use case pass rates to 75.2%, compared to only
24.1% without human involvement. AISD demonstrates the
agents’ autonomous learning ability by allowing LLMs to
generate all code files in a single session, continually refining
and modifying based on user feedback. This also ensures code
dependency and consistency, further proving the importance
of human involvement in the requirement analysis and system
testing stages.
Furthermore, in generating safety requirements for au-
tonomous driving, LLM-based agents have shown unique
advantages by introducing multimodal capabilities. The sys-
tem employs LLMs as automated agents to generate and
refine safety requirements with minimal human intervention
until the verification stage, which is unattainable with only
LLMs. [62] describes an LLM prototype integrated into the ex-
isting Hazard Analysis and Risk Assessment (HARA) process,
significantly enhancing efficiency by automatically generating
specific safety-related requirements. The study through three
design iterations progressively improved the LLM prototype’s
efficiency by completing within a day compared to months
manually. In agile software development, the quality of user
stories directly impacts the development cycle and the realiza-
tion of customer expectations. [63] demonstrates the successful
application of the ALAS system in six agile teams at the Aus-
trian Post Group IT. The ALAS system significantly improved
the clarity, comprehensibility, and alignment with business
objectives of user stories through automated analysis and
enhancement. The entire agent framework allows the model to
perform specific roles in the Agile development process, the
study results indicated that the ALAS-improved user stories
received high satisfaction ratings from team members.
C. Analysis
The application of LLM-based agents in requirement engi-
neering has demonstrated significant efficiency improvements
and quality assurance. Through multi-agent collaboration and
automated processing, these systems not only reduce manual
intervention but also enhance the accuracy and consistency of
requirement generation and verification. We can see that the
tasks of LLM-based agents are no longer limited to simply
generating requirements or filling in the gaps in descriptions.
Instead, they involve the implementation of an automated
process, with the generation of requirement documents being
just one part of it, integrating LLM into agents enhances the
overall system’s natural language processing and reasoning
capabilities. In the real-world application, many tasks can no
longer be accomplished by simple LLMs alone, especially
for high-level software design. The emergence of LLM-based
agents addresses this issue through a multi-agent collaborative
system centered around LLMs, these agents continuously ana-
lyze and refine the deficiencies in the requirement documents,
this is might be the main application trend of LLM-based
agents in requirements engineering in the future.
The application of LLM-based agents in requirements engi-
neering is still relatively limited, with most efforts focusing onJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 10
FIG. 4: I LLUSTRATION OF COMPARISON FRAMEWORK BETWEEN
LLM- BASED AGENT AND LLM INUSERSTORY REFINEMENT
leveraging the collaborative advantage of multi-agent systems
to generate and refine requirements engineering documents.
As illustrated in Figure.4, which roughly simulates the ar-
chitectures presented in [58] and [63], both applied to the
generation and refinement of user stories, we can clearly
compare the differences between the two architectures. On
the left is the architecture of the LLM-based agent, while on
the right is the approach of using prompt engineering and
LLMs alone to refine user stories. The figure omits more
detailed and complex aspects of the architecture to highlight
the core differences between the two approaches. LLM-based
agents can continuously improve from different professional
perspectives by utilizing a shared database. Although there are
not many papers on LLM-based agents, we can observe the
trend and benefits of transitioning from LLMs to LLM-based
agents.
D. Benchmarks
Requirement engineering, unlike tasks such as bug fixing
and code generation, does not have an abundance of public
datasets available, such as HumanEval which commonly used
for code generation assessment. Most training datasets for
models in requirement engineering are self-collected by the
authors and not all of them are open-sourced on Huggingface,
resulting in a limited amount of dataset in requirement engi-
neering. For instance, some papers do not mention a specific
benchmark dataset but instead focus on practical examples
and case studies to demonstrate the effectiveness of proposed
prompt patterns [55]. The researcher Let actual developers
and requirements engineers use the generated requirements
documents and code to evaluate its accuracy, usability, and
completeness. User feedback will be collected to further
improve and optimize the prompt mode.In [50], four datasets are primarily used, characterized by
average length, type-token ratio (TTR), and lexical density
(LD). The NFR Multi-class Classification dataset includes 249
non-functional requirements (NFRs) across 15 projects from
the PROMISE NFR dataset. The App Review NFR Multi-
label Classification dataset comprises 1800 app reviews from
Google Play and Apple App Store, labeled with various NFRs.
The Term Extraction dataset contains 100 smart home user
stories with 250 manually extracted domain terms. Lastly,
the Feature Extraction dataset consists of 50 app descrip-
tions across 10 application categories with manually identified
feature phrases. In [56], the PURE dataset consisting of 40
requirements specifications totaling over 23,000 sentences,
is used to test BERT’s ability to complete requirements.
In [64], the benchmark dataset comprised 36 responses to
six questions: 6 responses generated by ChatGPT and 30
responses from five human RE experts (each expert provided 6
responses). These datasets serve as evaluation metrics for the
models. Combining these papers, we can see that benchmark
datasets for LLMs in requirement engineering mainly include
various classifications of software requirements and functional
and non-functional requirements to aid and assist models in
learning this domain, the dataset utilization are quite flexible
and diversifies.
In LLM-based agents’ research in requirement engineering,
the selection and construction of datasets are also important.
In [47], the dataset mainly consists of semantic templates
from the public administration domain. These templates cover
various semi-structured forms of administrative documents,
such as official certificates and public service forms. Although
the detailed composition of the dataset is not specified, it can
be inferred that these templates include a large number of
practical cases and contextual information to ensure that the
documents generated by the multi-agent system meet actual
needs.
Additionally, in [61], the CAASD (Capability Assessment
of Automatic Software Development) dataset is introduced.
This specially constructed benchmark dataset is used to
evaluate the capabilities of AI-assisted software development
systems. The CAASD dataset contains 72 tasks from various
domains, such as small games and personal websites, each
with reference use cases to define system requirements. The
purpose of constructing this dataset is to provide a comprehen-
sive evaluation benchmark that covers different types of devel-
opment tasks, testing the performance of LLM-based agents
in diverse tasks. In [62], the study mainly uses Design Science
Methodology to design and evaluate the LLM prototype but
does not mention a specific dataset, focusing on validating
the model’s effectiveness through practical application and
case studies. Despite the lack of detailed dataset descriptions,
this approach emphasizes iterative improvement and practical
application to ensure that the safety requirements generated
by LLM-based agents meet high safety standards. Finally,
in [63], 25 synthetic user stories are used, derived from a
mobile delivery application project. The study evaluates the
ALAS system’s effectiveness by testing it in six agile teams
at the Austrian Post Group IT. Although these user stories are
synthetic data designed for the experiment, they realisticallyJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 11
TABLE V: E VALUATION METRICS IN REQUIREMENT ENGINEERING AND DOCUMENTATION
Reference Paper Benchmarks Evaluation Metrics Agent
[51] Evaluation on ActApp Precision and Recall for Elicitation
Clarity, Consistency, and Compliance
Completeness and accuracy of acceptance
criteriaNo
[50] NFR, Smarthome user stories Precision, recall, and F \beta (F1 or F2) No
[56] PURE Precision, F1 Score, Recall No
[52] Not Specified Likert Scale No
[55] Case studies Accuracy in identifying missing requirements.
Quality and Modularity of generated code.
Correctness of refactoring suggestions.
Efficiency in automating software engineering
tasksNo
[64] 36 responses to the six questions Abstraction, Atomicity, Consistency, Correctness
Unambiguity, Understandability, FeasibilityNo
[48] PROMISE NFR-Review, NFR-SO F1 Score, Weighted F1 Score (w-F)) No
[53] SV-COMP, SpecGenBench Number of Passes, Success Probability
Number of Verifier Calls, User RatingNo
[65] Jdoctor-data, DocTer-data,
SpecGenBench, SV-COMPAccuracy, Precision, Recall, F1 Score No
[58] Benchmark evaluations of user
stories using the AQUSA toolAgreement Rate, Precision, Recall, Specificity,
F1 ScoreNo
[57] Crawled Documents from Wikipedia Manual validation No
[59] CM1, CCHIT, Dronology, PTC-A,
PTC-BF\beta score, Mean Average Precision (MAP) No
[49]] PROMISE NFR Precision (P), Recall (R), F1-score (F1),
Weighted average F1-score (A)No
[66] CS-specific corpora, PURE Contextual Clarity, User Feedback No
[60] Semantic Templates from Public
AdministrationAccuracy, Prompt Conformity, User Intervention
Frequency, Hallucination RateYes
[61] CAASD Pass Rate, Token Consumption Yes
[62] AEB, CAEM Performance Accuracy and Relevance, Efficiency,
Feedback from industryYes
[63] 25 Synthetic User Stories for a
Mobile Delivery ApplicationIndependence, Negotiability, Value, Estimability,
Smallness, Testability.
Survey among professionalsYes
reflect the requirements in actual projects, providing a valuable
testing benchmark.
From these papers, it can be seen that the selection and
construction of datasets in LLM-based agents’ research in
requirement engineering often rely on practical projects and
case studies, lacking standardization and large-scale datasets.
Compared to LLM literature, the datasets used are broader
and in a higher level like an actual system’s files, not lim-
ited to the classification of non-functional requirements and
pure software requirement specifications. Researchers focus
more on validating the model’s effectiveness through practical
application and iterative improvement to enhance model per-
formance. While this approach is flexible and targeted, it also
highlights the field’s shortcomings in dataset standardization
and scaling. In the future, with more public datasets being
constructed and shared, the application of LLM-based agents
in requirement engineering is expected to achieve broader and
deeper development.
E. Evaluation Metrics
In the field of requirement engineering, LLMs and LLM-
based agents are evaluated using various metrics. These met-
rics not only include traditional indicators such as precision,recall, and F1 score but also more specific indicators tailored to
the unique nature of requirement engineering. Through these
evaluations, we can see how these models are assessed and
how they are changing the practice of requirement engineering.
The specific evaluation metrics are detailed in Table V. In [51],
while precision and recall are fundamental for evaluating the
effectiveness of information retrieval, additional evaluations
of clarity, consistency, and compliance are included, which
are crucial quality indicators in requirement engineering. This
multidimensional evaluation method not only measures the
operational performance of LLMs but also examines their
ability to maintain the quality of requirement specifications.
Through this approach, LLMs have demonstrated their value in
automating and optimizing the requirement elicitation process,
enhancing both efficiency and the reliability of results. The
paper [52] use the Likert Scale to measure the quality of
generated specifications, the specification will be scored by its
Unambiguous, Understandable, Conciseness, etc. The Likert
Scale will be scored from 1 to 5 of agreements.
For agent-based LLMs, as demonstrated in [63], the evalu-
ation extends to assessing the independence and negotiability
of the agents, elevating their functionality to a new level.
These agents provide technical solutions and also interact
with users, autonomously adjusting to meet specific projectJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 12
needs, thus resembling collaborative partners. This capability
makes LLM-based agents valuable in requirement engineering
in the requirement management and decision optimization,
also highlight that LLMs typically focus on improving the
accuracy and efficiency of specific tasks, while LLM-based
agents exhibit higher capabilities in autonomy and adaptability.
In Table V, we can see that the application of LLMs in
requirements engineering typically requires common metrics
such as F1 Score to evaluate model’s performance. However,
for LLM-based agents, the evaluation focus shifts from the per-
formance of requirement document generation to the quality of
the final product. Therefore, evaluation metrics emphasize user
satisfaction, such as pass rate, feedback, etc. Essentially, LLM-
based agents still leverage LLMs themselves to achieve higher-
level development, and depends pretty much on the nature of
the task. In summary we can conclude that, the characteristics
of agent models both reflect their complex decision-making
and learning abilities also reveal their potential advantages
in collaboration with human or other tools to provide higher
scalability and flexibility design. This phenomenon implies
the potential opportunities that the methods of requirement
elicitation and processing in future software development will
become more efficient, precise, and continuous refine to better
align with stakeholder’s needs by using LLM-based agents.
V. C ODE GENERATION AND SOFTWARE DEVELOPMENT
Code generation and software development are core areas
within software engineering which plays a crucial role in
the software development process. The primary objective of
using LLMs in code generation is to enhance development
efficiency and code quality through automation processes,
thereby meeting the needs of both developers and users.
In recent years, the application of LLMs in code generation
and software development has made significant progress, this
has changed the way developers work and revealed a shift in
automated development processes. Compared to requirement
engineering, research on the application of LLMs and LLM-
based agents in code generation and software development
is more extensive and in-depth. Using natural language pro-
cessing and generation technologies, LLMs can understand
and generate complex code snippets, assisting developers in
automating various stages from code writing and debugging
to software optimization. The decoder-based large language
models such as GPT-4 have shown significant potential in code
generation by providing accurate code suggestions and auto-
mated debugging, greatly improving development efficiency.
Recently, the application of LLM-based agents in software
development is also gaining attention, these intelligent agents
can not only perform complex code generation tasks but also
engage in autonomous learning and continuous refinement,
thereby offering flexible assist in dynamic development en-
vironments. Tools like GitHub Copilot [12], which integrate
LLMs, have already demonstrated their advantages in enhanc-
ing programming efficiency and code quality.
A. LLMs Tasks
Large language models have optimized various tasks in
code generation and software development through automa-tion and reasoning, covering areas such as code generation,
debugging, code comprehension, code completion, code trans-
formation, and multi-turn interactive code generation. The
primary method is generating executable code from natural
language descriptions, where models utilize previously learnt
code snippets or apply few-shot learning to better understand
user requirements. Nowadays the AI tools integrates deeply
with IDEs like Visual Studio Code2and JetBrains3to enhance
code writing and translation tasks such as OpenAI’s Codex
model [67]. Codex fine-tuned on public code from GitHub,
demonstrate the capability to generate Python functions from
doc-strings also outperformed other similar models on the
HumanEval benchmark.
In [68], researchers comprehensively evaluated the perfor-
mance of multiple LLMs on L2C(language to code) tasks.
The results showed that GPT-4 demonstrates strong capability
in tasks such as semantic parsing, mathematical reasoning,
and Python programming. With instruction tuning and support
from large-scale training data, the model can understand and
generate code that aligns with user intent, achieving high-
precision code generation. Applying LLMs to text-to-database
management and query optimization is also a novel research
direction in natural language to code generation task. By
converting natural language queries into SQL statements,
LLMs help developers quickly generate efficient database
query code. In [69], proposed the SQL-PaLM framework
which significantly enhances the execution accuracy and exact
match rate for text-to-SQL tasks through a few-shot prompt
and instruction fine-tuning, providing an effective solution for
complex cross-domain SQL generation tasks. The improve-
ments in accuracy and exact match achieved in the SQL-PaLM
model are considered state-of-the-art (SOTA) in tested bench-
marks, the SQL-PaLM performed promise results comparing
with existing methods such as T5-3B + PICARD, RASAT +
PICARD, and even GPT-4, achieving the highest test accuracy
of 77.3% and an execution accuracy of 82.7%. Multilingual
code generation is another important application of LLMs,
particularly suited to the transformer architecture. In [70],
researchers introduced the CodeGeeX model, which was pre-
trained on multiple programming languages and performed
well in multilingual code generation and translation tasks.
Experimental results showed that CodeGeeX outperformed
other multilingual models on the HumanEval-X benchmark.
Although current LLMs possess excellent code generation
capabilities, with accuracy and compile rates reaching usable
levels, the quality of generated code often depends on the
user’s prompts. If the prompts are too vague or general,
the LLM typically struggles to understand the user’s true
requirements, making it difficult to generate the desired code
in a single attempt. In [71], researchers introduced ”print
debugging” technique, using GPT-4 to track variable values
and execution flows, which enhancing the efficiency and ac-
curacy by using in-context learning techniques. This method is
particularly suitable for medium-difficulty problems on Leet-
code, compared to the rubber duck debugging method, print
2https://code.visualstudio.com/
3https://www.jetbrains.com/JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 13
debugging improved performance by 1.5% on simple Leetcode
problems and by 17.9% on medium-difficulty problems.
Additionally, the application of LLMs in improving pro-
gramming efficiency has garnered widespread attention, the
tools like GitHub Copilot which integrating OpenAI’s Codex
model, provide real-time code completion and suggestions dur-
ing coding. According to [72], researchers present a controlled
experiment with the Github Copilot, the result demonstrated
that with developers completing HTTP server tasks 55.8%
faster when using Copilot. Another similar study also using
LLM to be the programmer tools, in [73], researchers intro-
duced the INCODER model which capable of both program
synthesis and editing. By leveraging bidirectional context,
the model performs well in both single-line and multi-line
code filling tasks, providing developers with smarter code
editing tools. This real-time code generation and completion
functionality not only improves programming efficiency but
also reduce the burden on developers, allowing them to focus
on higher-level design which is a common problem in software
development where substantial workforce and time are wasted
on tedious coding tasks.
The multi-turn program synthesis tasks represent a signif-
icant breakthrough for LLMs in handling complex program-
ming tasks, in [74], researchers introduced the CODEGEN
model, which iteratively generates programs through multiple
interactions, significantly improving program synthesis quality
and making the development process more efficient and ac-
curate. By gradually generating and continuously optimizing
code at each interaction, LLMs can better understand user
intent and generate more precise and optimized code. In the
experiments, comparisons were made with the Codex model,
which was considered state-of-the-art in code generation at
the time. CODEGEN-MONO 2.7B outperformed the Codex
model of equivalent outcome in pass@k metrics for both k=1
and k=10. Furthermore, CODEGEN-MONO 16.1B exhibited
performance that was comparable to or better than the best
Codex model on certain metrics, further demonstrating its
SOTA performance in the code generation. By iteratively
generating and optimizing code, LLMs continuously improve
their output quality. In [75], researchers proposed the Cycle
framework, which enhances the self-improvement capability of
code language models by learning from execution feedback,
improving code generation performance by 63.5% on multiple
benchmark datasets. Although Cycle has a certain degree of
autonomy, its decision-making and planning capabilities are
mainly limited to code generation and improvement tasks
without overall planning, and the execution sequence is com-
pletely followed a fixed pattern, so it’s better to classified as
an advanced LLM application.
B. LLM-based Agents Tasks
LLM-based agents have shown significant potential and
advantages by substantially improving task efficiency and
effectiveness through multi-agent collaboration. Unlike tradi-
tional LLMs, LLM-based agents adopt a division of labor
approach, breaking down complex tasks into multiple subtasks
handled by specialized agents, this method can enhance taskefficiency and improves the quality and accuracy of generated
code to mitigate the hallucination from the single LLM.
In [76], researchers proposed a self-collaboration framework
where multiple ChatGPT (GPT-3.5-turbo) agents act as differ-
ent roles to collaboratively handle complex code generation
tasks. Specifically, the introduction of Software Development
Methodology (SDM) divides the development process into
three stages: analysis, coding, and testing. Each stage is
managed by specific roles, and after completing their tasks,
each role provides feedback and collaborates with others to
improve the quality of the generated code. Experiment shows
that this self-collaboration framework significantly improves
performance on both the HumanEval and MBPP benchmarks,
with the highest improvement reaching 29.9% in HummanEval
compared to the SOTA model GPT-4. This result demon-
strating the potential of collaborative teams in complex code
generation tasks. Although it lacks external tool integration
and dynamic adjustment capabilities, this framework exhibits
common characteristics of LLM-based agents, such as role dis-
tribution, self-improvement ability, and excellent autonomous
decision-making, these combined capabilities qualify it to be
considered an LLM-based agent. Similarly, In [77], the LCG
framework improved code generation quality also through
multi-agent collaboration and chain-of-thought techniques,
once again demonstrating the effectiveness of multi-agent
collaboration in the software development process.
The limitations of context windows was not discussed in
previous studies, this has been thoroughly explored in a 2024
by University of Cambridge team. In [78], researchers intro-
duced the L2MAC framework, which dynamically manages
memory and execution context through a multi-agent system
to generate large codebases, and achieved SOTA performance
in generating large codebases for system design tasks. The
framework is primarily divided into the following components:
the processor, which is responsible for the actual generation
of task outputs; the Instruction Registry, which stores program
prompts to solve user tasks; and the File Storage, which
contains both final and intermediate outputs. The Control Unit
periodically checks the outputs to ensure that the generated
content is both syntactically and functionally correct. The
researchers conducted multiple experiments and compared
with many novel methods like GPT-4, Reflexion, and Auto-
GPT, achieving a Pass@1 score of 90.2% on the HumanEval
benchmark, showcasing its superior performance in generating
large-scale codebases.
Recently, many studies have begun to use LLM-based
agents to simulate real software development processes, the
paper [79] introduced the MetaGPT framework, which en-
hanced problem-solving capabilities through standard operat-
ing procedures (SOPs) encoded in multi-agent collaboration.
The entire process of the multi-collaboration framework sim-
ulates the waterfall life-cycle of software development, with
each agent playing different roles and collaborating to achieve
the goal of automating software development. LLM-based
agents have also shown strong ability in automated software
development, [80] proposed a multi-GPT agent framework
that automates tasks such as project planning, requirement
engineering, software design, and debugging, illustrating theJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 14
potential for automated software development. Similarly [81]
introduced the model called CodePori, which is a novel
model designed to automate code generation for extensive
and complex software projects based on natural language
prompts. In [82] the AgentCoder framework collaborates with
programmer agents, test design agents, and test execution
agents to generate and optimize code, outperforming existing
methods, achieved SOTA performance on the HumanEval-ET
benchmark with pass@1 of 77.4% compared to the previous
state-of-the-art result of 69.5%, this result showcasing the
advantages of multi-agent systems in code generation and
testing.
The purpose of integrating LLMs into agents from many
framework is to enhance the self-feedback and reflection
capabilities of the entire agent system. Because the current
open-source LLMsgenerally have much lower capabilities in
this aspect compared to proprietary models, the emergence
of LLM-based agents can help bridge the gap between open-
source models and the advanced capabilities of proprietary
systems like GPT-4. [83] introduced the OpenCodeInterpreter
framework, which improved the accuracy of code generation
models by integrating code generation, execution, and human
feedback. Based on CodeLlama and DeepSeekCoder, this
framework performed close to the GPT-4 Code Interpreter on
the HumanEval and MBPP benchmarks. The abbility of using
external tools or APIs is another significant advantage of LLM-
based agents, [84] proposed the Toolformer model, which
significantly enhanced task performance by learning to call
APIs through self-supervision. The framework Based on GPT-
J (6.7B parameters) achieved significant performance improve-
ments across multiple benchmark tasks, demonstrating the
possibility of LLM-based agent brought by the external tool,
the diverse choice of tools and architectures, allowing LLMs
to continuously learn new things and improve themselves.
Similarly, [85] enhanced LLMs’ interaction with external
APIs through the ToolLLM framework, outperforming Text-
Davinci-003 and Claude-2 on the ToolBench and APIBench
benchmarks and excelling in multi-tool instruction processing.
C. Analysis
The main differences between LLM-based agents and tra-
ditional LLMs in software development applications mainly
focus on the efficiency and autonomy, particularly in task
division and collaboration. Traditional LLMs typically use
a single model to handle specific tasks, such as generating
code from text and code completion. However, this approach
has limitations when dealing with complex tasks, especially
regarding context window restrictions and the need for con-
tinuous feedback. LLM-based agents handle different subtasks
through collaboration with clear division of labor, thereby
enhancing task efficiency and quality. For example, in a code
generation task, one agent generates the initial code, another
designs test cases, and a third executes tests and provides
feedback, thus achieving iterative optimization. Through task
division, multi-agent systems, and tool integration, LLM-based
agents can tackle more complex and broader tasks, improving
the quality and efficiency of code generation. This approachovercomes the limitations of traditional LLMs also provides
new directions and ideas for future software development
research and applications, to frees programmers from the
boring test suite generation.
FIG. 5: I LLUSTRATION OF COMPARISON FRAMEWORK BETWEEN
LLM- BASED AGENT AND LLM INCODE GENERATION AND SOFT-
WARE DEVELOPMENT
In software engineering task handling, there are subtle
differences between LLMs and LLM-based agents in terms
of task focus, approach, complexity and scalability, automa-
tion level, and task management. LLMs primarily focus on
enhancing the code generation capabilities of a single LLM,
including debugging, precision, evaluation. These methods
typically improve specific aspects of code generation or eval-
uation through a single model, concentrating on performance
enhancement within existing constraints, such as context
windows and single-task execution. In contrast, LLM-based
agents emphasize handling more complex and broader tasks
through the collaboration of multiple specialized LLMs or
frameworks, integrating tool usage, iterative testing, and multi-
agent coordination to optimize the whole development process
and easily surpass the state-of-art model in common bench-
marks. The emeergence of multi-agent systems also brings
more possibilities, this system can imitate the real software
developer to perform the scrum development. Figure. 5 utilize
studies [77] and [75] showcase the differences between LLM-
based agents and LLMs on the same code generation task.
The LLM-based agents system are able to perform multi-agent
collaboration and simulate the real scrum development team
in the industry. In contrast the LLMs on the right are normally
use multi-LLMs to analysis mistakes from the test cases, and
refine the initial generated code, but they lack autonomy and
efficiency, as the test cases are manually generated by humans.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 15
D. Benchmarks
In the field of code generation and software development,
there are notable differences and commonalities in the dataset
used for research on LLMs and LLM-based agents. These
datasets provide important benchmarks for evaluating model
performance, the HumanEval dataset, widely used for assess-
ing code generation models, is handcrafted by OpenAI and
contains 164 programming problems, each including a func-
tion signature, problem description, function body, and unit
tests. This dataset is primarily used to evaluate a model’s abil-
ity to generate correct code, particularly in tasks that involve
converting natural language descriptions into executable code.
Many studies have utilized HumanEval to test the performance
of code generation models [76]. The MBPP (Mostly Basic
Python Programming) dataset is another common benchmark,
comprising 427 Python programming problems that cover ba-
sic concepts and standard library functions, this dataset is used
to evaluate model performance across various programming
scenarios. In [82], researchers used the MBPP dataset to test
the performance of multi-agent systems in code generation
and optimization, improving the accuracy and robustness of
generated code through agent collaboration. The HumanEval-
ET and MBPP-ET datasets are extensions of the original
HumanEval and MBPP datasets, adding more test cases and
more complex problems for a comprehensive evaluation of
model performance [86]. The Spider and BIRD datasets focus
on converting natural language to SQL queries, evaluating
the model’s ability to handle complex query generation tasks.
In [69], researchers used these datasets to test the SQL-PaLM
framework, which evaluating the execution accuracy and exact
match rate for SQL generation tasks through few-shot prompt
and instruction fine-tuning. ToolBench and APIBench datasets
are used to evaluate a model’s capability in using tools and
APIs, ToolBench contains 16,464 real-world RESTful API
instructions, and APIBench normally tests a model’s gener-
alization ability to unseen API instructions [85]. The CAASD
(Capability Assessment of Automatic Software Development)
dataset is a newly developed benchmark comprising 72 soft-
ware development tasks from various domains, each with a
set of reference use cases to evaluate AI-assisted software
development systems [61].
There are some obvious commonalities in dataset selection
for LLMs and LLM-based agents, the HumanEval and MBPP
datasets are widely used to assess code generation capabili-
ties, covering a variety of programming tasks and languages.
Moreover, many studies have adopted multilingual and cross-
domain datasets, such as HumanEval-X and CodeSearchNet,
to evaluate model performance across different languages
and tasks. For the differences, LLM-based agents tend to
use multi-agent collaboration frameworks to handle complex
tasks, thus favoring benchmark datasets that emphasize multi-
turn interactions and iterative optimization, also focus on
tool usage and API integration capabilities, the framework
TOOLLLM used ToolBench and APIBench to assess its tool
usage capabilities, while Toolformer demonstrated its ability to
autonomously learn to use tools. These differences primarily
from the different approaches to task handling between LLMsand LLM-based agents, LLMs typically optimize a single
model’s performance by fine-tuning on relevant datasets.
E. Evaluation Metrics
Various evaluation metrics are used to assess the perfor-
mance of LLMs and LLM-based agents in code generation and
software development. These metrics measures the models’
performance in specific tasks and how they improve the
code generation and software development process. Table VI
includes the distribution of evaluation metrics cited in this
paper, encompassing both LLMs and LLM-based agents.
In research on LLMs and LLM-based agents, Pass@k is
a common evaluation metric used to measure the propor-
tion of generated code that passes all test cases within the
first k attempts, this metric is widely applied across various
datasets. In [86], Pass@k was used to evaluate the quality
of code generation in multi-turn interactions, showing that
the model’s Pass@k significantly improved by introducing
a planning phase. Besides Pass@k, BLEU score is another
common evaluation metric, mainly used to measure the syn-
tactic similarity and correctness between generated code and
reference code. In [73], BLEU score was used to evaluate
the quality of generated code. Complete Time and Success
Rate are other important evaluation metrics, particularly when
assessing the productivity impact of AI-assisted development
tools, these metrics are crucial as we expect LLMs to generate
accurate code while maintaining expected speed. Confidence
Calibration and Execution Rate are metrics used to evaluate
the confidence level and execution success rate of the model
when generating code. Researchers often use these metrics
to assess various LLMs’ performance in understanding user
intent and generating correct code with high precision.
Compared to the evaluation metrics for LLMs in software
development, LLM-based agents also use Pass@k but more
diverse to reflect their multi-agent collaboration characteristics.
Win Rate and Agreement Rate are important metrics for evalu-
ating the effectiveness of multi-agent collaboration. Addition-
ally, LLM-based agents often use metrics like Execution Ef-
fectiveness and Cost Efficiency to evaluate their performance
in real-world applications. For instance, in MetaGPT [79],
researchers evaluated not only the correctness of code gen-
eration but also analyzed the execution effectiveness, develop-
ment costs, and productivity. Results indicated that MetaGPT
significantly improved development efficiency and reduced
development costs while generating high-quality code. Overall
both are using traditional metrics such as Pass@k, Win Rate,
and task completion time to evaluate their code generation
capabilities, these metrics directly reflect the accuracy and
efficiency of the model in generating code. But LLM-based
agents normally requiring more comprehensive and diverse
metrics for evaluation to help assess the performance of
multiple agents and the whole development process, that’s why
we can see the human revision cost, qualitative feedback in
the evaluation metrics. Researchers consider user or developer
satisfaction metrics, as agent applications often involve ex-
tensive projects rather than isolated small-scale development,
these metrics focus on the correctness of code generation and
also resource utilization efficiency of the agent system.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 16
TABLE VI: E VALUATION METRICS IN CODE GENERATION AND SOFTWARE DEVELOPMENT
Reference Paper Benchmarks Evaluation Metrics Agent
[71] Leetcode problem Accuracy No
[72] HTTP server in JavaScript by
95 programmerTask Completion Time,
Task SuccessNo
[68] Spider, WikiTQ, GSM8k,
SV AMP, MBPP, MBPP, DS-1000Execution Accuracy,
Confidence Calibration
Execution RateNo
[45] Alpaca Data Win-Rate, Agreement
RateNo
[86] HumanEval/-X/-ET,
MBPP-sanitized/-ETPass@k, AvgPassRatio,
CodeBLEUNo
[73] HumanEval, CodeXGLUE Pass rate, Exact Match
BLEU ScoreNo
[69] Spider Accuracy,
Exact MatchNo
[74] HumanEval, MTPB Pass@k, Pass rate No
[30] HumanEval, MathQA-Python,
GSM8K-Python, CodeSearchNet,
CosQA, AdvTestPass@k, BLEU-4,
Exact Matcha
Edit Similarity,
Mean Reciprocal Rank (MRR)No
[70] HumanEval/-X Pass@k No
[67] HumanEval Pass@k, BLEU Score No
[75] HumanEval, MBPP-S, APPS Pass Rate, Token Edit Distance,
Exact Copy RateNo
[76] MBPP/-ET, HumanEval/-ET Pass@k Yes
[78] HumanEval Pass@1 Yes
[87] CAASD Pass Rate, Token Consumption Yes
[84] CCNet, SQuAD, Google-RE, T-REx,
ASDiv, SV AMP, MAWPS,
Web Questions, Natural Questions,
TriviaQA, MLQA, TEMPLAMAZero-shot performance,
Perplexity, Tool usage effectivenessYes
[82] MBPP/-ET, HumanEval/-ET Pass@1 Yes
[79] HumanEval, HumanEval,
SoftwareDevPass@k, Executability, Cost,
Code Statistics, Productivity,
Human Revision CostYes
[81] HumanEval, MBPP Pass@k, Practitioner-Based
AssessmentYes
[85] ToolBench, APIBench Pass Rate, Win Rate Yes
[80] No Specificed Pass Rate, Win Rate Yes
[77] MBPP/-ET, HumanEval/-ET Pass@1 Yes
[83] HumanEval, MBPP, EvalPlus Pass@1 Yes
[88] First-party data from Meta’s
code repositories and notebooksAcceptance Rate, P
ercentage of Code Typed,
Qualitative FeedbackYes
VI. A UTONOMOUS LEARNING AND DECISION MAKING
Autonomous Learning and Decision Making is a critical
and evolving field in modern software engineering, especially
under the influence of artificial intelligence and big data.
The core task of autonomous learning and decision making
is to achieve automated data analysis, model building, and
decision optimization through machine learning algorithms
and intelligent systems, thereby enhancing the autonomy and
intelligence of systems.
In this process, LLMs and LLM-based agents bring numer-
ous possibilities, following the development of NLP technol-
ogy, a lot of achievements have been made in the application
of LLMs in this field. These models can handle complex
language tasks and also demonstrate powerful reasoning and
decision-making abilities, the research on voting inference
using multiple LLMs calls has revealed new methods for op-timizing performance, with the frequently used method called
majority vote [89], this improves the accuracy of inference
systems and ensures the selection of the optimal possibility.
Additionally, the performance of LLMs in tasks such as
automated debugging and self-correction has enhanced the
system’s autonomous learning capabilities, achieving efficient
error identification and correction. At the same time, the
application of LLM-based agents in autonomous learning and
decision-making is also a novel but popular topic, these agents
can perform complex reasoning and decision-making tasks
with the help from the LLM, and also improve their adapt-
ability in dynamic environments through continuous learning
and optimization. In this context, we have collected nineteen
research papers on LLM-based agents in this field. This survey
will provides a general review of these studies, analyzing
the specific applications and technical implementations inJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 17
autonomous learning and decision making.
A. LLMs Tasks
The API call for the LLMs is one common applications,
often requiring continuous calls to enable the model to make
judgments and inferences, but does continuously increasing
the number of calls always improve performance? In [90],
researchers explored the impact of increasing LLM calls on the
performance of composite reasoning systems. Paper analyze
the voting inference design systems, the result showed that
there is a non-linear relationship between the number of LLM
calls and system performance; performance improves initially
with more calls but declines after reaching a certain thresh-
old. This research provides a theoretical basis for optimizing
LLM calls, helping to allocate resources reasonably in prac-
tical applications to achieve optimal performance. However,
the performance of V oting Inference Systems shows a non-
monotonic trend due to the diversity of query difficulties, and
the continuously increasing cost also needs to be considered.
Autonomous learning is also applied in bug fixing, where
researchers hope LLMs can continuously learn to fix bugs
and eventually identify human oversights or common errors.
In [91], the SELF-DEBUGGING method was proposed, en-
abling LLMs to debug code by analyzing execution results and
natural language explanations. This method significantly im-
proved the accuracy and sample efficiency of code generation
tasks especially for complex problems. Experimental results on
the Spider and TransCoder benchmarks showed that the SELF-
DEBUGGING method increase the model’s accuracy by 2-
12% which demonstrates the potential of LLMs in autonomous
learning to debug and correct any erros. Another similar
study introduced the AutoSD (Automated Scientific Debug-
ging) technique [92], which simulates the scientific debugging
process through LLMs, generating explainable patched code.
Researchers evaluated AutoSD’s capabilities from six aspects:
feasibility, debugger ablation, language model change, devel-
oper benefit, developer acceptance, and qualitative analysis.
Result have shown that AutoSD can generate effective patches
and also improve developers’ accuracy in evaluating patched
code by providing explanations, its explainability function
makes it easier for developers to understand and accept au-
tomatically generated patches. Although the above two stud-
ies primarily focus on automated debugging techniques, the
frameworks designed in these studies automatically determine
the optimal repair solution based on the debugging results
after collecting sufficient information, and provide specific
code implementations, which demonstrated the capability of
autonomous decision-making and learning.
Since the rise of LLMs applied to various fields, one
research direction has been the rational analysis of their
creativity and the exploration of their potential for continuous
learning, this creativity also highly determined by the decision
making capability of the models. [93] analyzed the outputs
of LLMs from the perspective of creativity theory, exploring
their ability to generate creative content, the study used metrics
such as value, novelty, and surprise, finding that current LLMs
have limitations in generating combinatorial, exploratory, andtransformative creativity. Although LLMs can generate high-
quality creative content, further research and improvement are
needed to achieve true creative breakthroughs. Additionally,
innovative responses generated by LLMs may come with the
possibility of hallucination, a long-standing issue for large
language models. Despite many techniques to mitigate its
downsides, it still cannot be entirely prevented. There are
many interesting experiments in decision making, such as
having LLMs act as judges to determine whether a person
has committed a crime [94]. A familiar attempt is to have
a primary LLM interact with other LLMs. [95] explored the
effectiveness of using LLMs as judges to evaluate other LLM-
driven chat assistants. The study validated the consistency of
LLM judgements with human preferences through the MT-
Bench and Chatbot Arena benchmarks, with results showing
that GPT-4’s judgments were highly consistent with human
judgments across various tasks. This research demonstrates the
potential of LLMs in simulating human evaluation, providing
new ideas for automated evaluation and optimization.
B. LLM-based Agents Tasks
Multi-agent collaboration and dialogue frameworks also
demonstrated strong capabilities in both decision making and
autonomous learning. [96] explores whether multi-agent dis-
cussions can enhance the reasoning abilities of LLMs. The
proposed CMD framework simulates human group discussion
processes, showing that multi-agent discussions can improve
performance in commonsense knowledge and mathematical
reasoning tasks without task-specific examples. Additionally,
the study found that multi-agent discussions also correct com-
mon errors in single agents, such as judgment errors and the
propagation of incorrect answers, thereby enhancing overall
reasoning accuracy. [97] researchers explored the potential
of multi-modal large language models (MLLMs) like GPT4-
Vision in enhancing agents’ autonomous decision-making pro-
cesses. The paper introduce the PCA-EV AL benchmark, and
evaluated multi-modal decision-making capabilities in areas
such as autonomous driving, home assistants, and gaming.
The results showed that GPT4-Vision exhibiting outstanding
performance across the dimensions of perception, cognition,
and action.
[98] proposes the Reflexion framework, a novel approach
that strengthens learning through language feedback rather
than traditional weight updates to avoid expensive re-train
costs. The framework uses self-reflection and language feed-
back to help language agents learn from mistakes, significantly
improving performance in decision-making, reasoning, and
programming tasks. The Reflexion’s first-pass success rate
on the HumanEval Python programming task increased from
80.1% to 91.0%, success rates in the ALFWorld decision-
making task improved by 22%, and performance in the
HotPotQA reasoning task increased by 14%. These results
indicate that the Reflexion framework demonstrate the state-
of-art performance in various tasks through self-reflection and
language feedback.
Another agent framework [35] introduces the ExpeL agent
framework which enhances decision-making capabilities byJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 18
autonomously collecting experiences and extracting knowl-
edge from a series of training tasks using natural language, this
experience collection process is similar to how humans gain in-
sights through practice and apply them in exams. By accessing
internal databases, ExpeL also reduces hallucinations, employ-
ing the RAG technique discussed in III. The ExpeL framework
doesn’t require parameter updates it enhances decision-making
capabilities by recalling past successes and failures which
fully leveraging the advantages of ReAct framework [36].
Experimental showed that ExpeL can continuous improvement
across tasks in multiple domains and exhibited cross-task
transfer learning capabilities. The combination of ExpeL and
Reflexion even further enhance the performance in iterative
task attempts, highlighting the importance of autonomous
learning and experiential accumulation in developing intelli-
gent agents. The ExpeL framework demonstrates its potential
as a state-of-the-art (SOTA) LLM-based agents in several
aspects, particularly in cross-task learning, self-improvement,
and memory mechanisms. By comparing ExpeL with existing
SOTA agents like Reflexion [98], ExpeL outperforms baseline
methods in various task environments. These studies collec-
tively indicate the importance of autonomous learning and im-
provement in LLM-based agents, agent systems continuously
optimize and improve decision-making processes through self-
feedback, self-reflection, and experiential accumulation which
shows higher autonomy and flexibility in handling dynamic
and complex tasks compared to traditional LLMs. Unlike
traditional LLMs, which mainly rely on pre-training data and
parameter updates, LLM-based agents adapt and improve their
performance in real-time through continuous self-learning and
feedback mechanisms, thus demonstrating outstanding perfor-
mance in various tasks.
[99] proposes the AGENTVERSE multi-agent framework,
designed to improve task completion efficiency and effective-
ness through collaboration. The framework draws on human
group dynamics by designing a collaborative system of expert
agents that exhibit outstanding performance in tasks such as
text understanding, reasoning, coding, and tool usage. Experi-
ments showed that the AGENTVERSE framework performed
well not only in independent task completion but also sig-
nificantly improved performance through group collaboration
especially in coding tasks where the framework use GPT-4
to be the brain of the agent groups. The framework also ob-
served emergent behaviors in agents during collaboration, such
as voluntary actions, conformity, and destructive behaviors,
providing valuable insights for understanding and optimizing
multi-agent systems.
Another multi-agents study [100] Introducing the CAMEL
framework, this is a well known agent framework, which
explores building scalable techniques to facilitate autonomous
collaborative agent frameworks. The study proposes a role-
playing collaborative agent framework that guides dialogue
agents to complete tasks through embedded prompts while
maintaining alignment with human intentions. The CAMEL
framework generates dialogue data to study behaviors and
capabilities within the agent society, the study further en-
hanced agent performance by fine-tuning the LLaMA-7B
model, validating the effectiveness of generated datasets inenhancing LLM capabilities. [101] investigates the compre-
hensive comparison of LLM-augmented autonomous agents
and proposes a new multi-agent coordination strategy for
solving complex tasks through efficient communication and
coordination called BOLAA. The experiment showed that the
BOLAA outperforms other agent architectures in the WebShop
environment especially in high-performance LLMs The above
three studies focus on achieving a multi-agent collaboration
architecture by increasing the number of agents. This trend
indicates that more frameworks are beginning to explore the
potential of multi-agent systems. [44] explores methods to
enhance LLMs performance by increasing the number of
agents. Using sampling and voting methods, the study showed
that as the number of agents increased, LLM performance
in arithmetic reasoning, general reasoning, and code gener-
ation tasks improved significantly. This method proves the
effectiveness of multi-agent collaboration in enhancing model
performance. These studies collectively indicate the impor-
tance of multi-agent collaboration and dialogue frameworks
in autonomous learning and decision-making tasks. Compared
to traditional LLMs, these multi-agent frameworks enhance
reasoning accuracy under zero-shot learning and demonstrate
higher autonomy and flexibility which reduce the burden on
developers.
LLM-based agents not only perform complex data analysis
tasks but also demonstrate potential in simulating and under-
standing human trust behaviors. [102] introduces a framework
named SELF, designed to achieve self-evolution of LLMs
through language feedback, use RLHF to train agent behavior
to meet the human alignment. The framework enhances model
capabilities through iterative processes of self-feedback and
self-improvement without human intervention. In experiments,
the test accuracy on GSM8K and SV AMP datasets increasing
by 6.82% and 4.9%, respectively and the overall task win rates
on the Vicuna test set and Evol-Instruct test set also increased
by 10% and 6.9%. Another similar study exploring the po-
tential of LLM-based agents to simulate the human trust be-
haviors. [103] also examines whether LLM-based agents can
simulate human trust behaviors. The study aims to determine if
LLM-based agents exhibit trust behaviors similar to humans
and explore whether these behaviors can align with human
trust. Through a series of trust game variants such as initial
fund allocation and return trust games, the research analyzes
LLM-based agents’ trust decisions and behaviors in different
contexts. Results show that particularly for GPT-4, LLM-based
agents exhibit trust behaviors consistent with human expecta-
tions in these trust games, validating the potential of LLM-
based agents in simulating human trust behaviors. The efficient
and accurate handling of diverse datasets highlights the broad
application prospects in fields such as software engineering.
In terms of simulating trust behaviors, LLM-based agents
demonstrate human-like behavior patterns through complex
trust decisions and behavior analysis providing an important
theoretical foundation for future human-machine collaboration
and human behavior simulation.
Integrating LLMs into agents allows for more complex
task processing. [104] proposes a lightweight user-friendly
library named AgentLite whic designed to simplify the de-JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 19
velopment, prototyping and evaluation of task-oriented LLM-
based agents systems. The main goal of the study is to
enhance the capabilities and flexibility of LLM-based agents in
various applications by introducing a flexible framework. This
framework enhances task-solving capabilities through task de-
composition and multi-agent coordination, using a hierarchical
multi-agent coordination approach where a managing agent
supervises the task execution of each agent. [105] introduces
a framework, GPTSwarm, that represents LLM-based agents
as computational graphs to unify existing prompt engineering
techniques and introduces methods for optimizing these graphs
to enhance agent performance. The study verifies the effective-
ness of the framework through various benchmarks such as
MMLU, Mini Crosswords, and HumanEval. The framework
demonstrated significant performance improvements on the
GAIA benchmark with an improvement margin of up to
90.2% compared to the best existing methods. Additionally,
agents have shown strong capabilities in autonomous learning
and decision-making in software engineering and security,
which will be introduced in the subsequent Software Security
section [106] [107] [108] [109].
C. Analysis
Overall, LLMs and LLM-based agents exhibit strong capa-
bility on the autonomous learning and decision making but
slightly different view. These differences are reflected in the
focus of task execution and also in autonomy, interactivity,
learning and adaptation mechanisms, and the integration with
other systems and modalities. From the perspective of task
execution focus, LLMs primarily concentrate on enhancing
specific functions in software engineering, such as debugging,
problem-solving and automated reasoning. The tasks they
perform are usually static and well-defined, such as automatic
debugging, enhancing debugging capabilities to autonomously
identify and correct errors, evaluating creativity and judging
responses from other chatbots. In contrast, LLM-based agents
not only focus on specific tasks but also manage multi-
ple tasks simultaneously, often involving dynamic decision-
making and interaction with other agents or systems. Examples
of these agents’ tasks include enhancing reasoning through
multi-agent discussions, continuous learning from experiences,
requiring real-time dynamic decision-making, and also LLM-
based agents can get in touch with the multimodal task in the
visual environment.
We can conclude that, the application of LLM-based agents
in the topic of autonomous learning and decision-making
primarily involves exploring their performance in specific tasks
through various framework designs. These studies evaluate the
agents’ autonomy and decision-making capabilities to deter-
mine whether they align with human behavior and decision-
making processes. If we dive into the specific task deigns,
in terms of autonomy and interactivity LLMs are usually
designed to perform highly specific tasks without needing
to adapt to external input or environmental changes, they
mainly operate as single models focusing on processing and
responding within predefined boundaries, this also applied
to all LLM applications. On the other hand, LLM-basedagents exhibit higher autonomy which are typically designed
to interact with or adapt to the environment in real-time, they
are often part of multi-agent systems where collaboration and
communication are key components, for example use extra
model or tools to further help with the planning phases. In
terms of integration with other systems and modalities, LLMs
typically operate in text input-output scenarios and even in
multi-modal settings, their role is usually limited to processing
and generating text-based content. Also, LLM-based agents are
more likely to integrate with other systems and modalities such
as visual input or real-world perception data, enabling them
to perform more complex and context-based decision-making
tasks.
FIG. 6: E XPEL [35] F RAMEWORK WITH REFLEXION [98] INEXPE-
RIENCE GATHERING
Regarding learning and adaptation mechanisms, LLMs’
adaptation and learning are usually confined to the model’s
training data and parameter range, although they can adapt
through new data updates, they lack the ability to continuously
learn from real-time feedback, they are more focused on using
existing knowledge to solve problems and generate responses.
In contrast, LLM-based agents are often equipped with ex-
periential learning and real-time feedback adaptation mech-
anisms, allowing them to optimize strategies and responses
based on continuous interactions. One good example of LLm-
based agents framework is Expel [35], which utilize the
previous researches ReAct [36] and Reflexion [98] as shown
in Figure. 6. This framework utilizes a memory pool and
insights pool to enable the LLM to learn from past knowledge,
thereby aiding subsequent decision-making. This autonomous
decision-making capability is something that traditional LLM
frameworks cannot achieve.
D. Benchmarks
In the field of autonomous learning and decision-making,
the benchmark datasets used by LLMs and LLM-based agents
are quite similarly in the task handling and application require-
ments. We can gain a deeper understanding of the strengths
and weaknesses of both approaches in different tasks and their
application contexts. The specific dataset references, please see
Table VII.
In the research on LLMs, the main datasets include De-
fects4J, MMLU, TransCoder, and MBPP. These datasets are
primarily used to evaluate model performance in specific
domains and tasks. Defects4J is a widely used in the soft-
ware engineering, this software defect dataset containing 525JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 20
real defects from 17 Java projects. It’s designed to test the
effectiveness of automated program repair and defect detection
tools by providing a standardized benchmark that allows
researchers to compare the performance of different methods.
MMLU (Massive Multitask Language Understanding) is a
large-scale benchmark dataset covering 57 subjects, testing
models on a broad spectrum of knowledge and reasoning
abilities in multitask language understanding. It includes ques-
tions ranging from elementary education to professional level
such as College Mathematics, Business Ethics, and College
Chemistry, challenging the models’ diverse knowledge base
and reasoning capabilities. The TransCoder dataset focuses
on code translation across programming languages which
evaluate the model’s ability to automatically translate code
from one programming language to another. This is crucial for
multilingual software development and maintenance, as it can
greatly enhance development efficiency. MBPP (Mostly Basic
Python Programming) has been introduced in previous section,
it’s a dataset containing 427 Python programming problems,
covering basic concepts and standard library functions, it’s
widely used to test the model’s performance in different pro-
gramming scenarios, evaluating its ability to generate correct
and efficient code.
In contrast, LLM-based agents use datasets that empha-
size multitasking and decision-making capabilities in complex
scenarios. The main datasets include HotpotQA, ALFWorld,
FEVER, WebShop, and MGSM. HotpotQA is a multi-hop
question-answering dataset that requires models to reference
content from multiple documents when answering questions,
evaluating their information synthesis and reasoning abilities,
this dataset challenges the model’s performance in complex
reasoning tasks. ALFWorld is a text-based environment simu-
lation dataset requiring multi-step decision-making where the
model completes tasks in a virtual home environment. The
dataset combines natural language processing and decision-
making, evaluating the model’s performance in dynamic and
interactive tasks. The FEVER (Fact Extraction and VERifica-
tion) dataset is used for fact verification tasks, where the model
needs to verify the truthfulness of given statements and provide
evidence, it assesses the model’s capabilities in information
retrieval and logical reasoning. WebShop is an online shopping
environment simulation dataset containing 1.18 million real-
world products and human instructions, it used to test the
model’s performance in complex decision-making tasks such
as completing shopping tasks and attribute matching. MGSM
(Multimodal Generalized Sequence Modeling) is a multimodal
dataset containing tasks related to dialogue, creative writing,
mathematical reasoning, and logical reasoning, evaluating the
model’s comprehensive abilities in multimodal tasks.
Comparatively, LLM datasets typically focus on single,
static tasks such as code generation, mathematical reason-
ing and creative writing, which suitable for models work-
ing within predefined task scopes. Datasets like Defects4J,
MMLU, and MBPP help evaluate model capabilities in spe-
cific domains. LLM-based agents are more suited for com-
plex, multitasking, and dynamic environments where datasets
require models to handle multimodal inputs and real-time
decision-making, it can showcase their advantages in handlingcomplex interactions and multitasking scenarios. Datasets
like HotpotQA, ALFWorld, FEVER, and WebShop challenge
the models’ performance in information synthesis, dynamic
decision-making/interaction and multimodal tasks. This dif-
ference arises from the distinct design goals of the two:
LLMs aim to optimize performance on single tasks, while
LLM-based agents are designed to handle complex or multi-
modal task, this require higher autonomy and adaptability.
It’s also reflects modern applications’ demand for highly
interactive, adaptive, and multifunctional AI systems, driving
the development from single LLM models to multi-agent
systems. Through these analyses, we can identify the different
application of LLMs and LLM-based agents in autonomous
learning and decision-making, it’s important to choose the
appropriate framework to meet different task requirements in
the real world applications.
E. Evaluation Metrics
various evaluation metrics are used in the research on
LLMs and LLM-based agents, these metrics used to evaluate
the models’ performance in specific tasks and analyze their
application effectiveness in this domain. Below, we discuss
several representative studies analyzing the evaluation metrics
they employed and exploring the differences between LLMs
and LLM-based agents in this field.
In research on LLMs, evaluation metrics primarily focus on
model accuracy and task completion. In [90], researchers used
the accuracy of a voting inference system which measured by
the expected 0/1 loss (the proportion of correct responses) to
assess model performance. This metric evaluates the accuracy
of models through multiple calls, reflecting the ability of
LLMs to improve result accuracy via iterative reasoning.
Common evaluation metrics in the literature include accuracy
and sample efficiency, accuracy refers to the proportion of
correct predictions made by the model, while sample efficiency
measures the number of samples required to achieve a certain
accuracy level. These metrics assess both the predictive and
decision making ability of the model and its data utilization
efficiency during training. In [92], evaluation metrics include
possible patches, correct patches, precision, and developer
accuracy. Possible patches refer to patches that pass all
tests, while correct patches are semantically equivalent to
the original developer patches. Precision measures the pro-
portion of correct patches among the possible patches, and
developer accuracy assesses the correctness of patches with
and without explanations through human evaluation. These
metrics emphasize the model’s explanatory capability and
practical effectiveness in automated code repair, increasing
reliance on human evaluation. To assess model creativity,
value, novelty and surprise are used as creativity dimensions.
Quality, social acceptability, and similarity of generated works,
as well as the ability to generate creative product, are also
included in the evaluation. [110] used the success rate in the
Game of 24 and the coherence of generated paragraphs in
creative writing as evaluation metrics. These metrics assess the
model’s performance in problem-solving and text generation,
showcasing LLMs’ potential in solving complex problems andJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 21
generating coherent text. In [95], consistency and success rate
were used as evaluation metrics, the consistency calculates
the probability of agreement between two judges on randomly
selected questions which measures the alignment of LLM
judges with human preferences. Success rate is used for
specific tasks (such as the Game of 24) to measure the correct
response rate.
In contrast, LLM-based agents use more diverse evaluation
metrics to reflect their multi-agent collaboration character-
istics. In [97], evaluation metrics include Perception Score
(P-Score), Cognition Score (C-Score), and Action Score (A-
Score). These metrics comprehensively assess the model’s per-
ception, cognition and action capabilities, demonstrating the
comprehensive performance of LLM-based agents in handling
multimodal tasks. In multimodal applications, success rate
(SR) is often used as a primary metric, evaluated through tasks
such as HotpotQA and FEVER to assess precise matching
success. These metrics focus on task completion success and
accuracy, showcasing the practical execution capabilities of
LLM-based agents in different task environments. In [111],
evaluation metrics include practitioner feedback, efficiency,
and accuracy. Practitioner feedback uses the Likert scale
to collect satisfaction and performance feedback, the Likert
scale is a commonly used psychometric tool designed to
measure an individual’s attitude or opinion toward a particular
statement. The scale typically consists of the following five
options: Strongly Disagree, Disagree, Neutral, Agree, Strongly
Agree. While efficiency and accuracy are measured through
the effectiveness of model-executed qualitative data analysis
validated by practitioners. These metrics assess the agents’
performance in qualitative data analysis, demonstrating their
utility and accuracy in practical applications.
By comparing these metrics, we find that LLMs using
traditional metrics such as accuracy and sample efficiency
to assess their capabilities. In contrast, LLM-based agents
handle more complex algorithm through multi-agents, which
requires more comprehensive and diverse metrics to evaluate
their performance from multiple directions. LLM-based agents
in multimodal tasks and self-evolution tasks emphasize the
integrated performance of perception, cognition, and action
capabilities. This difference reflects LLMs’ strengths in single-
task optimization and LLM-based agents’ potential in col-
laborative handling of complex tasks with higher capability
of autonomous learning. Additionally, practical application
evaluation metrics for LLM-based agents, such as practitioner
feedback, efficiency, and accuracy, demonstrate their utility
and user satisfaction in real-world scenarios. This evaluation
approach assesses task completion but also consider a compre-
hensive evaluation of user experience, which can also evaluate
the human alignment of their decision making capabilities.
VII. S OFTWARE DESIGN AND EVALUATION
The application of LLMs to software design and evaluation
has very similar overlaps with previous topics, software design
is an early phase of software development, and the quality of
the design directly impacts the quality of furture development.
Modern software engineering methodologies emphasize theintegration of design and development to ensure that decisions
made during the design phase seamlessly translate into high-
quality code. Consequently, the research on software design
often explores aspects related to code generation and devel-
opment by utilizing LLMs for software development with a
certain framework and special architecture design. Software
design frameworks often involve multiple stages of continuous
refinement to achieve optimal results, which can be considered
part of LLM applications in software development [83].
Similarly, [85] and [84] highlight the frequent use of tools
or API interfaces when using LLMs to assist in development
and design, demonstrating an overlap with the topic of code
generation and software development.
LLMs in software design and evaluation also intersect
extensively with autonomous learning and decision making,
these two topics are interrelated fields. Software design needs
to consider system adaptability and learning capabilities to
handle dynamic environments, therefore design evaluations
involving autonomous learning and decision making naturally
become a focal point of intersection for these two topics.
Many LLM techniques and methods find similar applications
in both fields, for example LLMs based on reinforcement
learning can be used for automated design decisions and
evaluations, as well as for self-learning and optimization.
Common applications of LLMs in software engineering in-
volve fine-tuning models with prompt engineering techniques
to continuously enhance performance particularly in soft-
ware design and evaluation, more sample learning is often
required to ensure that the model outputs align with user
expectations [93] [102] [44] [111] [105] [96]. Additionally,
requirement elicitation and specification in requirement engi-
neering can also be considered part of software design and
evaluation [51] [112]. This section reviews the main research
achievements of LLMs in software design and evaluation
in recent years, discussing their application scenarios and
practical effects.
A. LLMs Tasks
In recent years, there has been extensive research on the use
of LLMs in tasks such as automation, optimization, and code
understanding. ChatGPT has been widely utilized for various
software engineering tasks and demonstrated excellent perfor-
mance in tasks like log summarization, pronoun resolution,
and code summarization, achieving a 100% success rate in
both log summarization and pronoun resolution tasks [113].
However, its performance on tasks such as code review
and vulnerability detection is relatively poor, which shows
that it needs further improvement for more complex tasks.
Another framework EvaluLLM addresses the limitations of
traditional reference-based evaluation metrics (such as BLEU
and ROUGE) by using LLMs to assess the quality of natural
language generation (NLG) outputs [114]. The EvaluLLM
introduces a new evaluation method that compares genera-
tive outputs in pairs and uses win rate metrics to measure
model performance, this approach can simplifies the evaluation
process also ensures consistency with human assessments,
showcasing the broad application prospects of LLMs in gener-JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 22
TABLE VII: E VALUATION METRICS IN AUTONOMOUS LEARNING AND DECISION MAKING
Reference Paper Benchmarks Evaluation Metrics Agent
[90] MMLU Accuracy No
[91] Spider, TransCoder, MBPP Accuracy, Sample Efficiency No
[92] Defects4J v1.2, Defects4J v2.0,
Almost-Right HumanEvalPlausible Patches,
Correct Patches,
Precision, AccuracyNo
[93] No Specific Quality, Acceptance Rate No
[110] Game of 24, Creative Writing,
5x5 CrosswordsSuccess rate, Coherency No
[95] MT-Bench, Chatbot Arena Agreement Rate, Success Rate
Human JudgementNo
[96] ECQA, GSM8K, FOLIO-wiki Accuracy Yes
[97] PCA-EV AL Accuracy, P/C/A-Score Yes
[35] HotpotQA, ALFWorld, WebShop, FEVER Success Rate Yes
[106] Not specified Success Rate, Autonomy Leve Yes
[44] GSM8K, MATH, MMLU, Chess, HumanEval Accuracy Yes
[107] MITRE ATTCK framework Ability Identify Vulnerabilities Yes
[102] GSM8K, SV AMP, Vicuna testset,
Evol-Instruct testsetAccuracy, Feedback Accuracy Yes
[98] HotPotQA, ALFWorld, HumanEval,MBPP,
LeetcodeHardGymPass@1, Success Rate Yes
[111] Github Developer Discussions,BBC News,
Social MediaConversations,
In-depth InterviewsPractitioner Feedback,
Efficiency and AccuracyYes
[100] AI Society, Code, Math,Science,
MisalignmentHuman Evaluation,
GPT-4 EvaluationYes
[99] FED, Commongen Challenge,
MGSM, Logic Grid Puzzles,
HumanEvalPass@1, Task completion rate Yes
[36] HotpotQA, FEVER, ALFWorld, WebShop Exact Match, Accuracy,
Success rate, Average ScoreYes
[103] Trust Game, Dictator Game,
MAP Trust Game,
Risky Dictator Game,
Lottery Game, Repeated Trust GameValid Response Rate,
AlignmentYes
[104] HotPotQA, WebShop F1-Score, Average Reward Yes
[108] 263 real smart contract vulnerabilities F1 Score, Accuracy
Precision, Recall
Consistency Rate.Yes
[109] 15 real-world one-day vulnerabilities
from CVE databaseSuccess Rate, Cost Yes
[101] WebShop, HotPotQA with Wikipedia AP Reward Score, Recall Yes
[105] MMLU, Mini Crosswords, HumanEval,
GAIAAccuracy, Pass@1 Yes
ative tasks. Similarly, in the LLMs evaluation domain, LLM-
based NLG Evaluation provides a review and classification of
current LLMs used for NLG evaluation, the paper summarizes
four main evaluation methods: LLM-derived metrics, prompt-
based LLMs, fine-tuned LLMs, and human-LLM collaborative
evaluations [115]. These methods demonstrate the potential of
LLMs in evaluating generative outputs which also mention
challenges such as the need for improved evaluation metrics
and further exploration of human-LLM collaboration.
There are also many novel application design with the
LLMs which applied in the engineering design, one study ex-
plores strategies for software/hardware co-design to optimize
LLMs and applies these strategies to design verification [116].
Through quantization, pruning, and operation-level optimiza-
tion, this research demonstrates applications in high-level
synthesis (HLS) design functionality verification, GPT-4 wasused to generate high-level synthesis (HLS) designs containing
predefined errors to create a dataset called Chrysalis, this
dataset provides a valuable resource for evaluating and opti-
mizing LLM-based HLS debugging assistants. The optimized
LLM significantly improves inference performance, providing
new possibilities for error detection and correction in the elec-
tronic design automation (EDA) field. In [117], the researchers
introduces RaWi, a data-driven GUI prototyping approach. The
framework allows users to retrieve GUIs from this repository,
edit them, and create new high-fidelity prototypes quickly. The
experiment conducted by comparing RaWi with a traditional
GUI prototyping tool (Mockplus) to measure how quickly
and effectively users can create prototypes. The result demon-
strated that RaWi outperformed on multiple benchmarks, with
40% improvement on precision@k metric. This study proves
the possibility of LLMs to improve the efficiency duringJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 23
prototyping phase of software design, which allows designers
to quickly iterate on GUI designs, facilitating early detection of
design flaws. With the new possibility brought by the LLMs,
there has been much discussion in the education field, with
researchers exploring the implications of the prevalence of
large language models for education [118]. Study indicates
that ChatGPT shows significant potential but some limitations
in answering questions from software testing courses [119].
ChatGPT was able to answer about 77.5% of the questions
and provided correct or partially correct answers 55.6% of
the time. However, the correctness of its explanations was
only 53.0%, indicating the need for further improvement in
educational applications.
B. LLM-based Agents Tasks
The application of LLM-based agents in software design
and evaluation enhance the development efficiency and code
quality, as well as showcase the broad applicability and
immense potential of LLM-based agents in practical software
engineering tasks. [120] explores the current capabilities,
challenges, and opportunities of autonomous agents in soft-
ware engineering. Study evaluate Auto-GPT’s performance
across different stages of the software development lifecycle
(SDLC), including software design, testing, and integration
with GitHub, the paper finds that detailed contextual prompts
significantly enhance agent performance in complex software
engineering tasks which mentions the importance of context-
rich prompts in reducing errors and improving efficiency,
underscoring the potential of LLM-based agents to automate
and optimize various SDLC tasks, thereby enhancing devel-
opment efficiency. This paper also evaluate the limitation
of the Auto-GPT, includes task or goal skipping, generating
unnecessary code or files (hallucinations), repetitive or looping
responses, lack of task completion verification mechanisms.
These limitations can lead to incomplete workflows, inaccurate
outputs, and unstable performance in practical applications.
[121] introduces ChatDev, the first virtual chat-driven
software development company, a concept of using LLMs not
just for specific tasks but as central coordinators in a chat-
based, multi-agent framework. this approach allows for more
structured, efficient, and collaborative software development
processes, exploring how chat-driven multi-agent systems can
achieve efficient software design and evaluation, reduce code
vulnerabilities, and enhance development efficiency and qual-
ity. Experiments show that ChatDev can design and generate
software in an average of 409.84 seconds at a cost of only
$0.2967 while significantly reducing code vulnerabilities. This
indicates that chat-based multi-agent frameworks capable to
improve software development efficiency and quality. Another
similar collaboration framework introduced by Microsoft re-
search team, [122] demonstrates the effectiveness of using
LLMs, particularly ChatGPT as agent’s controllers to manage
and execute various AI tasks. The HuggingGPT system that
uses ChatGPT to orchestrate the execution of tasks by various
AI models available in Hugging Face, the purpose is to test
how effectively the system can handle complex AI tasks,
including language, vision, and speech tasks, by executingappropriate models based on user requests. The innovation
lies in using LLMs not just as tools for direct task execution
but as central orchestrators that leverage existing AI models
to fulfill complex tasks, This approach expands the practical
applicability of LLMs beyond typical language tasks. [123]
proposes the LLMARENA benchmark framework to evaluate
LLMs’ capabilities in dynamic multi-agent environments, the
idea is similar to the ChatDev but innovates by shifting the
focus from single-agent static tasks to dynamic and interac-
tive multi-agent environments, providing a more realistic and
challenging setting to assess the practical utility of LLMs, this
approach mirrors real-world conditions where multiple agents
(either AI or human) interact and collaborate. Experiments
show that this framework can test LLMs’ spatial designing,
strategic planning, and teamwork abilities in gaming environ-
ments, offering new possibilities and tools for designing and
evaluating LLMs in multi-agent systems.
[124] introduces the ”Flows” conceptual framework for
structuring interactions between AI models and humans to
improve reasoning and collaboration capabilities. The study
present the idea of conceptualizing processes as indepen-
dent, goal-driven entities that interact through standardized
message-based interfaces, enabling a modular and extensible
design. This approach is inherently concurrency-friendly and
supports the development of complex nested AI interactions
without having to manage complex dependencies. Experiments
in competitive coding tasks show that the ”Flows” frame-
work increases the AI model’s problem-solving rate by 21
percentage points and the human-AI collaboration rate by 54
percentage points. This demonstrates how modular design can
enhance AI and human collaboration, thereby improving the
software design and evaluation process.
[125] presents a new taxonomy to structurally understand
and analyze LLM-integrated applications, providing new the-
ories and methods for software design and evaluation. This
taxonomy helps in understanding the integration of LLM com-
ponents in software systems, laying a theoretical foundation
for developing more effective and efficient LLM-integrated
applications. Similarly, [126] explores the application of LLM-
based agents in software maintenance tasks, improving code
quality and reliability through a collaborative framework.
This study should origin be categorized under the software
maintenance domain but exhibit the iterative manner of the
design structure. The framework utilize the task decomposition
and multi-agent strategies to tackle complex engineering tasks
that traditional one-shot methods cannot handle effectively,
multiple agents can learn from each other, leading to improved
software maintenance outcomes. Experiments show that multi-
agent systems outperform single-agent systems in complex
debugging tasks, indicating that this new framework can be
applied in software design to provide safer architectures.
C. Analysis
Overall, LLM applications in software design and evaluation
typically focus on the automation of specific tasks, such
as code generation and log summarization, with a tendency
towards evaluation the capability rather than implementationJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 24
during the design phases. The process of software design is
largely intertwined with software development and require-
ments engineering. As previously mentioned, the use of LLMs
to assist in software development often includes aspects of
the software design process, particularly in generating related
design documentation. Therefore, there is relatively limited
research focused on using LLMs for higher-level software
design tasks.
LLM-based agents expand the capabilities of LLMs by han-
dling more complex workflows through intelligent decision-
making and task execution, these agents can collaborate,
dynamically adjust tasks and gather and utilize external infor-
mation. In software design and evaluation, a single model often
cannot comprehensively consider both design and evaluation
aspects, which is why more software developers are reluctant
to entrust high-level tasks to AI. LLM-based agents, through
collaborative work and more refined role division, can effi-
ciently complete design tasks and adapt to various application
scenarios. However, the application of LLM-based agents
in software design is commonly included in the software
development, like previously discussed, the self-reflection and
reasoning before action occurs during the software design
phases. The Chatdev [121] framework uses role distribution
to create a separate software design phase which significantly
increases the flexibility and accuracy in the later development
phases. In terms of efficiency and cost, LLMs are still slightly
superior to LLM-based agents in text generation and vulner-
ability detection. However, handling tasks similar to software
maintenance and root cause analysis requires more complex
architectures, such as multi-turn dialogues, knowledge graphs,
and RAG techniques, which can further benefit the design and
evaluation phases.
D. Benchmarks
The benchmarks include public datasets and datasets self-
crafted by the authors themselves, and the application scenar-
ios are also quite differently as shown in the Table VIII. Big-
CloneBench is a benchmark dataset for code clone detection,
containing a large number of Java function pairs. These pairs
are classified as clones and non-clones, used for training and
evaluating clone detection models, with the main evaluation
metric being the correct identification rate. The Chrysalis
dataset created by [116], it contains over 1000 function-
level designs from 11 open-source synthesizable HLS datasets,
primarily used to evaluate the effectiveness of LLM debugging
tools in detecting and correcting injected errors in HLS de-
signs, with the main evaluation metric being the effectiveness
of error detection and correction. The CodexGLUE dataset is a
comprehensive benchmark dataset covering various code gen-
eration and understanding tasks such as code completion, code
repair, and code translation, used to evaluate the performance
of code generation models in practical programming tasks. In
addition to these public datasets, some artificially simulated
datasets are used, such as a simulated job fair environment
dataset. This dataset simulates a virtual job fair environment
containing multiple task scenarios such as interviews, recruit-
ment, and team project coordination. The dataset used toevaluate the coordination capabilities of generative agents in
complex social tasks, with the main evaluation metrics being
task coordination success rate and role matching accuracy.
Comparatively, LLMs research tends to use specific and
publicly available datasets, such as BigCloneBench. These
datasets provide standardized evaluation benchmarks, aiding
in the reproducibility and comparability of results. Researches
on LLM-based agents tends to use customized experimental
settings or unspecified datasets, such as requirement documen-
tations, without specifying particular datasets but emphasizing
that the experiments involve 70 user requirements. This choice
is usually because the research needs to evaluate the perfor-
mance from multiple angles, and it is difficult to perfectly
adapt to the vertical application scenarios if some general
datasets are used. Both LLM and LLM-based agents use a
variety of datasets to evaluate the performance of the model,
these datasets cover tasks ranging from code generation, code
understanding, to natural language generation and task man-
agement, due to the topic of software design and evaluation
is relatively inter-related with others. However, because the
LLM-based agents can be expanded to application scenarios
such as videos and pictures, the agents like Auto-GPT and
HuggingGPT also use multimodal datasets. These datasets not
only contain code and text, but also involve multiple data types
such as images and speech. Moreover, compared with a single
LLM framework, LLM-based agents need to evaluate more
areas, so benchmarks also need to be considered separately.
For example, LLMARENA is specially designed to test the
performance of LLM in dynamic, multi-agent environments,
covering complex tasks such as spatial reasoning, strategic
planning, and risk assessment.
E. Evaluation Metrics
In Software Design and Evaluation, various studies have
employed different evaluation metrics to measure the perfor-
mance of LLMs and LLM-based agents across a range of
tasks. Both LLM and LLM-based agent research use more
than one metrics to comprehensively assess model perfor-
mance, LLMs research tends to focus on traditional metrics
such as accuracy, win rate, and consistency, while LLM-
based agent research still consider those fundamental metrics
but further introduces complex evaluation methods, such as
task coordination success rate and role matching accuracy.
However, it cannot be definitively stated that future LLM-
based agent research will always use more flexible evaluation
metrics considering multiple dimensions, but more dependent
on the specific task and dataset being used. The reason for this
phenomenon, as observed in this survey, is primarily that tasks
in LLMs research are relatively single-tasked, mainly focusing
on static tasks such as log summarization with traditional eval-
uation methods. On the other hand, LLM-based agent research
involves more general multi-agent tasks, and its evaluation
methods emphasize interactivity and dynamics. LLM-based
agent research focuses more on the model’s collaboration
and decision-making capabilities by using multi-dimensional
evaluation metrics to comprehensively assess their potential
in practical applications consider not only the accuracy. ThisJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 25
TABLE VIII: E VALUATION METRICS IN SOFTWARE DESIGN AND EVALUATION
Reference Paper Benchmarks Evaluation Metrics Agent
[113]BigCloneBench,
Python functions,
Java methods,
Random logs,
Bug reports,
Requirement specificationsAccuracy No
[114] Not Specified Win rate, Agreement score No
[115] Not SpecifiedEmbedding-based metrics,
probability-based metrics,
Comparison, RankingNo
[116] Chrysalis Effectiveness No
[127]CommonsenseQA,
StrategyQA, GSM8KAccuracy,
Token, Time costsNo
[119]31 Questions from
software testing textbook.Correctness, Effectiveness No
[128]Medical transcripts,
Amazon Product
DescriptionsCoverage,
False Failure Rate
Alignment.No
[117] RicoPrecision@k,
NDCG@k,
Mean Reciprocal Rank,
Average Precision, HITS@kNo
[120] Not SpecifiedAccuracy, Success rate,
Consistency, EffectivenessYes
[122]Hugging Face’s
Model Repository.Accuracy,
Precision,
Recall,
F1-Score,
Edit Distance,
GPT-4 Score,
Passing Rate,
Rationality,
Success Rate.Yes
[124] Codeforces, LeetCode Pass@1 Yes
[121] 70 User Requirements.Number of files generated,
Time taken, CostYes
[121] CodeforcesComprehensiveness,
Robustness, Conciseness,
Mutual exclusiveness,
Explanatory power,
Extensibility.Yes
[125] Sample Applications. BERTScore, BLEU Yes
[126] CodexGLUEBLEU, METEOR,
ROUGE-L, BERTScoreYes
[129] Production IncidentsSuccess rate,
Accuracy, Alignment,
AppropriatenessYes
[130]Simulated Job Fair
EnvironmentCompletion time,
Task Progress,
Understanding LevelYes
explains why, despite the similarity in evaluation metrics
such as accuracy and completion time, LLM-based agents
use flexible evaluation metrics, including metrics like mutual
exclusiveness and appropriateness.
VIII. S OFTWARE TESTGENERATION
In software development, a crucial component is software
testing, which need to continuously been conducted from
the initial system development to the final deployment. In
industry, agile development is commonly used which test
system continuously at every stage to ensure the robustness
of the entire system, whenever new code is committed to
the GitHub, tests are conducted to ensure the usability of
the updated version. A common approach is to use Jenkins4
to achieve continuous integration and continuous deployment.
Jenkins automatically hooks into the developer’s action of
pushing code to GitHub and runs a test suite against the new
version. Although the entire process leans towards automated
4https://www.jenkins.io/development, creating and refining test cases still requires large
human effort.
Typical roles in development involve software testing, such
as writing unit tests, integration tests, and fuzz tests. Re-
searchers have been attempting to use AI to help generate test
cases since before the 2000. Initial implementations typically
involved simpler forms of AI and machine learning to auto-
mate parts of the test case generation process. Over time, more
sophisticated methods such as natural language processing
and machine learning models have been applied to improve
the precision and scope of test case generation. Online tools
like Sofy5, which use machine learning to generate context-
based paths in applications, also exist to aid in generating test
suites. Using large language models to generate test cases is
a relatively new attempt but has been developing rapidly. In
2020, researchers utilized pre-trained language models fine-
tuned on labeled data to generate test cases. They devel-
oped a sequence-to-sequence transformer-based model called
5https://sofy.ai/JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 26
”ATHENATEST” and compared its generated results with
EvoSuite and GPT-3, demonstrating better test coverage [131].
More research and models are being dedicated to test suite
generation experiments, for instance, the Codex model [67],
mentioned earlier in the code generation section, combined
with chain-of-thought prompting, achieved high-quality test
suite generation with CodeCoT, even in zero-shot scenarios.
The introduction of LLMs aims to automate and streamline
the testing process, making it more rigorous and capable of
addressing aspects that humans might easily overlook.
A. LLMs Tasks
The application of LLMs in software test generation is
extensive and encompasses more than just test suite generation.
The reviewed paper included in this survey covers several
aspects, including security test generation, bug reproduction,
general bug reproduction, fuzz testing, and coverage-driven
test generation. These tasks are achieved through various mod-
els and techniques, significantly improving software quality
and reducing developers’ workload. [132] aims to evaluate
the effectiveness of using GPT-4 to generate security tests,
demonstrating how to conduct supply chain attacks by ex-
ploiting dependency vulnerabilities. The study experimented
with different prompt styles and templates to explore the
effectiveness of varying information inputs on test generation
quality, the results showed that tests generated by ChatGPT
successfully discovered 24 proof-of-concept vulnerabilities in
55 applications, outperforming existing tools TRANSFER
[133] and SIEGE6. This research introduces a new method for
generating security tests using LLMs and provides empirical
evidence of LLM’s potential in the security testing domain,
offering developers a novel approach to handling library
vulnerabilities in applications.
Another application is bug reproduction, which allows
testers to locate and fix bugs more quickly and effi-
ciently. [134] addresses the limitations of current bug repro-
duction methods, which are constrained by the quality and
clarity of handcrafted patterns and predefined vocabularies.
The paper proposes and evaluates a new method framework
called AdbGPT, which uses a large language model to auto-
matically reproduce errors from Android bug reports. AdbGPT
is described as outperforming current SOTA approaches in the
context of automated bug replay for only Android system. The
experimental results show that AdbGPT achieved accuracies of
90.4% and 90.8% in S2R entity extraction and a success rate
of 81.3% in error reproduction, significantly outperforming the
baseline ReCDroid and ablation study versions. By introducing
prompt engineering, few-shot learning, and chain-of-thought
reasoning, AdbGPT demonstrates the powerful capabilities
of LLMs in automated error reproduction. It also uses GUI
encoding to convert the GUI view hierarchy into HTML-like
syntax, providing LLMs with a clear understanding of the
current GUI state. While AdbGPT is specialized for Android
systems, [135] proposes the LIBRO framework, which uses
LLMs to generate bug reproduction tests from bug reports.
6https://siegecyber.com.au/services/penetration-testing/The experimental results show that LIBRO successfully re-
produced 33.5% of bugs in the Defects4J dataset and 32.2%
in the GHRB dataset. By combining advanced prompt engi-
neering and post-processing techniques, LIBRO demonstrates
the effectiveness and efficiency of LLMs in generating bug
reproduction tests. Although LIBRO has a lower absolute
effectiveness compared to AdbGPT, it was tested across a more
diverse set of Java applications and not limited to Android.
Therefore, while AdbGPT excels in specialized bug replay
for Android, LIBRO provides a wider range of bug repro-
duction for Java applications. The extensive application of
LLMs in test generation tasks such as security test generation,
bug reproduction, fuzz testing, program repair, and coverage-
driven test generation highlights their significant potential
in improving software quality and reducing the burden on
developers. Through various models and techniques, these
tasks demonstrate how LLMs can automate and enhance the
software testing process, addressing aspects that are often
overlooked by humans.
Similarly, in fuzz testing, LLMs have shown promise poten-
tial. [136] developed a universal fuzzing tool, Fuzz4All, which
uses LLMs to generate and mutate inputs for various software
systems. This tool addresses the issues of traditional fuzzers
being tightly coupled with specific languages or systems and
lacking support for evolving language features. The study
conducted various experiments to test the tool’s capabilities,
including coverage comparison, bug finding, and targeted
fuzzing. The results showed that Fuzz4All achieved the highest
code coverage in all tested languages, with an average increase
of 36.8%, and discovered 98 bugs across nine systems, which
considered as state-of-art technique in universal fuzzing with
LLMs at that time. Through self-prompting and LLM-driven
fuzzing loops, Fuzz4All demonstrated the effectiveness of
LLMs in fuzz testing and showcased their capability across
multiple languages and systems under test (SUTs) through
comprehensive evaluations.
[137] introduced SymPrompt, a new code-aware prompting
strategy aimed at addressing the limitations of existing Search-
Based Software Testing (SBST) methods and traditional LLM
prompting strategies in generating high-coverage test cases. By
decomposing the original test generation process into a multi-
stage sequence aligned with the execution paths of the method
under test, SymPrompt generated high-coverage test cases.
Experimental results indicated that SymPrompt increased cov-
erage on CodeGen2 and GPT-4 by 26% and 105% respectively.
Through path constraint prompting and context construction
techniques, SymPrompt demonstrated the potential of LLMs
in generating high-coverage test cases. [138] also focused
on test suite coverage, this study introduced the COVERUP
system which generates high-coverage Python regression tests
through coverage analysis and interaction with LLMs. The
experimental results showed that COVERUP increased code
coverage from 62% to 81% and branch coverage from 35% to
53% through iterative prompting and coverage-driven meth-
ods. [139] proposed the AID method, which combines LLMs
with differential testing to improve fault detection in ”plausibly
correct” software. By comparing the effectiveness of AID in
generating fault-revealing test inputs and oracles, the experi-JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 27
ments showed that AID improved recall and precision by 1.80
times and 2.65 times respectively, and increased the F1 score
by 1.66 times. By integrating LLMs with differential testing,
AID showcased the powerful capability of LLMs in detecting
complex bugs.
B. LLM-based Agents Tasks
In the field of software test generation, the application of
LLM-based agents demonstrates their potential in automated
test generation. While relying on LLM-based agents for soft-
ware test generation might seem excessive, more research
is directed towards vulnerability detection and system main-
tenance. LLM-based agents can enhance test reliability and
quality by distributing tasks such as test generation, execution,
and optimization through a multi-agent collaborative system.
These multi-agent systems offer obvious improvements in
error detection and repair, and coverage testing. An example
of such a system is AgentCoder’s multi-agent framework, as
discussed in the code generation and software development
section [82]. The primary goal of this system is to leverage
multiple specialized agents to iteratively optimize code gener-
ation, overcoming the limitations of a single agent model in
generating effective code and test cases. The paper introduce
the test design agent, which creates diverse and comprehensive
test cases; and the test execution agent, which executes the
tests and provides feedback, it reached an 89.9% pass rate
on the MBPP dataset. Similarly, the SocraTest framework
falls under the Autonomous Learning and Decision Making
topic [106]. This framework automates the testing process
through conversational interactions, the paper presents detailed
examples of generating and optimizing test cases using GPT-
4, emphasizing how multi-step interactions enhance testing
methods and generate test code. Experimental results show
that through conversational LLMs, SocraTest can effectively
generate and optimize test cases and utilize middleware to
facilitate interactions between the LLM and various testing
tools, achieving more advanced automated testing capabilities.
The paper collected for the software test generation topic
are mostly multiple agents based system. The study [140]
evaluates the effectiveness of LLMs in generating high-
quality test cases and identifies their limitations. It proposes
a novel multi-agent framework called TestChain. The paper
evaluates StarChat, CodeLlama, GPT-3.5, and GPT-4 on the
HumanEval and LeetCode-hard datasets. Experimental results
show that the TestChain framework, using GPT-4, achieved
71.79% accuracy on the LeetCode-hard dataset, an improve-
ment of 13.84% over baseline methods. On the HumanEval
dataset, TestChain with GPT-4 achieved 90.24% accuracy. The
TestChain framework designs agents to generate diverse test
inputs, maps inputs to outputs using ReAct format dialogue
chains, and interacts with the Python interpreter to obtain
accurate test outputs.
LLM-based agents can also be applied in user acceptance
testing (UAT), [141] aims to enhance the automation of the
WeChat Pay UAT process by proposing a multi-agent collab-
orative system named XUAT-Copilot, which uses LLMs to
automatically generate test scripts. The study evaluates XUAT-
Copilot’s performance on 450 test cases from the WeChatPay UAT system, comparing it to a single-agent system and a
variant without the reflection component. Experimental results
show that XUAT-Copilot achieved a Pass@1 rate of 88.55%,
compared to 22.65% for the single-agent system and 81.96%
for the variant without the reflection component, with a
Complete@1 rate of 93.03%. XUAT-Copilot employs a multi-
agent collaborative framework, including action planning, state
checking, and parameter selection agents, and uses advanced
prompting techniques. XUAT-Copilot demonstrates the poten-
tial and feasibility of LLMs in automating UAT test script
generation.
C. Analysis
FIG. 7: I LLUSTRATION OF COMPARISON FRAMEWORK BETWEEN
LLM- BASED AGENT [141] AND LLM [136] INSOFTWARE TEST
GENERATION
In comparison, LLMs perform well in single-task imple-
mentations, generating high-quality test cases through tech-
niques like prompt engineering and few-shot learning. The
number of related studies is increasing as the capabilities
of LLMs improve. On the other hand, LLM-Based Agents,
through multi-agent collaborative systems, decompose tasks
for specialized processing, significantly enhancing the ef-
fectiveness and efficiency of test generation and execution
through iterative optimization and feedback. Considering the
cost, using LLMs for test generation only is enough and
more cost saving than using LLM-based agents. However, if a
specific model performs poorly, it can affect the entire system’s
performance.
A single LLM may struggle with complex, multi-step tasks.
For example, in high-coverage test generation, LLMs may
require more complex prompts and post-processing steps to
achieve the desired results. Additionally, the quality of the
generated results depends heavily on the prompt design and
quality. For tasks requiring fine control and continuous opti-
mization, a single LLM may find it challenging to deal with.
As shown in Figure. ??, the LLM framework uses [136] as an
example to demonstrate the usage of LLMs in fuzz testing, theJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 28
prompt will be optimized by given code snippets (fuzz inputs),
and re-select by the LLM again to choose the best prompt for
the future generation. The overall framework lacks autonomy,
the LLM-based agent [141] framework on the left fills this gap,
as well as able to perceive the UI and interact with the skill
library for the operations. The operation agent will receive
any error reported by the inspection agent and do the self-
reflection to refine the process autonomously. However, as
previously discussed, build a LLM-based agents framework
only for the software test generation task are ”overkill”, so
the collected paper for LLM-based agents system generally
focused on program repair by generated test cases or bug
replay system, like in the Figure. ??, the LLM-based agent
framework is actually used for automatically test the Wechat
Pay system.
D. Benchmarks
In the tasks of LLMs in software test generation, the
dataset Defects4J used to evaluate bug reproduction and
program repair techniques. Other public datasets such as
ReCDroid, ANDROR2+, and Themis are primarily used to
evaluate mobile application bug reproduction and security
test generation, particularly for the Android platform. GCC,
Clang, Go toolchain, Java compiler (javac), and Qiskit involve
fuzz testing datasets for various programming languages and
toolchains, aimed at assessing the effectiveness of fuzz testing
in multi-language environments. TrickyBugs and EvalPlus are
datasets containing complex bug scenarios, used to evaluate
the precision and recall of generated test cases, the benchmark
applications evaluated by CODAMOSA are used to assess the
effectiveness of coverage-based test generation tools.
The datasets used in LLM-Based Agents research are also
quite common, HumanEval, MBPP, and LeetCode-hard are
mainly used to evaluate the accuracy and coverage of code
generation and test generation, involving various program-
ming problems and challenges which frequently appeared in
previous sections. Datasets like Codeflaws, QuixBugs, and
ConDefects are collected to familiarize LLMs with erroneous
code and programs, containing multiple program errors and
defects, and are used to evaluate the effectiveness of automated
debugging and bug repair. A unique dataset is the WeChat Pay
UAT system, which includes user acceptance test cases from
actual applications and is used to evaluate the performance
of multi-agent systems in user acceptance testing, focusing
specifically on WeChat’s security system.
Overall, the datasets used in LLM-based agents’ research
are broader covering a wide range of programming problems
and challenges, LLM research is more focused on the actual
generation tasks, such as bug reproduction on the Android
platform and fuzz testing in multi-language environments.
This because the LLM-Based agents not only focus on the
quality of generated test cases and code but also evaluate
the collaborative effects and iterative optimization capabilities
of multi-agent systems, so the benchmarks also include the
dataset used to evaluate performance of the framework. For in-
stance, AgentCoder [82] improves the efficiency and accuracy
of test generation and execution through multi-agent collab-
oration consider qualitative and quantitative evaluations andusing MBPP,HummanEval to do the evaluations, researches
on LLM-Based agents places more emphasis on verifying the
effectiveness of the system through qualitative evaluation and
user feedback.
E. Evaluation Metrics
As seen in Table IX, LLMs research predominantly utilizes
traditional quantitative metrics such as bug reproduction rate,
code coverage, precision, and recall, these metrics directly
reflect the effectiveness and quality of test generation. In
contrast, LLM-Based agents research not only focuses on
quantitative metrics but also introduces qualitative evaluations,
such as improvements through conversational interactions and
the collaborative effects of multi-agent systems. This diver-
sified evaluation approach provides a more comprehensive
reflection of the system’s practical application effects. From
the task perspectives, LLMs are more inclined to single task
processing, such as generating test sets and considering the
coverage of generated test sets. However, because of the
expansion of agents framework, LLM-based agents often tend
to use the generated test sets to evaluate whether vulnerabilities
can be found to achieve a more ideal practicality. From
a design perspective, LLM systems are relying on prompt
engineering and the generative capabilities of the models
themselves, their evaluation metrics are also mainly focused
on the quality and effectiveness of the model outputs, also
their evaluation metrics include the collaborative effects and
efficiency within the system, such as improving Pass@1 and
Complete@1 rates through multi-agent collaboration. Overall,
LLMs are more suited for rapid test generation and evaluation
for specific tasks, with evaluation metrics directly reflecting
the generation’s effectiveness and quality. LLM-Based Agents
excel in handling complex and diversified tasks, achieving
higher system efficiency and effectiveness through multi-agent
collaboration and iterative optimization.
IX. S OFTWARE SECURITY AND MAINTENANCE
In the software engineering, software security and mainte-
nance is a popular area for the application of LLMs, primarily
aimed at enhancing the security and stability of software
systems through existing technologies to meet the needs of
users and developers. These models provide promise methods
of vulnerability detection and repair, while also enabling auto-
mated security testing and innovative maintenance processes.
The application of LLMs in software security and maintenance
encompasses several aspects, including vulnerability detection,
automatic repair, penetration testing, and system robustness
evaluation. Compared to traditional methods, LLMs leverage
natural language processing and generation technologies to
understand and generate complex code and security policies,
thereby automating detection and repair tasks. For exam-
ple, LLMs can accurately identify potential vulnerabilities
by analyzing code structures and contextual information and
generate corresponding repair suggestions which improves the
efficiency and accuracy of vulnerability recovery.
Moreover, LLMs not only exhibit strong capabilities in vul-
nerability detection but also play a role in tasks like penetrationJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 29
TABLE IX: E VALUATION METRICS IN SOFTWARE TESTGENERATION
Reference Paper Benchmarks Evaluation Metrics Agent
[132]26 libraries and 55
applications with
known vulnerabilitiesNumber of applications for
which security tests were successfully
generated.Number of tests that could
demonstrate exploits.No
[134]ReCDroid, ANDROR2+,
Themis Empirical Study DatasetAccuracy of S2R Entity Extraction.
Reproducibility of Bugs.
Runtime Efficiency.
User Satisfaction.No
[135] Defects4J, GHRBBug Reproduction Rate.
Precision and Recall.
Execution Time.
Developer Effort.No
[136]GCC and Clang.
CVC5 and Z3.
Go Toolchain.
Java Compiler (javac).
Qiskit.Code Coverage.
Validity Rate.
Hit Rate.
Bugs Detected.No
[137]897 focal methods from 26
widely used open-source
Python projects.Pass@1.
FM Call@1.
Correct@1.
Line & Branch Coverage.No
[139]TrickyBugs
EvalPlus datasets.Recall.
Precision.
F1 Score.No
[138]Benchmark applications originally
used to evaluate CODAMOSA.Line Coverage.
Branch Coverage.
Line + Branch Coverage.No
[82]HumanEval.
MBPP.
HumanEval-ET.
MBPP-ET.Pass@1 Yes
[106] Not SpecifiedQualitative improvement through
conversational interactions.Yes
[140]HumanEval.
LeetCode-hard.Accuracy.
Line Coverage (Line Cov).
Code-with-Bugs (CwB).Yes
[142]Codeflaws.
QuixBugs.
ConDefects.Number of Correct Patches.
Number of Plausible Patches.
Correctness Rate.Yes
[141]450 test cases from the
WeChat Pay UAT systemPass@1.
Complete@1.Yes
testing and security evaluations. Automated penetration testing
tools, such as PENTESTGPT [143]. LLMs also demonstrate
significant advantages in evaluating system robustness by sim-
ulating various attack scenarios to assess system performance
under different conditions, helping developers better identify
and address potential security issues. Research on LLM-
based agents in software security and maintenance is also
keep growing, these intelligent agents can execute complex
code generation and vulnerability repair tasks and possess
self-learning and optimization capabilities to handle issues
encountered in dynamic development environments. Tools like
RITFIS [144] and NA VRepair [145] have shown potential in
improving the precision and efficiency of program repairs by
using LLM-based agents.
A. LLMs Tasks
In the field of software security and maintenance, research
on LLMs can be categorized into three main areas: vulnerabil-
ity detection, automatic repair, and penetration testing, along
with some evaluation studies. The collected papers reviewed
on LLMs in these domain illustrate their diverse applications
and potential.
1) Program Vulnerability: In the domain of vulnerability
detection, researchers have fine-tuned LLMs to enhance the
accuracy of source code vulnerability detection. [146] aims
to investigate the potential of applying LLMs to the taskof vulnerability detection in source code and to determine
if the performance limits of CodeBERT-like models are due
to their limited capacity and code understanding ability. The
study fine-tuned the WizardCoder model (an improved ver-
sion of StarCoder) and compared its performance with the
ContraBERT model on balanced and unbalanced datasets. The
experimental results showed that WizardCoder outperformed
ContraBERT in both ROC AUC and F1 scores, significantly
improving Java function vulnerability detection performance,
which achieved the state-of-art performance at that time by
improving ROC AUC from 0.66 in CodeBERT to 0.69.
There are study mainly explored the applications of pure
LLMs without any framework architecture in vulnerability
detection, uncovering current challenges. [147] evaluated only
the performance of ChatGPT and GPT-3 models in detecting
vulnerabilities in Java code, the study compared text-davinci-
003 (GPT-3) and gpt-3.5-turbo against a baseline virtual
classifier in binary and multi-label classification tasks. The
experimental results showed that while text-davinci-003 and
gpt-3.5-turbo had high accuracy and recall rates in binary
classification tasks, their AUC (Area Under Curve) scores were
only 0.51, indicating performance equivalent to random guess-
ing. In multi-label classification tasks, GPT-3.5-turbo and text-
davinci-003 did not significantly outperform the baseline vir-
tual classifier in overall accuracy and F1 scores. These findings
indicate that the earlier model like GPT-3 has limited capabil-JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 30
ities in practical vulnerability detection tasks, suggesting the
need for further research and model optimization to improve
their performance in real-world applications, fine-tuning and
optimizing LLMs can significantly enhance their performance
in source code vulnerability detection, However, these models
still face many challenges in practical applications, requiring
further research and technological improvements to enhance
their real-world effectiveness and reliability.
In the later years, [148] introduced a method to incorporate
complex code structures directly into the model learning
process, the GRACE framework combines graph structure
information and in-context learning, using Code Property
Graphs (CPGs) to represent code structure information. By
integrating the semantic, syntactic, and lexical similarities
of code, the framework GRACE addresses the limitations
of text-based LLM analysis, improves the precision and re-
call rates of vulnerability detection tasks. The study utilized
three vulnerability datasets, showing a 28.65% improvement
in F1 scores over baseline models, an important aspect of
vulnerability detection is enhancing LLM performance in code
security tasks. [149] fine-tuned LLMs for specific tasks and
evaluated their performance against existing models such as
ContraBERT. The researchers conducted numerous experi-
ments to determine the optimal model architecture, training
hyperparameters, and loss functions to optimize performance
in vulnerability detection tasks. The study primarily focused on
WizardCoder and ContraBERT, validating their performance
through comparisons on balanced and unbalanced datasets and
developing an efficient batch packing strategy that improved
training speed. Results indicated that with appropriate fine-
tuning and optimization, LLMs could surpass state-of-the-
art models, contributing to more robust and secure software
development practices.
Despite the development of numerous models, it is still
necessary to investigate their practical effectiveness. [150] ex-
plored the effectiveness of code language models (code LMs)
in detecting software vulnerabilities and identified significant
flaws in existing vulnerability datasets and benchmarks. The
researchers developed a new dataset called PRIMEVUL, and
conducted experiments using it, they compared PRIMEVUL
with existing benchmarks such as BigVul to evaluate several
code LMs, including state-of-the-art base models like GPT-
3.5 and GPT-4, using various training techniques and evalu-
ation metrics. The results revealed that existing benchmarks
significantly overestimated the performance of code LMs. For
example, a state-of-the-art 7B model scored an F1 of 68.26%
on BigVul but only 3.09% on PRIMEVUL, highlighting the
gap between current code language models’ performance and
actual requirements for vulnerability detection.
2) Automating Program Repair: In the domain of software
security and maintenance, LLMs have not only been applied to
vulnerability detection but also extensively used for automat-
ing program repair. One study proposed using Round-Trip
Translation (RTT) for automated program repair, researchers
translated defective code into another language and then back
to the original language to generate potential patches. The
study used various language models and benchmarks to eval-
uate RTT’s performance in APR. The experiments exploredhow RTT performs when using programming languages as an
intermediate representation, how RTT performs when using
natural language (English) as an intermediate representation,
and what qualitative trends can be observed in the patches
generated by RTT. Three measurement standards and eight
models were used in the experiments, the results showed that
the RTT method achieved significant repair effects on multiple
benchmarks, particularly excelling in terms of compilation and
feasibility [151]. Similarly, in automated program repair, [145]
introduced several innovative methods. For example, NA VRe-
pair specifically targe to theets C/C++ code vulnerabilities by com-
bining node type information and error types. Du
unique pointer operations and memory management issues in
C/C++, this language poses complexities. The framework uses
Abstract Syntax Trees (ASTs) to extract node type information
and combines it with CWE-derived vulnerability templates
to generate targeted repair suggestions, the study evaluated
NA VRepair on several popular LLMs (ChatGPT, DeepSeek
Coder, and Magicoder) to demonstrate its effectiveness in
improving code vulnerability repair performance. The results
showed that NA VRepair achieved state-of-art performance in
C/C++ program repair task, which improved repair accuracy
by 26% compared to existing methods.
In order to address the two main limitations of existing
fine-tuning methods for LLM-based program repair, which is
the lack of reasoning about the logic behind code changes
and high computational costs associated with fine-tuning large
datasets. [152] introduced the MOREPAIR framework, this
framework improve the performance of LLMs in automated
program repair (APR) by simultaneously optimizing syntactic
code transformations and the logical reasoning behind code
changes, the study used techniques to enhance fine-tuning
efficiency, such as QLoRA (Quantized Low-Rank Adapta-
tion) [153] to reduce memory requirements and NEFTune
(Noisy Embedding Fine-Tuning) [154] to prevent overfitting
during the fine-tuning process. The experiments evaluated
MOREPAIR on four open-source LLMs of different sizes
and architectures (CodeLlama-13B, CodeLlama-7B, StarChat-
alpha, and Mistral-7B) using two benchmarks, evalrepair-C++
and EvalRepair-Java. The results indicated that CodeLlama
improved by 11% and 8% on the first 10 repair suggestions
for evalrepair-C++ and EvalRepair-Java respectively. Another
study introduced the PyDex system, which automatically
repairs syntax and semantic errors in introductory Python
programming assignments using LLMs, the system combines
multimodal prompts and iterative querying methods to gener-
ate repair candidates and uses few-shot learning to improve
repair accuracy. PyDex was evaluated on 286 real student
programs from an introductory Python programming course
and compared against three baselines. The results showed that
PyDex significantly improved the repair rate and effectiveness
compared to existing baselines [155].
[156] introduced a new system named RING that lever-
ages large language models (LLMCs) to perform multilingual
program repair across six programming languages. RING em-
ploys a prompt strategy that minimizes customization efforts,
including three stages: fault localization, code transforma-
tion, and candidate ranking. The results showed that RINGJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 31
was particularly effective in Python, successfully repairing
94% of errors on the first attempt. The study also intro-
duced a new PowerShell command repair dataset, providing
valuable resources for the research community, this research
demonstrated that AI-driven automation makes program repair
more efficient and scalable. Another study, [157] conducted
a comprehensive investigation into function-level automated
program repair, introducing a new LLM-based APR technique
called SRepair. SRepair utilizes a dual-LLM framework to
enhance repair performance, the SRepair framework combines
a repair suggestion model and a patch generation model. It
uses chain-of-thought to generate natural language repair sug-
gestions based on auxiliary repair-related information and then
utilizes these suggestions to generate the repaired function.
The results showed that SRepair outperformed existing APR
techniques on the Defects4J dataset, repairing 300 single-
function errors, with an improvement of at least 85% over pre-
vious techniques. This study demonstrated the effectiveness of
the dual-LLM framework in function-level repair and, for the
first time achieved multi-function error repair, highlighting the
significant potential of LLMs in program repair. By extending
the scope of APR, SRepair paves the way for applying LLMs
in practical software development and evaluation.
3) Penetration Testing: LLMs can also be applied in the
field of penetration testing, where they are used to enhance the
efficiency and effectiveness of automated penetration testing.
Although not as frequently studied as vulnerability detection
and automated repair, this review includes two relevant pa-
pers. [143] investigates the development and evaluation of an
LLM-driven automatic penetration testing tool PENTESTGPT.
The main purpose of this study is to evaluate the perfor-
mance of LLMs in practical penetration testing tasks and
address the issue of context loss during the penetration testing
process, the paper introduces three self-interaction modules
of PENTESTGPT (reasoning, generation, and parsing) and
provides empirical research based on benchmarks involving 13
targets and 182 sub-tasks. It compares the penetration testing
performance of GPT-3.5, GPT-4, and Bard. The experimental
results show that PENTESTGPT’s task completion rate is
228.6% higher than GPT-3.5 and 58.6% higher than GPT-4,
this study demonstrates the potential of LLMs in automated
penetration testing, helping to identify and resolve security
vulnerabilities, thereby enhancing the security and robustness
of software systems.
A similar research paper explores the application of genera-
tive AI in penetration testing. [158] evaluates the effectiveness,
challenges, and potential consequences of using generative
AI tools (specifically ChatGPT 3.5) in penetration testing.
Through practical application experiments. The research con-
ducts a five-stage penetration test (reconnaissance, scanning,
vulnerability assessment, exploitation, and reporting) on a vul-
nerable machine from VulnHub, integrating Shell GPT (sgpt)
with ChatGPT’s API to automate guidance in the penetration
testing process. The experimental results demonstrate that
generative AI tools can significantly speed up the penetration
testing process and provide accurate and useful commands,
enhancing testing efficiency and effectiveness. This study in-
dicates that the need to consider potential risks and unintendedconsequences, emphasizing the importance of responsible use
and human oversight. Assessing the robustness of systems is
also a crucial part of development, LLMs are used to develop
and evaluate new testing frameworks to detect and improve the
robustness of intelligent software systems. [144] introduces
a robust input testing framework named RITFIS, designed
to evaluate the robustness of LLM-based intelligent software
against natural language inputs. The study adapts 17 existing
DNN testing methods to LLM scenarios and empirically
validates them on multiple datasets to highlight the current
robustness deficiencies and limitations of LLM software. The
study indicate that RITFIS effectively assesses the robustness
of LLM software and reveals its vulnerabilities in handling
complex natural language inputs. This research underscores
the importance of robustness testing for LLM-based intelligent
software and provides directions for improving testing meth-
ods to enhance reliability and security in practical applications.
B. LLM-based Agents Task
LLM-based Agents primarily appied in areas such as
autonomous decision-making, task-specific optimization, and
multi-agent collaboration, these frameworks showcasing their
strong potential in proactive defense. [159] aims to address
the limitations of existing debugging methods that treat the
generated program as an indivisible entity. By segmenting the
program into basic blocks and verifying the correctness of each
block based on task descriptions, the proposed method LDB
(Large Language Model Debugger) provide a more detailed
and effective debugging tool that closely reflects human debug-
ging practices. The study’s experiments covered testing LDB
on several benchmarks and compared with baseline models
without a debugger and those using traditional debugging
methods (self-debugging with explanations and traces). LDB’s
accuracy increased from a baseline of 73.8% to 82.9% on
the HumanEval benchmark, an improvement of 9.1%. In the
domain of vulnerability detection, researchers have enhanced
detection accuracy by combining Role-Based Access Control
(RBAC) practices with deep learning of complex code struc-
tures.
[160] addresses the challenge of automatically and appro-
priately repairing access control (AC) vulnerabilities in smart
contracts. The innovation of this paper lies in combining mined
RBAC practices with LLMs to create a context-aware repair
framework for AC vulnerabilities. The model primarily uses
GPT-4, enhanced by a new method called ACFIX, which
mines common RBAC practices from existing smart contracts
and employs a Multi-Agent Debate (MAD) mechanism to
verify the generated patches through debates between gen-
erator and verifier agents to ensure correctness. Experimen-
tal results show that ACFIX successfully repaired 94.92%
of access control vulnerabilities, significantly outperforming
the baseline GPT-4’s 52.54%. Another application in smart
contracts [161], this paper introduces a two-stage adversarial
framework, GPTLENS, which improves vulnerability detec-
tion accuracy through generation and discrimination phases.
GPTLENS achieved a 76.9% success rate in detecting smart
contract vulnerabilities, better than the 38.5% success rate ofJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 32
traditional methods. Another study, [109] investigates the use
of GPT-4 to automatically exploit disclosed but unpatched vul-
nerabilities, the experiments showed that the LLM-based agent
achieved an 87% success rate in exploiting vulnerabilities
when provided with CVE descriptions. Finally another LLM-
based agent application in the penetration test, [107] employs
GPT-3.5 to assist penetration testers by automating high-level
task planning and low-level vulnerability discovery, thereby
enhancing penetration testing capabilities. The experiments
demonstrated successful automation of multiple stages of pen-
etration testing, including high-level strategy formulation and
low-level vulnerability discovery, showcasing the effectiveness
of LLMs in penetration testing.
In the field of software repair by multi-agents collabora-
tions, [162] proposes a dual-agent framework that enhances
the automation and accuracy of repairing declarative specifi-
cations through iterative prompt optimization and multi-agent
collaboration. The researcher compare the effectiveness of the
LLM-based repair pipeline with several state-of-the-art Alloy
APR techniques (ARepair, ICEBAR, BeAFix, and ATR). In
the result, framework repaired 231 defects in the Alloy4Fun
benchmark which surpassing the 278 defects repaired by tradi-
tional tools. In [142], developed and evaluated an automated
debugging framework named FixAgent, which improves fault
localization, repair generation, and error analysis through
an LLM-based multi-agent system. Although this research
primarily focuses on automated debugging, incorporating el-
ements like fault localization and automated program repair
(APR), it intersects with test generation, particularly in the
validation phase for testing bug fixes. The study evaluates
FixAgent’s performance on the Codeflaws, QuixBugs, and
ConDefects datasets, comparing it to 16 baseline methods,
including state-of-the-art APR tools and LLMs. Experimental
results show that FixAgent fixed 78 out of 79 bugs in the
QuixBugs dataset, including 9 never-before-fixed bugs. In the
Codeflaws dataset, FixAgent fixed 3982 out of 2780 defects,
with a correctness rate of 96.5%. The framework includes
specialized agents responsible for localization, repair, and
analysis tasks and uses the rubber duck debugging principle.
FixAgent demonstrates the powerful capabilities of LLMs in
automated debugging, improving the performance of existing
APR tools and LLMs which can be considered as state-of-art
framework in APR by LLM-based agent.
[46] introduces an automated program repair agent named
RepairAgent, this agent can dynamically generates prompts
and integrates tools to automatically fix software bugs. This
researcher also address the limitations of current LLM-based
repair techniques, which typically involve fixed prompts or
feedback loops that do not allow the model to gather compre-
hensive information about the bug or code. RepairAgent is a
LLM-based agent designed to alternately collect information
about the bug, gather repair ingredients, and validate the re-
pairs, similar to how human developers fix bugs. RepairAgent
achieved impressive result by overall repaired 186 bugs in
the Defects4J benchmark, with 164 being correctly repaired
outperforming existing repair techniques achieved the state-
of-art performances.
In the realm of software security, researchers have combinedLLM and security engineering models to improve security
analysis and design processes. [163] aims to propose a com-
plex hybrid strategy to ensure the reliability and security of
software systems, this involves a concept-guided approach
where LLM-based agents interact with system model diagrams
to perform tasks related to safety analysis. [108] introduces
the TrustLLM framework which increase the accuracy and
interpretability of smart contract auditing by customizing
LLM capabilities to the specific requirements of smart con-
tract code. This paper conducts experiments on a balanced
dataset comprising 1,734 positive samples and 1,810 negative
samples, comparing TrustLLM with other models such as
CodeBERT, GraphCodeBERT, and several versions of GPT
and CodeLlama. TrustLLM achieves an F1 score of 91.21%
and an accuracy of 91.11% which outperforming other models.
Beyond software-level security design, LLMs can also be
integrated into autonomous driving systems. [164] which has
already been discussed in the IV.
C. Analysis
Overall, the direction of LLM-based Agents represents
significant innovative advancements in software security and
maintenance, demonstrating improvements across all areas.
LLM-based Agents, through multi-agent collaboration and
runtime information tracking to help with debugging tasks,
compared to traditional LLMs approaches are often rely on
fixed prompts or feedback loops to debug a given code snippet
or program. In vulnerability detection, LLM-based Agents
combine RBAC practices and in-depth learning of complex
code structures to improve the accuracy and efficiency of
detecting vulnerabilities, traditional LLMs methods normally
depend on extensive manual intervention and detailed guid-
ance when handling tasks. LLM-based Agents also demon-
strate effectiveness in penetration testing by automating high-
level task planning and low-level vulnerability exploration,
thereby enhancing penetration testing capabilities. In contrast,
traditional LLM methods are more suited for passive detection
and analysis, lacking proactive testing and defense capabilities.
From the perspective of automation, LLM-based agents
automate the detection and repair of software errors through
multi-agent frameworks and dynamic analysis tools, improving
the automation and accuracy of the repair process. Traditional
LLMs methods also have a good performance on various main-
tenance or debug tasks, but often lack autonomous decision-
making and dynamic adjustment capabilities during the repair
process. In terms of software security, intelligent agent become
more flexibly by combining LLM and security engineering
models to improve security analysis and design processes,
thereby enhancing the reliability and security of software
systems. when dealing with security tasks by LLMs only, often
rely on static analysis lacking adaptability and optimization ca-
pabilities. As shown in the Figure.8, the comparison using the
MOREPAIR [152] for LLMs and RepairAgent [46] for LLM-
based agents. The LLM framework utilize the optimization
techniques (QLoRA, NEFTune) to generate repair advices, the
RepairAgent utilize multiple tools during the inspection which
facilitate the precision and accuracy of the analysis before theJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 33
FIG. 8: I LLUSTRATION OF COMPARISON FRAMEWORK BETWEEN
LLM- BASED AGENT [46] AND LLM [152] INSOFTWARE SECU-
RITY AND MAINTENANCE
repair process, the idea is quite similar with ”reasoning before
action”. Then the agent framework utilize state machine and
LLM to refine continuously, and if failed during the repair
process, the RepairAgent will enter the self-reflection phase
to understand the reason autonomously.
Thus, from the review, we can say that LLM-based agents
brings more autonomy and flexibility in the field of software
security and maintenance. These improvements can enhance
task execution efficiency and accuracy, also extend the appli-
cation scope of LLMs in complex software engineering tasks,
demonstrating their strong potential in proactive defense, com-
plex task handling, and meeting high reliability requirements.
D. Benchmarks
When analyzing the benchmarks used in LLM literature,
several public datasets stand out due to their frequent use and
presence across different application scenarios. Datasets such
as Defects4J, Codeflaws, QuixBugs, and the Common Vulner-
ability and Exposure (CVE) database are commonly employed
in the domains of vulnerability detection and software security.
For instance, Defects4J is widely used in papers like [46]
and [159] to evaluate automated program repair tools. Sim-
ilarly, Codeflaws and QuixBugs are used in papers like [142]
to test debugging capabilities, focusing on smaller algorithmic
problems typically found in competitive programming and
educational settings. These datasets effectively measure the
ability of LLMs to detect vulnerabilities and modify code in
specific code blocks.
CVE is a critical benchmark for evaluating the security
capabilities of LLMs, offering a repository of known vulnera-
bilities that allow LLMs to assess their ability to autonomously
detect and exploit security flaws, bridging the gap between
theoretical research and practical cybersecurity applications.
Another notable dataset is ARepair, used in [162]. This dataset
consists of defective specifications and tests the ability of
LLMs to understand and repair formal specifications. Morecommon datasets like HumanEval and MBPP are also fre-
quently used to evaluate the functional correctness of code
generated by LLMs. Similarly, Alloy4Fun is used to test the
repair of declarative specifications in Alloy framework [162],
reflecting the LLM’s performance in understanding and fixing
logical errors in formal languages.
Specialized datasets such as VulnHub and HackTheBox
are used to evaluate the penetration testing capabilities of
LLMs. Papers like [107] utilize these environments to simulate
real-world hacking scenarios, thereby assessing the practical
applications of LLMs in cybersecurity. These benchmarks
are crucial for evaluating the real-world efficacy of LLM-
based agents in cyber-security environments, bridging the gap
between theoretical capabilities and practical applications. In
the context of smart contract security, datasets extracted from
Etherscan and those compiled for tools like SmartFix provide
benchmarks for evaluating LLMs’ ability to identify and fix
vulnerabilities in blockchain-based applications, emphasizing
the reliability and security of decentralized applications.
When comparing the benchmarks used in LLM and LLM-
based agent research, several key similarities and differences
emerge. Both approaches frequently use datasets like De-
fects4J, CVE, and HumanEval, highlighting their foundational
role in evaluating software engineering tasks. However, LLM-
based agent research often combines these datasets with spe-
cialized benchmarks like VulnHub and HackTheBox to test
more dynamic and interactive capabilities, especially in the
context of cybersecurity. LLM-based agent research typically
focuses more on real-time autonomous decision-making and
action, reflected in their choice of benchmarks. These datasets
test not only the agents’ knowledge but also their ability to
autonomously apply this knowledge in real-world scenarios.
This contrasts with traditional LLMs research, which typ-
ically focuses on static tasks like vulnerability repair and
code generation without requiring real-time interaction and
further changes or decision-making. Moreover, the use of
specialized benchmarks like the smart contract datasets from
Etherscan in LLM-based agent research underscores the im-
portance of blockchain technology and the need for robust
security measures in decentralized applications, this trend
highlights the adaptability and diversity of LLM-based agents
in addressing emerging challenges in software security and
maintenance. This distinction reflects the broader and more
interactive application scenarios of LLM-based agents, also
the public dataset may not suitable for LLM-based agent in
particular structure designed, so there are a lot of self-collected
benchmark emerged which provide more flexibility.
E. Evaluation Metrics
The evaluation metrics for the llm in the software se-
curity and maintenance are quite diverse. Researchers need
to consider various factors such as coverage, efficiency, and
reliability of the model or framework. Evaluation Metrics
like success rate and pass rate are directly related to the
performance of LLMs in different scenarios. In Table X,
common standards such as success rate and change rate are
frequently used to evaluate the robustness of models whenJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 34
faced with diverse inputs. Time overhead and query number
are used to assess the efficiency and resource consumption
of models when performing specific tasks. Additionally, ROC
AUC, F1 score, and accuracy are important for evaluating
the model’s ability to identify vulnerabilities, especially in
binary classification tasks. In code repair tasks, metrics such as
compilability and plausibility are very common, these metrics
ensure that the generated solutions are correct and deployable.
Common standards like BLEU and CodeBLEU are used to
evaluate the quality and human-likeness of generated code,
which helps determine if the model’s capabilities and perfor-
mance are comparable to human performance. Furthermore,
domain-specific metrics like tree edit distance and test pass
rate are used to evaluate the effectiveness of LLM applications
in specialized fields of software engineering, these metrics are
used to address the limitations posed by software security
and maintenance. In contrast, while LLM-based agents use
evaluation metrics similar to those used for LLMs, such as
success rate, they also incorporate more subjective metrics
for evaluation. These include appropriateness, relevance, and
adequacy, which are human-judged standards. Overall, the
evaluation metrics used by agents tend to be simpler and
easier to understand than those used for LLMs. This is likely
because agents handle high-level tasks, such as the success
rate of generating potential vulnerabilities and the frequency
of agents calling external tools, so they also need to consider
computational and time overheads of the overall architecture.
By comparing these metrics, we can see that LLMs empha-
sising the success rate of individual testing methods, LLM-
based agents focus more on the overall task completion
time/cost/effectiveness. LLMs typically use binary classifica-
tion metrics like ROC, AUC, and F1 score, while agents tend
to emphasize the success rate and accuracy during both the
generation and validation phases, providing a comprehensive
evaluation. For the time cost and performance, LLMs mainly
focus on the execution time of testing methods and the number
of queries to assess their efficiency. In contrast, LLM-based
agents focus more on the completion time of repair tasks and
the number of API calls, it will make sure the efficiency and
practicality of overall architecture.
X. D ISCUSSION
A. Experiment Models
In section 3-8, we reviewed and introduced the research
on LLMs and LLM-based agent applications in software
engineering in recent years. These studies have different
research directions and we divided them into six subtopics
for classification and discussion. With the advancement of
large language models, thousands of models have appeared
in the public eye, in order to more intuitively understand the
application of large language models in various fields and the
use of large language models as the core of intelligent agents,
we summarized a total of 117 papers, mainly to discuss the
frequency of use of LLMs in the field of software engineering.
Based on the review of 117 papers, our primary focus
is on the models or frameworks utilized by the authors in
their experiments. This is due to the fact that these papersoften include tests of model performance in specific domains,
such as evaluating LLaMA’s performance in code generation.
Therefore, during our data collection process, we also included
models used for comparison purposes, as these models often
represent the state-of-the-art capabilities in their respective
fields at the time of the study. In summary, across the 117
papers, we identified a total of 79 unique large language
models. We visualized the frequency of these model names
in a word cloud for a more intuitive representation, as shown
in Figure.9. From the figure, we can observe that models
such as GPT-3.5, GPT-4, LLaMA2, and Codex are frequently
used. Although close source LLMs cannot be locally deployed
or further trained, their exceptional capabilities make them
a popular choice for comparison in experiments or for data
augmentation, where GPT-4 is used to generate additional
data to support the research model frameworks. For instance,
researchers might use OpenAI’s API to generate initial text and
then employ locally deployed models for further processing
and optimization [76] [122] [119] [113].
Therefore, it is not difficult to see that the use of general
large models with superior performance to assist development
or as a measurement standard has been increasingly used in
the vertical field of software engineering in the past two years.
In addition, for some fields that have never been touched
by LLMs before, many researchers first refer to the model
ChatGPT and conduct various performance experiments on the
newer GPT-4 [55] [58] [64]. Those models can be integrated
into larger systems and combining with other machine learning
models and tools, these models can be used to generate
natural language responses, while another model handles intent
recognition and dialogue management.
FIG. 9: E XPERIMENT MODELS USAGE WORDCLOUD
Although the word cloud provides a rough overview of
model usage frequency, it lacks detailed information. To gain
deeper insights, we combined a grouped bar chart and a
stacked bar chart to further analyze the usage of models
in studies across different subtopics. The corresponding bar
charts are presented in Figure.10. During the analysis, we
found that a large number of models appeared only once.
Including these in the bar chart would have made the overall
representation cluttered. Therefore, we excluded models that
appeared only once and focused on the versatility of the
remaining models. On the left side of each subtopic, we
depict the models used in LLM-related studies, with the
models used in LLM-based agent-related studies highlighted
in red-bordered bars. From the figure, it is evident that inJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 35
TABLE X: E VALUATION METRICS IN SOFTWARE SECURITY AND MAINTENANCE
Reference Paper Benchmarks Evaluation Metrics Agent
[144]Financial Sentiment Analysis
Movie Review Analysis
News ClassificationSuccess Rate, Change Rate, Perplexity, Time Overhead, Query Number No
[146]CVEfixes
Manually-Curated Dataset
(624 vulnerabilities across
205 Java projects)
VCMatch
(10 popular repositories)ROC AUC, F1 Score, Accuracy, Optimal Classification, Threshold No
[149]CVEfixes
Manually-Curated Dataset
VCMatchPrecision, Recall No
[143]HackTheBox
VulnHubOverall Task Completion, Sub-task Completion, Task Variety, Challenge
Levels, Progress TrackingNo
[151]Defects4J v1.2
Defects4J v2.0
QuixBugs
HumanEval-JavaCompilability, Plausibility, Test pass rate, Exact Match, BLEU No
[148]Devign
Reveal
Big-VulF1 score, Accuracy, Precision,Recall. No
[150]PRIMEVUL
BigVulF1 score, Accuracy, Precision, Recall, VD-S, Pair-wise, evaluation metrics No
[147]Customized GitHub dataset
(308 binary classification and
120 multi-label classification)Precision, Recall, F1-Score, AUC, Accuracy No
[158] VulnHub Output’s Description No
[165]VulDeePecker
SeVCFalse Positive Rate, False Negative Rate, Precision, Recall, F1-score No
[145] CVEFixes CodeBLEU, Tree Edit Distance, Pass@k No
[152]EvalRepair-C++
EvalRepair-JavaTOP-5 and TOP-10, Repair No
[155]Introductory Python
Assignments DatasetRepair Rate, Token Edit Distance No
[156]Multi-languages dataset
(Excel,Power Fx,Python,
JavaScript,C andPowerShell)Exact Matches No
[157] Defects4J 1.2 and 2.0 Plausible Patches, Correct Fix No
[107]Vulnerable Virtual Machine
(lin.security Linux VM)Success Rate Yes
[160] 118 AC VulnerabilitiesSuccess Rate, Exploitation based evaluation, Manual inspection of the
patches.Yes
[161] 13 Smart Contract Bugs Success Rate, Contract level, Trial level Yes
[142]Codeflaws,QuixBugs,
ConDefectsNumber of correctly fixed bugs, Number of plausibly patched bugs,
Correctness rate of generated patchesYes
[162] ARepair,Alloy4Fun Correct@6, Runtime and Token Usage Yes
[163] System Model Graph Accuracy, Effectiveness, Appropriateness Yes
[108]1734 Positive Samples,
1810 Negative SamplesF1 score, Accuracy, Consistency Yes
[166]HumanEval,MBPP,
TransCoderAccuracy, Pass@1 Yes
[139]13 Android Applications
from GitHubRecall, Precision, Correct, Over-fitting, Correct@k Yes
[164] System Model Graph Relevance, Adequacy Yes
[109] 15 Vulnerabilities from CVE Lib Success Rate Yes
[46] Defects4J Plausible Fixes, Correct Fixes Yes
the Autonomous Learning and Decision Making subtopic, the
number of models used in LLM-based agent-related studies
is quite high. Specifically, GPT-4 and GPT-3.5 were used in
10 out of 18 papers and 15 out of 18 papers, respectively.
In this subtopic, studies commonly utilized GPT-3.5/4 and
LLaMA-2 for research and evaluation. During our analysis, we
found that many studies on LLM-based agents evaluated the
agents’ ability to mimic human behavior and decision-making
or perform some reasoning tasks [103] [111] [108]. Since these
studies do not require local deployment, they mainly assess the
performance of state-of-the-art models in specific directions,
leading to the frequent use of the GPT-family models. Frame-
works like [98] [36] constructed LLM-based agents by calling
the GPT-4 API, using verbal reinforcement to help language
agents learn from their mistakes. Due to the limitations of
GPT models, many studies also used LLaMA as the LLMfor agents, fine-tuning it on the generated datasets to evaluate
the emergence of knowledge and capabilities. Overall, we
found that in the Autonomous Learning and Decision Making
subtopic, LLM-based agents often use multiple models for
testing and performance evaluation in a single task, this results
in a significantly higher model usage frequency in this topic
compared to others.
Not only in the Autonomous Learning and Decision Making
subtopic, but also across other themes, we observe that the
variety of models (represented by the number of colors) used
by LLM-based agents is relatively limited. For instance, in
the requirement engineering and documentation subtopic, only
GPT-3.5 and GPT-4 models were involved in the experiments.
To analyze the reasons behind this phenomenon, we need to
exclude the factors that models appearing only once were
not considered and that there are inherently fewer studiesJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 36
REQCODE AUTODESTESTSEC
Subtopics051015202530354045Frequency
68
2552222
2342
267
24
2222742
22
287
101511243
3772
226
288LLM BERT
LLM GPT-3.5
LLM Alpaca
LLM CodeGen
LLM Codex
LLM GPT-4
LLM InCoder
LLM LLaMA
LLM StarCoder
LLM code-davinci-002
LLM GPT-3
LLM CodeBERT
LLM CodeLlama
LLM CodeT5
LLM WizardCoder
Agent GPT-3.5
Agent GPT-4
Agent CodeLlama
Agent Llama2-13B
Agent Mixtral
Agent Vicuna
Agent text-davinci-003
Agent GPT-3
FIG. 10: E XPERIMENT MODELS USAGE IN DIFFERENT SUBTOPICS (REQ DENOTES ”R EQUIREMENT ENGINEERING AND DOCUMEN -
TATION ”, CODE DENOTES ”C ODE GENERATION AND SOFTWARE DEVELOPMENT ”, AUTO DENOTES ”A UTONOMOUS LEARNING
AND DECISION MAKING ”, DES DENOTES ”S OFTWARE DESIGN AND DECISION MAKING ”, SEC DENOTES ”S OFTWARE SECURITY
AND MAINTENANCE ”)
on intelligent agents. We believe this primarily reflects the
integration relationship between the agents and the large
language models. The combination of these two technologies
aims to address the limitations of large language models in
specific tasks or aspects. Intelligent agents allow researchers
to design a more flexible framework and incorporate the large
language model into it. These models, having been trained on
vast amounts of data, possess strong generalizability, making
them suitable for a wide range of tasks and domains.
Therefore, researchers and developers can use the same
model to address multiple issues, reducing the need for var-
ious models. In code generation [83] [79], test case genera-
tion [140] [142], and software security [167] [159], there are
instances of using CodeLlama. This model is fine-tuned and
optimized based on the LLaMA architecture. At its release,
it was considered one of the state-of-the-art models for code
generation and understanding tasks, showing strong perfor-
mance and potential compared to other models like Codex.
Another potential reason is the previous successful applica-
tions and research outcomes that have proven these models’
effectiveness, further enhancing researchers’ trust and reliance
on them. Compared to models that perform well in specific
domains, in intelligent agent development, there is a preference
for using general-purpose large models to ensure that the core
of the agent possesses excellent text comprehension abilities,allowing for further reasoning, planning, and task execution.
From the Figure.10, we can also observe that research in the
code generation and software development fields adopts a wide
variety of models, further indicating the extensive attention this
area receives and the excellent performance of models in code
generation task.
B. Topics Overlapping
Figure.11 shows the distribution of all collected literature
across six themes. For LLM-type literature, the theme of
software security and maintenance accounts for nearly 30%,
whereas test case generation accounts for less than 10%.
This trend is similarly reflected in the LLM-based agent
literature. Research on using LLM-based agents to address
requirements engineering and test case generation is relatively
sparse. Requirements engineering is a new endeavor for LLM-
based agents, and using the entire agent framework to generate
test cases might be considered excessive. Therefore, more
research tends to evaluate and explore the changes LLMs bring
within the agent framework, such as autonomous decision-
making abilities and capabilities in software maintenance and
repair.
Table‘XI presents the number of papers spanning multiple
themes. For instance, five papers can be classified under both
software security and maintenance and autonomous learningJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 37
FIG. 11: D ISTRIBUTION OF LLM S AND AGENTS ACROSS SIX TOPICS
CODE REQ AUTO DESIGN SEC TEST
CODE X 1 0 2 3 1
REQ 1 X 1 0 2 0
AUTO 0 1 X 6 5 1
DESIGN 2 0 6 X 1 0
SEC 3 2 5 1 X 2
TEST 1 0 1 0 2 X
TABLE XI: O VERLAP OF PAPERS AMONG DIFFERENT TOPICS (REQ DENOTES ”R EQUIREMENT ENGINEERING AND DOCUMENTA -
TION ”, CODE DENOTES ”C ODE GENERATION AND SOFTWARE DEVELOPMENT ”, AUTO DENOTES ”A UTONOMOUS LEARNING AND
DECISION MAKING ”, DES DENOTES ”S OFTWARE DESIGN AND DECISION MAKING ”, SEC DENOTES ”S OFTWARE SECURITY AND
MAINTENANCE ”)
and decision making. These two themes also overlap the most
with other themes, indicating that LLMs and LLM-based agent
research is broad and these tasks often require integrating
knowledge and techniques from various fields such as code
generation, design, and testing. The significant overlap reflects
the close interrelation between these themes and other areas.
For example, autonomous learning and decision-making often
involve the model’s ability to autonomously learn and optimize
decision trees, techniques that are applied in many specific
software engineering tasks. Similarly, software security and
maintenance typically require a combination of multiple tech-
niques to enhance security, such as automatic code generation
tools and automated testing frameworks [71] [80] [83] [102].
The overlap in literature highlights the increasing need for
integrating methods and techniques from different research
areas within software engineering. For instance, ensuring
software security relies not only on security measures but also
on leveraging code generation, automated testing, and design
optimization technologies. Similarly, autonomous learning and
decision-making require a comprehensive consideration of
requirements engineering, code generation, and system design.
Moreover, it suggests that certain technologies and methods
possess strong commonality. For instance, LLM-based agentsenhance capabilities in code generation, test automation, and
security analysis through autonomous learning and decision-
making. This sharing of technologies promotes knowledge
exchange and technological dissemination across various fields
within software engineering.
C. Benchmarks and Metrics
As shown in Figure.12, it include the distribution of com-
mon benchmarks across six topics. In reality, the number of
benchmark datasets used is far greater than what is shown
in the figure. Different software engineering tasks use various
benchmark datasets for evaluation and testing. For instance,
in requirements engineering, researchers often collect user
stories or requirement specifications as datasets [55] [63], and
these datasets are not well-known public datasets, so they
were not included in the statistics. Alternatively, some studies
specify their datasets as “Customized GitHub datasets” [168].
Therefore, the benchmark datasets shown in the figure repre-
sent commonly used public datasets. For example, MBPP and
HumanEval, which have been introduced in previous sections,
are frequently used. We can also observe that the datasets used
in LLM and LLM-based agents tasks, apart from common
public datasets, are different.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 38
For instance, the FEVER7dataset is often used in agent-
related research. In [35], the FEVER dataset is used to test the
ExpeL agent’s performance in fact verification tasks. Similarly,
the HotpotQA8dataset is frequently used in agent-related
research for knowledge-intensive reasoning and question-
answering tasks. When handling vulnerability repair tasks,
LLMs often use the Defects4J9benchmark dataset. This
dataset contains 835 real-world defects from multiple open-
source Java projects, categorized into buggy versions and
repaired versions, typically used to evaluate the effectiveness
of automated program repair techniques. Despite its extensive
use in LLM research, Defects4J is relatively less used in
LLM-based agents research. We speculate that this may be
because Defects4J primarily evaluates single code repair tasks,
which do not fully align with the multi-task and real-time
requirements of LLM-based agents. Additionally, new datasets
like ConDefects have been introduced [142], focusing on
addressing data leakage issues and providing more compre-
hensive defect localization and repair evaluations.
As shown in Figure.13, it includes the top ten evaluation
metrics for LLMs and LLM-based agents. The analysis reveals
that the evaluation methods used by both are almost identical.
In previous sections, we also discussed that for agents, it
is necessary to consider time and computational resource
consumption, which is evident from the pie chart. Meanwhile,
many studies focus on the code generation capabilities of
LLMs, so more evaluation metrics pertain to the correctness
and Exact Match of the generated code [73] [69] [30], but
overall, the evaluation metrics for LLMs and LLM-based
agents in software engineering applications are quite similar.
XI. C ONCLUSION
In this paper, we conducted a comprehensive literature
review on the application of LLM and LLM-based agents
in software engineering. We categorized software engineering
into six topics: requirement engineering and documentation,
code generation and software development, autonomous learn-
ing and decision making, software design and evaluation,
software test generation, and software security and mainte-
nance. For each topic, we analyzed the tasks, benchmarks, and
evaluation metrics, distinguishing between LLM and LLM-
based agents and discussing the differences and impacts they
bring. We further analyzed and discussed the models used in
the experiments of the 117 collected papers. Additionally, we
provided statistics and distinctions between LLM and LLM-
based agents regarding datasets and evaluation metrics. The
analysis revealed that the emergence of LLM-based agents
has led to extensive research and applications across various
software engineering topics, demonstrating different emphases
compared to traditional LLMs in terms of tasks, benchmarks,
and evaluation metrics.
REFERENCES
[1] S. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan, “Bugram:
bug detection with n-gram language models,” in Proceedings of the
7https://fever.ai/dataset/fever.html
8https://hotpotqa.github.io/
9https://github.com/rjust/defects4j31st IEEE/ACM International Conference on Automated Software
Engineering , pp. 724–735, 2016.
[2] A. V ogelsang and M. Borg, “Requirements engineering for machine
learning: Perspectives from data scientists,” in 2019 IEEE 27th In-
ternational Requirements Engineering Conference Workshops (REW) ,
(Jeju, Korea (South)), pp. 245–251, 2019.
[3] “Chatgpt: Optimizing language models for dialogue,” 11 2022. [Online;
accessed 17-July-2024].
[4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Rad-
ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder,
B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba,
“Evaluating large language models trained on code,” arXiv preprint
arXiv:2107.03374 , 2021. arXiv:2107.03374 [cs.LG].
[5] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Ra-
jamani, and R. Sharma, “Jigsaw: large language models meet program
synthesis,” in Proceedings of the 44th International Conference on
Software Engineering , ICSE ’22, (New York, NY , USA), p. 1219–1231,
Association for Computing Machinery, 2022.
[6] T. Li, G. Zhang, Q. D. Do, X. Yue, and W. Chen, “Long-context llms
struggle with long in-context learning,” 2024.
[7] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin,
and X. Hu, “Harnessing the power of llms in practice: A survey on
chatgpt and beyond,” ACM Trans. Knowl. Discov. Data , vol. 18, apr
2024.
[8] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo,
and J. M. Zhang, “Large language models for software engineering:
Survey and open problems,” in 2023 IEEE/ACM International Confer-
ence on Software Engineering: Future of Software Engineering (ICSE-
FoSE) , pp. 31–53, 2023.
[9] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,
J. Tang, X. Chen, Y . Lin, W. X. Zhao, Z. Wei, and J. Wen, “A
survey on large language model based autonomous agents,” Frontiers
of Computer Science , vol. 18, no. 6, pp. 186345–, 2024.
[10] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,
S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y . Zhou,
W. Wang, C. Jiang, Y . Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng,
Q. Zhang, W. Qin, Y . Zheng, X. Qiu, X. Huang, and T. Gui, “The rise
and potential of large language model based agents: A survey,” 2023.
[11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, S. Riedel, and D. Kiela,
“Retrieval-augmented generation for knowledge-intensive nlp tasks,” in
Advances in Neural Information Processing Systems (H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), vol. 33,
pp. 9459–9474, Curran Associates, Inc., 2020.
[12] GitHub, Inc., “Github copilot: Your ai pair programmer.” https://github.
com/features/copilot, 2024. [Online; accessed 17-July-2024].
[13] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach .
Pearson Education Limited, 2016.
[14] N. R. Jennings, “A survey of agent-oriented software engineering,”
Knowledge Engineering Review , vol. 15, no. 4, pp. 215–249, 2000.
[15] Y . Bisk, A. Holtzman, J. Thomason, J. Andreas, Y . Bengio, J. Chai,
M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian,
“Experience grounds language,” 2020.
[16] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,
D. Zhou, et al. , “Chain-of-thought prompting elicits reasoning in large
language models,” Advances in neural information processing systems ,
vol. 35, pp. 24824–24837, 2022.
[17] X. Hou, Y . Zhao, Y . Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo,
J. Grundy, and H. Wang, “Large language models for software engi-
neering: A systematic literature review,” 2024.
[18] Z. Zheng, K. Ning, J. Chen, Y . Wang, W. Chen, L. Guo, and W. Wang,
“Towards an understanding of large language models in software
engineering tasks,” 2023.
[19] A. Nguyen-Duc, B. Cabrero-Daniel, A. Przybylek, C. Arora,
D. Khanna, T. Herda, U. Rafiq, J. Melegati, E. Guerra, K.-K. Kemell,
M. Saari, Z. Zhang, H. Le, T. Quan, and P. Abrahamsson, “Generative
artificial intelligence for software engineering – a research agenda,”
2023.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 39
[20] W. Ma, S. Liu, Z. Lin, W. Wang, Q. Hu, Y . Liu, C. Zhang, L. Nie,
L. Li, and Y . Liu, “Lms: Understanding code syntax and semantics for
code analysis,” 2024.
[21] Z. Yang, Z. Sun, T. Z. Yue, P. Devanbu, and D. Lo, “Robustness, secu-
rity, privacy, explainability, efficiency, and usability of large language
models for code,” 2024.
[22] Y . Huang, Y . Chen, X. Chen, J. Chen, R. Peng, Z. Tang, J. Huang,
F. Xu, and Z. Zheng, “Generative software engineering,” 2024.
[23] C. Manning and H. Schutze, Foundations of statistical natural language
processing . MIT press, 1999.
[24] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[25] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems , vol. 30, 2017.
[27] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and
consequences,” Minds and Machines , vol. 30, pp. 681–694, 2020.
[28] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. , “Palm: Scaling
language modeling with pathways,” Journal of Machine Learning
Research , vol. 24, no. 240, pp. 1–113, 2023.
[29] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,
C. Dewan, M. Diab, X. Li, X. V . Lin, T. Mihaylov, M. Ott, S. Shleifer,
K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettle-
moyer, “Opt: Open pre-trained transformer language models,” 2022.
[30] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,
“Codet5+: Open code large language models for code understanding
and generation,” arXiv preprint arXiv:2305.07922 , 2023.
[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[32] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. , “Language mod-
els are few-shot learners,” Advances in neural information processing
systems , vol. 33, pp. 1877–1901, 2020.
[33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al. , “Llama:
Open and efficient foundation language models,” arXiv preprint
arXiv:2302.13971 , 2023.
[34] J. X. Chen, “The evolution of computing: Alphago,” Computing in
Science & Engineering , vol. 18, no. 4, pp. 4–7, 2016.
[35] A. Zhao, D. Huang, Q. Xu, M. Lin, Y .-J. Liu, and G. Huang, “Expel:
Llm agents are experiential learners,” in Proceedings of the AAAI
Conference on Artificial Intelligence , vol. 38, pp. 19632–19642, 2024.
[36] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,
“React: Synergizing reasoning and acting in language models,” arXiv
preprint arXiv:2210.03629 , 2022.
[37] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models
as zero-shot planners: Extracting actionable knowledge for embodied
agents,” in International conference on machine learning , pp. 9118–
9147, PMLR, 2022.
[38] G. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan,
and A. Anandkumar, “V oyager: An open-ended embodied agent with
large language models,” arXiv preprint arXiv:2305.16291 , 2023.
[39] C. Whitehouse, M. Choudhury, and A. F. Aji, “Llm-powered data
augmentation for enhanced cross-lingual performance,” 2023.
[40] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. El-
nashar, J. Spencer-Smith, and D. C. Schmidt, “A prompt pattern
catalog to enhance prompt engineering with chatgpt,” arXiv preprint
arXiv:2302.11382 , 2023.
[41] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.
Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. ,
“Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context,” arXiv preprint arXiv:2403.05530 , 2024.
[42] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang,
A. Madotto, and P. Fung, “Survey of hallucination in natural language
generation,” ACM Computing Surveys , vol. 55, no. 12, pp. 1–38, 2023.
[43] K. An, F. Yang, L. Li, Z. Ren, H. Huang, L. Wang, P. Zhao, Y . Kang,
H. Ding, Q. Lin, et al. , “Nissist: An incident mitigation copilot based
on troubleshooting guides,” arXiv preprint arXiv:2402.17531 , 2024.
[44] J. Li, Q. Zhang, Y . Yu, Q. Fu, and D. Ye, “More agents is all you
need,” 2024.
[45] Y . Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,
P. S. Liang, and T. B. Hashimoto, “Alpacafarm: A simulation frame-work for methods that learn from human feedback,” Advances in Neural
Information Processing Systems , vol. 36, 2024.
[46] I. Bouzenia, P. Devanbu, and M. Pradel, “Repairagent: An autonomous,
llm-based agent for program repair,” arXiv preprint arXiv:2403.17134 ,
2024.
[47] E. Musumeci, M. Brienza, V . Suriani, D. Nardi, and D. D. Bloisi, “Llm
based multi-agent generation of semi-structured documents from se-
mantic templates in the public administration domain,” in International
Conference on Human-Computer Interaction , pp. 98–117, Springer,
2024.
[48] X. Luo, Y . Xue, Z. Xing, and J. Sun, “Prcbert: Prompt learning for
requirement classification using bert-based pretrained language mod-
els,” in Proceedings of the 37th IEEE/ACM International Conference
on Automated Software Engineering , pp. 1–13, 2022.
[49] T. Hey, J. Keim, A. Koziolek, and W. F. Tichy, “Norbert: Transfer learn-
ing for requirements classification,” in 2020 IEEE 28th International
Requirements Engineering Conference (RE) , pp. 169–179, 2020.
[50] J. Zhang, Y . Chen, N. Niu, and C. Liu, “Evaluation of chatgpt on
requirements information retrieval under zero-shot setting,” Available
at SSRN 4450322 , 2023.
[51] C. Arora, J. Grundy, and M. Abdelrazek, “Advancing requirements
engineering through generative ai: Assessing the role of llms,” in Gen-
erative AI for Effective Software Development , pp. 129–148, Springer,
2024.
[52] M. Krishna, B. Gaur, A. Verma, and P. Jalote, “Using llms in software
requirements specifications: An empirical evaluation,” 2024.
[53] L. Ma, S. Liu, Y . Li, X. Xie, and L. Bu, “Specgen: Automated gen-
eration of formal program specifications via large language models,”
2024.
[54] C. Flanagan and K. R. M. Leino, “Houdini, an annotation assistant
for esc/java,” in FME 2001: Formal Methods for Increasing Software
Productivity (J. N. Oliveira and P. Zave, eds.), (Berlin, Heidelberg),
pp. 500–517, Springer Berlin Heidelberg, 2001.
[55] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt,
ChatGPT Prompt Patterns for Improving Code Quality, Refactoring,
Requirements Elicitation, and Software Design , pp. 71–108. Cham:
Springer Nature Switzerland, 2024.
[56] D. Luitel, S. Hassani, and M. Sabetzadeh, “Improving requirements
completeness: Automated assistance through large language models,”
Requirements Engineering , vol. 29, no. 1, pp. 73–95, 2024.
[57] A. Moharil and A. Sharma, “Identification of intra-domain ambiguity
using transformer-based machine learning,” in Proceedings of the 1st
International Workshop on Natural Language-Based Software Engi-
neering , NLBSE ’22, (New York, NY , USA), p. 51–58, Association
for Computing Machinery, 2023.
[58] K. Ronanki, B. Cabrero-Daniel, and C. Berger, “Chatgpt as a tool
for user story quality evaluation: Trustworthy out of the box?,” in
Agile Processes in Software Engineering and Extreme Programming
– Workshops (P. Kruchten and P. Gregory, eds.), (Cham), pp. 173–181,
Springer Nature Switzerland, 2024.
[59] A. Poudel, J. Lin, and J. Cleland-Huang, “Leveraging transformer-
based language models to automate requirements satisfaction assess-
ment,” 2023.
[60] E. Musumeci, M. Brienza, V . Suriani, D. Nardi, and D. D. Bloisi, “Llm
based multi-agent generation of semi-structured documents from se-
mantic templates in the public administration domain,” in Artificial
Intelligence in HCI (H. Degen and S. Ntoa, eds.), (Cham), pp. 98–
117, Springer Nature Switzerland, 2024.
[61] S. Zhang, J. Wang, G. Dong, J. Sun, Y . Zhang, and G. Pu, “Experi-
menting a new programming practice with llms,” 2024.
[62] A. Nouri, B. Cabrero-Daniel, F. T ¨orner, H. Sivencrona, and C. Berger,
“Engineering safety requirements for autonomous driving with large
language models,” 2024.
[63] Z. Zhang, M. Rayhan, T. Herda, M. Goisauf, and P. Abrahamsson,
“Llm-based agents for automating the enhancement of user story
quality: An early report,” in Agile Processes in Software Engineering
and Extreme Programming (D.ˇSmite, E. Guerra, X. Wang, M. March-
esi, and P. Gregory, eds.), (Cham), pp. 117–126, Springer Nature
Switzerland, 2024.
[64] K. Ronanki, C. Berger, and J. Horkoff, “Investigating chatgpt’s po-
tential to assist in requirements elicitation processes,” in 2023 49th
Euromicro Conference on Software Engineering and Advanced Appli-
cations (SEAA) , pp. 354–361, 2023.
[65] D. Xie, B. Yoo, N. Jiang, M. Kim, L. Tan, X. Zhang, and J. S. Lee, “Im-
pact of large language models on generating software specifications,”
2023.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 40
[66] A. Moharil and A. Sharma, “Tabasco: A transformer based con-
textualization toolkit,” Science of Computer Programming , vol. 230,
p. 102994, 2023.
[67] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Rad-
ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder,
B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba,
“Evaluating large language models trained on code,” 2021.
[68] A. Ni, P. Yin, Y . Zhao, M. Riddell, T. Feng, R. Shen, S. Yin, Y . Liu,
S. Yavuz, C. Xiong, S. Joty, Y . Zhou, D. Radev, and A. Cohan,
“L2ceval: Evaluating language-to-code generation capabilities of large
language models,” 2023.
[69] R. Sun, S. ¨O. Arik, A. Muzio, L. Miculicich, S. Gundabathula, P. Yin,
H. Dai, H. Nakhost, R. Sinha, Z. Wang, et al. , “Sql-palm: Improved
large language model adaptation for text-to-sql (extended),” arXiv
preprint arXiv:2306.00739 , 2023.
[70] Q. Zheng, X. Xia, X. Zou, Y . Dong, S. Wang, Y . Xue, Z. Wang,
L. Shen, A. Wang, Y . Li, T. Su, Z. Yang, and J. Tang, “Codegeex: A
pre-trained model for code generation with multilingual benchmarking
on humaneval-x,” 2024.
[71] X. Hu, K. Kuang, J. Sun, H. Yang, and F. Wu, “Leveraging print
debugging to improve code generation in large language models,” 2024.
[72] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, “The impact of
ai on developer productivity: Evidence from github copilot,” 2023.
[73] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
W. tau Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative
model for code infilling and synthesis,” 2023.
[74] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,
S. Savarese, and C. Xiong, “Codegen: An open large language model
for code with multi-turn program synthesis,” 2023.
[75] Y . Ding, M. J. Min, G. Kaiser, and B. Ray, “Cycle: Learning to self-
refine the code generation,” Proc. ACM Program. Lang. , vol. 8, apr
2024.
[76] Y . Dong, X. Jiang, Z. Jin, and G. Li, “Self-collaboration code genera-
tion via chatgpt,” 2024.
[77] F. Lin, D. J. Kim, et al. , “When llm-based code generation meets
the software development process,” arXiv preprint arXiv:2403.15852 ,
2024.
[78] S. Holt, M. R. Luyten, and M. van der Schaar, “L2MAC: Large
language model automatic computer for extensive code generation,”
inThe Twelfth International Conference on Learning Representations ,
2024.
[79] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y . Cheng, C. Zhang, J. Wang,
Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu,
and J. Schmidhuber, “Metagpt: Meta programming for a multi-agent
collaborative framework,” 2023.
[80] Z. Rasheed, M. Waseem, K.-K. Kemell, W. Xiaofeng, A. N. Duc,
K. Syst ¨a, and P. Abrahamsson, “Autonomous agents in software
development: A vision paper,” arXiv preprint arXiv:2311.18440 , 2023.
[81] Z. Rasheed, M. Waseem, M. Saari, K. Syst ¨a, and P. Abrahamsson,
“Codepori: Large scale model for autonomous software development
by using multi-agents,” arXiv preprint arXiv:2402.01411 , 2024.
[82] D. Huang, Q. Bu, J. M. Zhang, M. Luck, and H. Cui, “Agentcoder:
Multi-agent-based code generation with iterative testing and optimisa-
tion,” arXiv preprint arXiv:2312.13010 , 2023.
[83] T. Zheng, G. Zhang, T. Shen, X. Liu, B. Y . Lin, J. Fu, W. Chen,
and X. Yue, “Opencodeinterpreter: Integrating code generation with
execution and refinement,” arXiv preprint arXiv:2402.14658 , 2024.
[84] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, E. Ham-
bro, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer:
Language models can teach themselves to use tools,” Advances in
Neural Information Processing Systems , vol. 36, 2024.
[85] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong,
X. Tang, B. Qian, et al. , “Toolllm: Facilitating large language models
to master 16000+ real-world apis,” arXiv preprint arXiv:2307.16789 ,
2023.
[86] X. Jiang, Y . Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin, and
W. Jiao, “Self-planning code generation with large language models,”
ACM Trans. Softw. Eng. Methodol. , jun 2024. Just Accepted.[87] S. Zhang, J. Wang, G. Dong, J. Sun, Y . Zhang, and G. Pu, “Ex-
perimenting a new programming practice with llms,” arXiv preprint
arXiv:2401.01062 , 2024.
[88] V . Murali, C. Maddila, I. Ahmad, M. Bolin, D. Cheng, N. Ghor-
bani, R. Fernandez, and N. Nagappan, “Codecompose: A large-scale
industrial deployment of ai-assisted code authoring,” arXiv preprint
arXiv:2305.12050 , 2023.
[89] J. Huang, S. S. Gu, L. Hou, Y . Wu, X. Wang, H. Yu, and J. Han, “Large
language models can self-improve,” arXiv preprint arXiv:2210.11610 ,
2022.
[90] L. Chen, J. Q. Davis, B. Hanin, P. Bailis, I. Stoica, M. Zaharia, and
J. Zou, “Are more llm calls all you need? towards scaling laws of
compound inference systems,” arXiv preprint arXiv:2403.02419 , 2024.
[91] X. Chen, M. Lin, N. Sch ¨arli, and D. Zhou, “Teaching large language
models to self-debug,” arXiv preprint arXiv:2304.05128 , 2023.
[92] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, “Explainable automated
debugging via large language model-driven scientific debugging,” arXiv
preprint arXiv:2304.02195 , 2023.
[93] G. Franceschelli and M. Musolesi, “On the creativity of large language
models,” arXiv preprint arXiv:2304.00008 , 2023.
[94] J. Lai, W. Gan, J. Wu, Z. Qi, and P. S. Yu, “Large language models in
law: A survey,” 2023.
[95] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang,
Z. Lin, Z. Li, D. Li, E. Xing, et al. , “Judging llm-as-a-judge with mt-
bench and chatbot arena,” Advances in Neural Information Processing
Systems , vol. 36, 2024.
[96] Q. Wang, Z. Wang, Y . Su, H. Tong, and Y . Song, “Rethinking the
bounds of llm reasoning: Are multi-agent discussions the key?,” arXiv
preprint arXiv:2402.18272 , 2024.
[97] L. Chen, Y . Zhang, S. Ren, H. Zhao, Z. Cai, Y . Wang, P. Wang, T. Liu,
and B. Chang, “Towards end-to-end embodied decision making via
multi-modal large language model: Explorations with gpt4-vision and
beyond,” arXiv preprint arXiv:2310.02071 , 2023.
[98] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,
“Reflexion: Language agents with verbal reinforcement learning,”
Advances in Neural Information Processing Systems , vol. 36, 2024.
[99] W. Chen, Y . Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan,
Y . Qin, Y . Lu, R. Xie, et al. , “Agentverse: Facilitating multi-agent
collaboration and exploring emergent behaviors in agents,” arXiv
preprint arXiv:2308.10848 , 2023.
[100] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel:
Communicative agents for” mind” exploration of large language model
society,” Advances in Neural Information Processing Systems , vol. 36,
pp. 51991–52008, 2023.
[101] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y . Feng,
Z. Chen, J. C. Niebles, D. Arpit, et al. , “Bolaa: Benchmarking
and orchestrating llm-augmented autonomous agents,” arXiv preprint
arXiv:2308.05960 , 2023.
[102] J. Lu, W. Zhong, W. Huang, Y . Wang, Q. Zhu, F. Mi, B. Wang,
W. Wang, X. Zeng, L. Shang, X. Jiang, and Q. Liu, “Self: Self-evolution
with language feedback,” 2024.
[103] C. Xie, C. Chen, F. Jia, Z. Ye, K. Shu, A. Bibi, Z. Hu, P. Torr,
B. Ghanem, and G. Li, “Can large language model agents simulate
human trust behaviors?,” arXiv preprint arXiv:2402.04559 , 2024.
[104] Z. Liu, W. Yao, J. Zhang, L. Yang, Z. Liu, J. Tan, P. K. Choubey,
T. Lan, J. Wu, H. Wang, et al. , “Agentlite: A lightweight library for
building and advancing task-oriented llm agent system,” arXiv preprint
arXiv:2402.15538 , 2024.
[105] M. Zhuge, W. Wang, L. Kirsch, F. Faccio, D. Khizbullin, and
J. Schmidhuber, “Language agents as optimizable graphs,” arXiv
preprint arXiv:2402.16823 , 2024.
[106] R. Feldt, S. Kang, J. Yoon, and S. Yoo, “Towards autonomous
testing agents via conversational large language models,” in 2023
38th IEEE/ACM International Conference on Automated Software
Engineering (ASE) , pp. 1688–1693, IEEE, 2023.
[107] A. Happe and J. Cito, “Getting pwn’d by ai: Penetration testing
with large language models,” in Proceedings of the 31st ACM Joint
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , pp. 2082–2086, 2023.
[108] W. Ma, D. Wu, Y . Sun, T. Wang, S. Liu, J. Zhang, Y . Xue, and
Y . Liu, “Combining fine-tuning and llm-based agents for intuitive smart
contract auditing with justifications,” arXiv preprint arXiv:2403.16073 ,
2024.
[109] R. Fang, R. Bindu, A. Gupta, and D. Kang, “Llm agents
can autonomously exploit one-day vulnerabilities,” arXiv preprint
arXiv:2404.08144 , 2024.JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 41
[110] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y . Cao, and
K. Narasimhan, “Tree of thoughts: Deliberate problem solving with
large language models,” Advances in Neural Information Processing
Systems , vol. 36, 2024.
[111] Z. Rasheed, M. Waseem, A. Ahmad, K.-K. Kemell, W. Xiaofeng, A. N.
Duc, and P. Abrahamsson, “Can large language models serve as data
analysts? a multi-agent assisted approach for qualitative data analysis,”
arXiv preprint arXiv:2402.01386 , 2024.
[112] M. Ataei, H. Cheong, D. Grandi, Y . Wang, N. Morris, and A. Tessier,
“Elicitron: An llm agent-based simulation framework for design re-
quirements elicitation,” arXiv preprint arXiv:2404.16045 , 2024.
[113] G. Sridhara, S. Mazumdar, et al. , “Chatgpt: A study on its
utility for ubiquitous software engineering tasks,” arXiv preprint
arXiv:2305.16837 , 2023.
[114] M. Desmond, Z. Ashktorab, Q. Pan, C. Dugan, and J. M. Johnson,
“Evalullm: Llm assisted evaluation of generative outputs,” in Compan-
ion Proceedings of the 29th International Conference on Intelligent
User Interfaces , pp. 30–32, 2024.
[115] M. Gao, X. Hu, J. Ruan, X. Pu, and X. Wan, “Llm-based nlg evaluation:
Current status and challenges,” arXiv preprint arXiv:2402.01383 , 2024.
[116] L. J. Wan, Y . Huang, Y . Li, H. Ye, J. Wang, X. Zhang, and D. Chen,
“Software/hardware co-design for llm and its application for design
verification,” in 2024 29th Asia and South Pacific Design Automation
Conference (ASP-DAC) , pp. 435–441, IEEE, 2024.
[117] K. Kolthoff, C. Bartelt, and S. P. Ponzetto, “Data-driven prototyping via
natural-language-based gui retrieval,” Automated software engineering ,
vol. 30, no. 1, p. 13, 2023.
[118] V . D. Kirova, C. S. Ku, J. R. Laracy, and T. J. Marlowe, “Software
engineering education must adapt and evolve for an llm environment,”
inProceedings of the 55th ACM Technical Symposium on Computer
Science Education V . 1 , pp. 666–672, 2024.
[119] S. Jalil, S. Rafi, T. D. LaToza, K. Moran, and W. Lam, “Chatgpt and
software testing education: promises & perils (2023),” arXiv preprint
arXiv:2302.03287 , 2023.
[120] S. Suri, S. N. Das, K. Singi, K. Dey, V . S. Sharma, and V . Kaulgud,
“Software engineering using autonomous agents: Are we there yet?,” in
2023 38th IEEE/ACM International Conference on Automated Software
Engineering (ASE) , pp. 1855–1857, IEEE, 2023.
[121] C. Qian, X. Cong, C. Yang, W. Chen, Y . Su, J. Xu, Z. Liu, and
M. Sun, “Communicative agents for software development,” arXiv
preprint arXiv:2307.07924 , 2023.
[122] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, “Hugginggpt:
Solving ai tasks with chatgpt and its friends in hugging face,” Advances
in Neural Information Processing Systems , vol. 36, 2024.
[123] J. Chen, X. Hu, S. Liu, S. Huang, W.-W. Tu, Z. He, and L. Wen,
“Llmarena: Assessing capabilities of large language models in dynamic
multi-agent environments,” arXiv preprint arXiv:2402.16499 , 2024.
[124] M. Josifoski, L. Klein, M. Peyrard, Y . Li, S. Geng, J. P. Schnitzler,
Y . Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks
of reasoning and collaborating ai,” arXiv preprint arXiv:2308.01285 ,
2023.
[125] I. Weber, “Large language models as software components: A taxon-
omy for llm-integrated applications,” arXiv preprint arXiv:2406.10300 ,
2024.
[126] F. Vallecillos Ruiz, “Agent-driven automatic software improvement,” in
Proceedings of the 28th International Conference on Evaluation and
Assessment in Software Engineering , pp. 470–475, 2024.
[127] Z. Cheng, J. Kasai, and T. Yu, “Batch prompting: Efficient inference
with large language model apis,” arXiv preprint arXiv:2301.08721 ,
2023.
[128] S. Shankar, J. Zamfirescu-Pereira, B. Hartmann, A. G. Parameswaran,
and I. Arawjo, “Who validates the validators? aligning llm-assisted
evaluation of llm outputs with human preferences,” arXiv preprint
arXiv:2404.12272 , 2024.
[129] D. Roy, X. Zhang, R. Bhave, C. Bansal, P. Las-Casas, R. Fonseca, and
S. Rajmohan, “Exploring llm-based agents for root cause analysis,” in
Companion Proceedings of the 32nd ACM International Conference on
the Foundations of Software Engineering , pp. 208–219, 2024.
[130] Y . Li, Y . Zhang, and L. Sun, “Metaagents: Simulating interactions of
human behaviors for llm-based task-oriented coordination via collabo-
rative generative agents,” arXiv preprint arXiv:2310.06500 , 2023.
[131] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sundaresan,
“Unit test case generation with transformers and focal context,” arXiv
preprint arXiv:2009.05617 , 2020.
[132] Y . Zhang, W. Song, Z. Ji, N. Meng, et al. , “How well does llm generate
security tests?,” arXiv preprint arXiv:2310.00710 , 2023.[133] H. J. Kang, T. G. Nguyen, B. Le, C. S. P ˘as˘areanu, and D. Lo,
“Test mimicry to assess the exploitability of library vulnerabilities,”
inProceedings of the 31st ACM SIGSOFT International Symposium
on Software Testing and Analysis , pp. 276–288, 2022.
[134] S. Feng and C. Chen, “Prompting is all you need: Automated android
bug replay with large language models,” in Proceedings of the 46th
IEEE/ACM International Conference on Software Engineering , pp. 1–
13, 2024.
[135] S. Kang, J. Yoon, and S. Yoo, “Large language models are few-
shot testers: Exploring llm-based general bug reproduction,” in 2023
IEEE/ACM 45th International Conference on Software Engineering
(ICSE) , pp. 2312–2323, IEEE, 2023.
[136] C. S. Xia, M. Paltenghi, J. Le Tian, M. Pradel, and L. Zhang, “Fuzz4all:
Universal fuzzing with large language models,” in Proceedings of the
IEEE/ACM 46th International Conference on Software Engineering ,
pp. 1–13, 2024.
[137] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan, and
B. Ray, “Code-aware prompting: A study of coverage guided test gener-
ation in regression setting using llm,” arXiv preprint arXiv:2402.00097 ,
2024.
[138] J. A. Pizzorno and E. D. Berger, “Coverup: Coverage-guided llm-based
test generation,” arXiv preprint arXiv:2403.16218 , 2024.
[139] K. Liu, Y . Liu, Z. Chen, J. M. Zhang, Y . Han, Y . Ma, G. Li, and
G. Huang, “Llm-powered test case generation for detecting tricky
bugs,” arXiv preprint arXiv:2404.10304 , 2024.
[140] K. Li and Y . Yuan, “Large language models as test case gen-
erators: Performance evaluation and enhancement,” arXiv preprint
arXiv:2404.13340 , 2024.
[141] Z. Wang, W. Wang, Z. Li, L. Wang, C. Yi, X. Xu, L. Cao, H. Su,
S. Chen, and J. Zhou, “Xuat-copilot: Multi-agent collaborative system
for automated user acceptance testing with large language model,”
arXiv preprint arXiv:2401.02705 , 2024.
[142] C. Lee, C. S. Xia, J.-t. Huang, Z. Zhu, L. Zhang, and M. R. Lyu, “A
unified debugging approach via llm-based multi-agent synergy,” arXiv
preprint arXiv:2404.17153 , 2024.
[143] G. Deng, Y . Liu, V . Mayoral-Vilches, P. Liu, Y . Li, Y . Xu, T. Zhang,
Y . Liu, M. Pinzger, and S. Rass, “Pentestgpt: An llm-empowered
automatic penetration testing tool,” arXiv preprint arXiv:2308.06782 ,
2023.
[144] M. Xiao, Y . Xiao, H. Dong, S. Ji, and P. Zhang, “Ritfis: Robust input
testing framework for llms-based intelligent software,” arXiv preprint
arXiv:2402.13518 , 2024.
[145] R. Wang, Z. Li, C. Wang, Y . Xiao, and C. Gao, “Navrepair:
Node-type aware c/c++ code vulnerability repair,” arXiv preprint
arXiv:2405.04994 , 2024.
[146] A. Shestov, A. Cheshkov, R. Levichev, R. Mussabayev, P. Zadorozhny,
E. Maslov, C. Vadim, and E. Bulychev, “Finetuning large language
models for vulnerability detection,” arXiv preprint arXiv:2401.17010 ,
2024.
[147] A. Cheshkov, P. Zadorozhny, and R. Levichev, “Evaluation of chatgpt
model for vulnerability detection,” arXiv preprint arXiv:2304.07232 ,
2023.
[148] G. Lu, X. Ju, X. Chen, W. Pei, and Z. Cai, “Grace: Empowering
llm-based software vulnerability detection with graph structure and in-
context learning,” Journal of Systems and Software , vol. 212, p. 112031,
2024.
[149] H. Li, Y . Hao, Y . Zhai, and Z. Qian, “The hitchhiker’s guide to
program analysis: A journey with large language models,” arXiv
preprint arXiv:2308.00245 , 2023.
[150] Y . Ding, Y . Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair,
D. Wagner, B. Ray, and Y . Chen, “Vulnerability detection with code
language models: How far are we?,” arXiv preprint arXiv:2403.18624 ,
2024.
[151] F. V . Ruiz, A. Grishina, M. Hort, and L. Moonen, “A novel approach
for automatic program repair using round-trip translation with large
language models,” arXiv preprint arXiv:2401.07994 , 2024.
[152] B. Yang, H. Tian, J. Ren, H. Zhang, J. Klein, T. F. Bissyand ´e, C. L.
Goues, and S. Jin, “Multi-objective fine-tuning for enhanced program
repair with llms,” arXiv preprint arXiv:2404.12636 , 2024.
[153] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:
Efficient finetuning of quantized llms,” 2023.
[154] N. Jain, P. yeh Chiang, Y . Wen, J. Kirchenbauer, H.-M. Chu,
G. Somepalli, B. R. Bartoldson, B. Kailkhura, A. Schwarzschild,
A. Saha, M. Goldblum, J. Geiping, and T. Goldstein, “Neftune: Noisy
embeddings improve instruction finetuning,” 2023.
[155] J. Zhang, J. P. Cambronero, S. Gulwani, V . Le, R. Piskac, G. Soares,
and G. Verbruggen, “Pydex: Repairing bugs in introductory pythonJOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 42
assignments using llms,” Proceedings of the ACM on Programming
Languages , vol. 8, no. OOPSLA1, pp. 1100–1124, 2024.
[156] H. Joshi, J. C. Sanchez, S. Gulwani, V . Le, G. Verbruggen, and
I. Radi ˇcek, “Repair is nearly generation: Multilingual program repair
with llms,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 37, pp. 5131–5140, 2023.
[157] J. Xiang, X. Xu, F. Kong, M. Wu, H. Zhang, and Y . Zhang, “How
far can we go with practical function-level program repair?,” arXiv
preprint arXiv:2404.12833 , 2024.
[158] E. Hilario, S. Azam, J. Sundaram, K. Imran Mohammed, and B. Shan-
mugam, “Generative ai for pentesting: the good, the bad, the ugly,”
International Journal of Information Security , vol. 23, no. 3, pp. 2075–
2097, 2024.
[159] L. Zhong, Z. Wang, and J. Shang, “Ldb: A large language model
debugger via verifying runtime execution step-by-step,” arXiv preprint
arXiv:2402.16906 , 2024.
[160] L. Zhang, K. Li, K. Sun, D. Wu, Y . Liu, H. Tian, and Y . Liu, “Acfix:
Guiding llms with mined common rbac practices for context-aware
repair of access control vulnerabilities in smart contracts,” 2024.
[161] S. Hu, T. Huang, F. ˙Ilhan, S. F. Tekin, and L. Liu, “Large language
model-powered smart contract vulnerability detection: New perspec-
tives,” 2023.
[162] M. Alhanahnah, M. R. Hasan, and H. Bagheri, “An empirical evaluation
of pre-trained large language models for repairing declarative formal
specifications,” arXiv preprint arXiv:2404.11050 , 2024.
[163] F. Geissler, K. Roscher, and M. Trapp, “Concept-guided llm agents
for human-ai safety codesign,” in Proceedings of the AAAI Symposium
Series , vol. 3, pp. 100–104, 2024.
[164] A. Nouri, B. Cabrero-Daniel, F. T ¨orner, H. Sivencrona, and C. Berger,
“Engineering safety requirements for autonomous driving with large
language models,” arXiv preprint arXiv:2403.16289 , 2024.
[165] C. Thapa, S. I. Jang, M. E. Ahmed, S. Camtepe, J. Pieprzyk, and
S. Nepal, “Transformer-based language models for software vulnera-
bility detection,” in Proceedings of the 38th Annual Computer Security
Applications Conference , pp. 481–496, 2022.
[166] L. Zhong, Z. Wang, and J. Shang, “Debug like a human: A large
language model debugger via verifying runtime execution step-by-
step,” 2024.
[167] N. Alshahwan, M. Harman, I. Harper, A. Marginean, S. Sengupta, and
E. Wang, “Assured llm-based software engineering,” 2024.
[168] A. Cheshkov, P. Zadorozhny, and R. Levichev, “Evaluation of chatgpt
model for vulnerability detection,” 2023.
APPENDIX
A. Benchmarks
FIG. 12: D ISTRIBUTION OF BENCHMARKSB. Evaluation Metrics
FIG. 13: T OP10 E VALUATION METRICS

